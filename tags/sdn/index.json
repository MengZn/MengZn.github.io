[{"content":"  本篇文章的流程圖取自於KT Connnect 轻量级云原生测试环境治理工具並加以整理與新增實驗過程。\n越來越多的開發團隊基於 Kubernetes 部署公司的產品，一但服務上了 Kubernetes 當然會牽扯到基礎設計( Infra )、持續交付（ CD ）的過程，在如此龐大且複雜的架構與前提之下，我們有沒有什麼現有的專案可以幫助開發團隊快速的在 Kubernetes 上進行除錯。\n比如把線上（Remote）的流量導入本地(Local)，又會是反過說把本地測試的測試請求打到線上環境。\n痛點 在 Kubernetes 原生提供了一種方法（ port-forward ） ， 透過 port-forward 讓 local 端可以透過 : 存取線上的服務。\n我們應該知道 Kubernetes 原生提提供的 port-forward 能力不足，在微服務開發架構下，服務的調用除了調用方（ client ）依賴別人( service-A ) 以外，還有別人( service-B )依賴 調用方( client )。就目前 Kubernetes 原生提供的方法為基礎的話，勢必要將服務部署到一個測試環境中，但部署到測試環境又相當的麻煩，也有可能遇到各式不同的問題，例如：\n  單向調用：只能是從 local 向 kubernetes 發起請求，kubernetes 上的其他服務無法將請求轉發到local。\n  多個服務之間部署複雜：當微服務架構一大起來，部署與設定就會是一個問題需要 SRE 或是 DevOps 人員設置一個測試環境。\n  程式碼修改問題：一但採用 port-forward 去導流 loacl 端進入到 kubernetes 的流量，那我們的程式碼本來透過  存取的方式必定要修改成 : 一但導流的東西多了，也意味著程式碼修改的地方多了更容易發生人為設定上的錯誤。\n  解決方案 阿里巴巴（ alibaba ）這時腦洞大開開發了一個專案 kt-connect 我們先看看他們怎麼介紹自己的。\n Manage and Integration with your Kubernetes dev environment more efficient.\n 好吧看不是很懂xD，簡單來說阿里巴巴為了解決開發人員上述遇到的三個問題。\n 開發人員可以將 kubernetes 上的流量轉到 local 進行測試。 開發人員可以將 local 的測試請求發送到 kubernetes 進行測試。 開發人員不需要更動過多的程式碼，可以沿用  的方式對服務發起請求。  大概看完了 kt-connect 帶來的好處接著就來安裝玩玩看！\n 需要有 kubernetes 環境（開發環境具有 kubeconfig 部分權限 e.g. create list delete deploy 等） ssh 一顆熱愛的 debug 心  install kt-connect dependency package 使用 kc connection 之前 我們需要安裝一些依賴套件。\ninstall sshuttle #mac brew install sshuttle #linux pip install sshuttle #windows https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_windows_amd64.tar.gz install kt-connect 緊接著安裝 kt-connect 本體。\n#mac curl -OL https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_darwin_amd64.tar.gz tar -xzvf ktctl_darwin_amd64.tar.gz mv ktctl_darwin_amd64 /usr/local/bin/ktctl #linux curl -OL https://rdc-incubators.oss-cn-beijing.aliyuncs.com/stable/ktctl_linux_amd64.tar.gz tar -xzvf ktctl_linux_amd64.tar.gz mv ktctl_linux_amd64 /usr/local/bin/ktctl 安裝完後可以下 check 指令確認一下依賴包是否都安裝完成\nktctl check 1:01PM INF system info darwin-amd64 1:01PM INF checking ssh version OpenSSH_8.1p1, LibreSSL 2.7.3 1:01PM INF checking ssh version start at pid: 16888 1:01PM INF checking kubectl version Client Version: v1.18.2 Server Version: v1.17.1+6af3663 1:01PM INF checking kubectl version start at pid: 16890 1:01PM INF checking sshuttle version 1.0.4 1:01PM INF checking sshuttle version start at pid: 16891 1:01PM INF KT Connect is ready, enjoy it! connect kubernetes in localhost 先來測試看看從 localhost 與 remote 建立 tunnel ，記得一定要用 sudo 不然kt connection 無法操作系統的網路。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  sudo ktctl --namespace=default connect Password: 2:21PM INF Connect Start At 28422 2:21PM INF Client address 192.168.51.191 2:21PM INF deploy shadow deployment kt-connect-daemon-ojbky in namespace default 2:21PM INF pod label: kt=kt-connect-daemon-ojbky 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is running,but not ready 2:21PM INF Shadow pod: kt-connect-daemon-ojbky-6484749d95-2zqnl is ready. 2:21PM INF Fail to get pod cidr from node.Spec.PODCIDR, try to get with pod sample Forwarding from 127.0.0.1:2222 -\u0026gt; 22 Forwarding from [::1]:2222 -\u0026gt; 22 2:21PM INF port-forward start at pid: 28424 Handling connection for 2222 Warning: Permanently added \u0026#39;[127.0.0.1]:2222\u0026#39; (ECDSA) to the list of known hosts. client: Connected. 2:21PM INF vpn(sshuttle) start at pid: 28428 2:21PM INF KT proxy start successful client: warning: closed channel 1 got cmd=TCP_STOP_SENDING len=0 server: warning: closed channel 1 got cmd=TCP_EOF len=0   當 connection 建立好後可以在 remote 到建立了一個 tunnel 的 pod ，這個 pod 專門用來跟 local端進行 vpn/socks5 的連接方式(預設vpn)\n1 2 3 4  kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default kt-connect-daemon-ojbky-6484749d95-2zqnl 1/1 Running 0 60s ...   在 remote 環境準備一個 nginx deployment 進行測試\n1 2 3 4  kubectl create deploy nginx --image nginx deployment.apps/nginx created kubectl expose deploy nginx --port 80 service/nginx exposed   ","description":"","id":2,"section":"posts","tags":["kubernetes"],"title":"kubernetes ssh tunnel debug pratice","uri":"https://blog.jjmengze.website/posts/kubernetes/debug/kt-connection/"},{"content":"  前言 透過原理來了解事情的因果關係可能會太複雜，但作為一個軟體工程師理解背後如何實踐以及為什麼會有這樣的東西出現（歷史緣由）是非常重要的。\n本篇文章將會記錄 finalizers 的背後原理以及一些 source code ，這是使用者在操作 Kubernetes 常常會看到的一個欄位，好像有聽過但又不太了解的東西xD\n觀察細節 finalizers 定義於 Kubernetes 的 metadata的欄位\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  ... // ObjectMeta is metadata that all persisted resources must have, which includes all objects // users must create. type ObjectMeta struct { ... // Populated by the system when a graceful deletion is requested. \t// Read-only. \t// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata \t// +optional \tDeletionTimestamp *Time `json:\u0026#34;deletionTimestamp,omitempty\u0026#34; protobuf:\u0026#34;bytes,9,opt,name=deletionTimestamp\u0026#34;` // Number of seconds allowed for this object to gracefully terminate before \t// it will be removed from the system. Only set when deletionTimestamp is also set. \t// May only be shortened. \t// Read-only. \t// +optional \tDeletionGracePeriodSeconds *int64 `json:\u0026#34;deletionGracePeriodSeconds,omitempty\u0026#34; protobuf:\u0026#34;varint,10,opt,name=deletionGracePeriodSeconds\u0026#34;` // Must be empty before the object is deleted from the registry. Each entry \t// is an identifier for the responsible component that will remove the entry \t// from the list. If the deletionTimestamp of the object is non-nil, entries \t// in this list can only be removed. \t// Finalizers may be processed and removed in any order. Order is NOT enforced \t// because it introduces significant risk of stuck finalizers. \t// finalizers is a shared field, any actor with permission can reorder it. \t// If the finalizer list is processed in order, then this can lead to a situation \t// in which the component responsible for the first finalizer in the list is \t// waiting for a signal (field value, external system, or other) produced by a \t// component responsible for a finalizer later in the list, resulting in a deadlock. \t// Without enforced ordering finalizers are free to order amongst themselves and \t// are not vulnerable to ordering changes in the list. \t// +optional \t// +patchStrategy=merge \tFinalizers []string `json:\u0026#34;finalizers,omitempty\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; protobuf:\u0026#34;bytes,14,rep,name=finalizers\u0026#34;` ...   從註解中我們大概可以了解這個欄位要表達的意義,我把它整理成比較容易閱讀的方式（可能只有我覺得godoc的 // 註解換行很煩xD\n Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order. Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list.\n 大膽假設 我認為上述的文字簡單來說有三個重點(以下的順序不重要)\n If the deletionTimestamp of the object is non-nil, entries in this list can only be removed.\n簡單來說當 deletionTimestamp 不是 nil 的時候需要先刪除 Finalizers 內的條目 Finalizers may be processed and removed in any order\nFinalizers 條目的刪除順序不是固定的 Must be empty before the object is deleted from the registry.\n再刪除這個物件前 Finalizers 條目必須是空的  從上述註解的我們大概可以推測幾件事情，第一 當使用者刪除 Kubernetes 物件時，GC 回收機制需要檢查 Finalizers是否為空，第二其他物件可以任意刪除 Finalizers 欄位，前提是 deletionTimestamp 欄位不為 nil。\n大膽假設這個現象了，那就應該小心求證事實。\n小心求證 我已最近我在玩耍的 argo cd 進行球證對象，大部分處理物件的邏輯都會落在 controller 的 reconcile 階段裡，只要在 reconcile 搜尋 Finalizer 應該可以發現點什麼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  func (ctrl *ApplicationController) processProjectQueueItem() (processNext bool) { if origProj.DeletionTimestamp != nil \u0026amp;\u0026amp; origProj.HasFinalizer() { if err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil { log.Warnf(\u0026#34;Failed to finalize project deletion: %v\u0026#34;, err) } } return ... } func (ctrl *ApplicationController) finalizeProjectDeletion(proj *appv1.AppProject) error { apps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything()) if err != nil { return err } appsCount := 0 for i := range apps { if apps[i].Spec.GetProject() == proj.Name { appsCount++ break } } if appsCount == 0 { return ctrl.removeProjectFinalizer(proj) } else { log.Infof(\u0026#34;Cannot remove project \u0026#39;%s\u0026#39; finalizer as is referenced by %d applications\u0026#34;, proj.Name, appsCount) } return nil } func (ctrl *ApplicationController) removeProjectFinalizer(proj *appv1.AppProject) error { proj.RemoveFinalizer() var patch []byte patch, _ = json.Marshal(map[string]interface{}{ \u0026#34;metadata\u0026#34;: map[string]interface{}{ \u0026#34;finalizers\u0026#34;: proj.Finalizers, }, }) _, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{}) return err } func (proj AppProject) HasFinalizer() bool { return getFinalizerIndex(proj.ObjectMeta, common.ResourcesFinalizerName) \u0026gt; -1 } // getFinalizerIndex returns finalizer index in the list of object finalizers or -1 if finalizer does not exist func getFinalizerIndex(meta metav1.ObjectMeta, name string) int { for i, finalizer := range meta.Finalizers { if finalizer == name { return i } } return -1 } func (proj *AppProject) RemoveFinalizer() { setFinalizer(\u0026amp;proj.ObjectMeta, common.ResourcesFinalizerName, false) } // setFinalizer adds or removes finalizer with the specified name func setFinalizer(meta *metav1.ObjectMeta, name string, exist bool) { index := getFinalizerIndex(*meta, name) if exist != (index \u0026gt; -1) { if index \u0026gt; -1 { meta.Finalizers[index] = meta.Finalizers[len(meta.Finalizers)-1] meta.Finalizers = meta.Finalizers[:len(meta.Finalizers)-1] } else { meta.Finalizers = append(meta.Finalizers, name) } } }   我把主要的判斷邏輯抓出來，可以看到大致上的邏輯有幾項\n processProjectQueueItem function 裡面會判斷 Application 物件的 DeletionTimestamp 以及 Finalizers 內有沒有我們要關注的 key。 檢查 Applications namespaces 下面的其他的相關物件，並且計算物件的總數。  如果\u0026gt;0  直接回傳，因為相關的資源還沒情理乾淨   如果=0  移除 Finalizers 我們要關注的 key。      這邊可以看到幾個之前提過的重點\n 簡單來說當 deletionTimestamp 不是 nil 的時候需要先刪除 Finalizers 內的條目 Finalizers may be processed and removed in any order\nFinalizers 條目的刪除順序不是固定的  結語 類似像 Finalizers 這種的小螺絲都能起到這麼大的作用，我們應該更靜下心來學時事務的本質，不只要會操作它更要理解背後的原理是什麼。\n","description":"","id":3,"section":"posts","tags":["kubernetes"],"title":"Hey ! Kubernetes finalizers 不再煩惱","uri":"https://blog.jjmengze.website/posts/kubernetes/finalizers/"},{"content":"  由於協助自己公司內訓使用 helm 管理 Kubernetes Applicetion ，就非常的簡單的寫了怎麼看 Helm 的架構與 template 。\n目錄結構 可以先看一下這一個目錄結構\n. └── helm-example ├── Chart.yaml ├── charts ├── templates │ ├── NOTES.txt │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── hpa.yaml │ ├── ingress.yaml │ ├── service.yaml │ ├── serviceaccount.yaml │ └── tests │ └── test-connection.yaml └── values.yaml 首先我們會看到 chart,yaml 這個檔案，裡面記錄著這個 chart 是第幾版，是一個library 還是一個 application ，部署的 application 是幾版，另外最重要的是 apiVersion 是在定義這個chart 是 helm v2 還是 v3 ，因為 helm v2 跟 v3 不相容這邊要特別注意！\n可來看一下chart.yaml 裡面寫了什麼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v2name:helm-exampledescription:AHelmchartforKubernetes# A chart can be either an \u0026#39;application\u0026#39; or a \u0026#39;library\u0026#39; chart.## Application charts are a collection of templates that can be packaged into versioned archives# to be deployed.## Library charts provide useful utilities or functions for the chart developer. They\u0026#39;re included as# a dependency of application charts to inject those utilities and functions into the rendering# pipeline. Library charts do not define any templates and therefore cannot be deployed.type:application# This is the chart version. This version number should be incremented each time you make changes# to the chart and its templates, including the app version.# Versions are expected to follow Semantic Versioning (https://semver.org/)version:0.1.0# This is the version number of the application being deployed. This version number should be# incremented each time you make changes to the application. Versions are not expected to# follow Semantic Versioning. They should reflect the version the application is using.appVersion:1.16.0  value value.yaml 這個檔案定義了這個 chart 會用到的變數，基本上是提供給 _helpers.tpl 、Deployment.yaml\u0026hellip;做使用的。\n我們來看一下value.yaml 裡面寫了些什麼\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  # Default values for helm-example.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount:1image:repository:nginxpullPolicy:IfNotPresent# Overrides the image tag whose default is the chart appVersion.tag:\u0026#34;\u0026#34;imagePullSecrets:[]nameOverride:\u0026#34;example-helm-playground\u0026#34;fullnameOverride:\u0026#34;\u0026#34;serviceAccount:# Specifies whether a service account should be createdcreate:true# Annotations to add to the service accountannotations:{}# The name of the service account to use.# If not set and create is true, a name is generated using the fullname templatename:\u0026#34;\u0026#34;podAnnotations:{}podSecurityContext:{}# fsGroup: 2000securityContext:{}# capabilities:# drop:# - ALL# readOnlyRootFilesystem: true# runAsNonRoot: true# runAsUser: 1000service:type:ClusterIPport:80ingress:enabled:falseannotations:{}# kubernetes.io/ingress.class: nginx# kubernetes.io/tls-acme: \u0026#34;true\u0026#34;hosts:- host:chart-example.localpaths:[]tls:[]# - secretName: chart-example-tls# hosts:# - chart-example.localresources:{}# We usually recommend not to specify default resources and to leave this as a conscious# choice for the user. This also increases chances charts run on environments with little# resources, such as Minikube. If you do want to specify resources, uncomment the following# lines, adjust them as necessary, and remove the curly braces after \u0026#39;resources:\u0026#39;.# limits:# cpu: 100m# memory: 128Mi# requests:# cpu: 100m# memory: 128Miautoscaling:enabled:falseminReplicas:1maxReplicas:100targetCPUUtilizationPercentage:80# targetMemoryUtilizationPercentage: 80nodeSelector:{}tolerations:[]affinity:{}  可以看到裡面定義了很多參數，這些參數基本上 基本上 基本上 基本上 會套用到剛剛說到的_helpers.tpl 、Deployment.yaml\u0026hellip;做使用的，有些沒寫好的在這邊就算在這邊寫說我Deployment要使用什麼參數也沒用xDD\ntemplates templates 底下會放 Kubernetes要使用到的資源，如Deployment、configmap、service等等，這些資源都是一個樣板。\n有一個比較特別的檔案是_helpers.tpl，裡面放了一些變數，可以讓整個template底下的物件共享這些變數。\n我們可以把文件點開來看裡面裡面是什麼一回事。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  {{/* {{/* Expand the name of the chart. */}} {{- define \u0026#34;helm-example.name\u0026#34; -}} {{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{/* Create a default fully qualified app name. We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec). If release name contains chart name it will be used as a full name. */}} {{- define \u0026#34;helm-example.fullname\u0026#34; -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- printf \u0026#34;%s-%s\u0026#34; .Release.Name $name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{- end }} {{- end }} {{/* Create chart name and version as used by the chart label. */}} {{- define \u0026#34;helm-example.chart\u0026#34; -}} {{- printf \u0026#34;%s-%s\u0026#34; .Chart.Name .Chart.Version | replace \u0026#34;+\u0026#34; \u0026#34;_\u0026#34; | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{/* Common labels */}} {{- define \u0026#34;helm-example.labels\u0026#34; -}} helm.sh/chart: {{ include \u0026#34;helm-example.chart\u0026#34; . }} {{ include \u0026#34;helm-example.selectorLabels\u0026#34; . }} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} app.kubernetes.io/managed-by: {{ .Release.Service }} {{- end }} {{/* Selector labels */}} {{- define \u0026#34;helm-example.selectorLabels\u0026#34; -}} app.kubernetes.io/name: {{ include \u0026#34;helm-example.name\u0026#34; . }} app.kubernetes.io/instance: {{ .Release.Name }} {{- end }} {{/* Create the name of the service account to use */}} {{- define \u0026#34;helm-example.serviceAccountName\u0026#34; -}} {{- if .Values.serviceAccount.create }} {{- default (include \u0026#34;helm-example.fullname\u0026#34; .) .Values.serviceAccount.name }} {{- else }} {{- default \u0026#34;default\u0026#34; .Values.serviceAccount.name }} {{- end }} {{- end }}   簡單來說就是定義了好幾個全域參數，這邊我幫大家整理一下這幾個參數。\n  lafite-chatbot.name\n 條件是如果 value 的 nameOverride 有定義  就用 nameOverride 取代 .Chart.Name   如果長度超過63個字  後面的字都不要   如果最後一個字元有  把該字元修剪掉      helm-example.fullname\n 如果 Values.fullnameOverride 有定義的話  .Values.fullnameOverride  如果長度超過63個字  後面的字都不要   如果最後一個字元有  把該字元修剪掉       $name  如果 Values.fullnameOverride 有定義的話  就用 nameOverride 取代 .Chart.Name     如果 .Release.Name 有包含 $name 的話  如果長度超過63個字  後面的字都不要   如果最後一個字元有  把該字元修剪掉     最後按照上面的規則印出 .Release.Name-$name  如果長度超過63個字  後面的字都不要   如果最後一個字元有  把該字元修剪掉        helm-example.chart\n 印出 .Chart.Name-.Chart.Version  如果字元有遇到+  轉換成_   如果長度超過63個字  後面的字都不要   如果最後一個字元有  把該字元修剪掉        helm-example.labels\n helm.sh/chart : 加上之前定義的 helm-example.chart 如果有定義 .Chart.AppVersion  那就加上 app.kubernetes.io/version : .Chart.AppVersion 的雙引號   app.kubernetes.io/managed-by: {{ .Release.Service }}    `helm-example.selectorLabels\n app.kubernetes.io/name: 加上之前定義的 helm-example.name app.kubernetes.io/instance: 加上之前定義的 Release.Name    helm-example.serviceAccountName\n 如果.Values.serviceAccount.create 有定義的話  預設使用之前有定義的 helm-example.fullname  不然就是用 .Values.serviceAccount.name     預設使用default  不然就是 .Values.serviceAccount.name      了解了基礎的目錄結構後可以再深入地看一下元件的樣板\nDeployment 這邊遇到有用go template 的段落才會進行解說\n使用 _helpter.tpl 1 2 3 4  metadata:name:{{include\u0026#34;helm-example.fullname\u0026#34;.}}labels:{{- include\u0026#34;helm-example.labels\u0026#34;.|nindent4}}  先看到 name 的部分 這邊他會直接套用 _helpter.tpl 內的 fullname\n再來看到 labels 的部分 這邊他會直接套用 _helpter.tpl 內的 labels 並且透過 nindent function 將每個開頭空四格\n使用 value 1 2 3  {{- ifnot.Values.autoscaling.enabled}}replicas:{{.Values.replicaCount}}{{- end}}  接下來看到 replicas 的部分如果在 Values 沒有定義 autoscaling.enabled 就會採用 Values 內的 replicaCount。\n使用 go template function 1 2 3  selector:matchLabels:{{- include\u0026#34;helm-example.selectorLabels\u0026#34;.|nindent6}}  在這個部分 matchLabels 直接套用 _helpter.tpl 內的 selectorLabels 並且透過 nindent function 將每個開頭空四格。\n使用 go template to yaml 1 2 3 4 5  metadata:{{- with.Values.podAnnotations}}annotations:{{- toYaml.|nindent8}}{{- end}}  如果有在 value 寫 podAnnotations 這裡就會跑for each 把內容物把內容物，透過 nindent function 將每個開頭空四格。\n例如你在value內定義以下這個範例\n1 2  podAnnotations:prometheus.io/scrape:\u0026#34;true\u0026#34;  Helm 會幫你轉換成以下格式\n1 2 3 4  template:metadata:annotations:prometheus.io/scrape:\u0026#34;true\u0026#34;  只要了解上述這幾點並且搭配 go template 就可以做出非常多的變化，唯一缺點大概就是就是可讀性非常差吧。\nNote Note 是記錄這個 chart 是怎麼用的，直接來看範例。\n1 2 3 4 5 6 7 8 9  {{- if .Values.ingress.enabled }} 1. Get the application URL by running these commands: {{- if .Values.ingress.enabled }} {{- range $host := .Values.ingress.hosts }} {{- range .paths }} http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ . }}  {{- end }} {{- end }} ...   這邊如果我們有在 value 定義 ingress 定義 ingress.enabled 的話。\n就會用 for range 去遞迴取出 value 底下的 ingress 的所有 host\n並且用它組合成該 http:// \u0026hellip;\u0026hellip;的形式\n讓使用者知道這服務該怎麼存取。\n結語 go-template 非常不直觀在幫自己公司內訓的時候不太好 debug 找括弧看變數等，都不是容易維護。\n","description":"","id":4,"section":"posts","tags":["ci/cd"],"title":"Helm template 有講沒講差不多","uri":"https://blog.jjmengze.website/posts/cicd/helm/helmv3-gogo/"},{"content":"  記錄一下，在強迫使用TDD模式開發的情境與心得。\n比如我們需要一個下載器，這一個下載器會幫助我們從網路下載一些資料並且抽取出特定的資料。\n因應這個情境我使用TDD開發模式強迫自己使用測試驅動的思維思考。\n首先我們要需要一個下載器 Downloader ，所以我們會在 package 寫下 NewDownloader 的方法，接著讓 IDE 幫我補齊測試。\n以最小步驟開發 比如我在download package 先定義一個 NewDownloader 的方法，透過 IDE 指引幫我們建立他的測試。\n1 2 3  func NewDownloader() *Downloader { }     IDE會幫我們建立 Test Function 如下圖所示。\n  這時候我們會受到IDE的提示，告訴我們 Downloader 這個類型沒有定義。\n另外也告訴我們這裡有 TODO List 要我們加入一些 test cases 。\n我們先完成最少測試所需要的物件 Downloader ，透過IDE快速地建立所需要的物件。\n  其實 IDE 也只是簡單幫你打幾個字而已，像是這是一個 interface 還是一個 struct 還是一個 function 還是其他物件這邊就需要自己定義。\n  我們需要的是一個 struct ，為什麼？\n 還記得我們前面定義的 function 是一個 NewDownloader 嗎？\n通常這邊會需要的是 struct，所以 code 應該會長這樣子。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  type Downloader struct { } func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ // TODO: Add test cases. \t} for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } }   還缺少什麼呢？\n剛剛有提到的 Test Case ，接著補滿我們的 Test Case 。\n這裡只要因為我們只是 New 一個 NewDownloader 出來而已所以 Test Case 很間單的建立一個物，這時候程式碼可能會長這樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ { name: \u0026#34;really init\u0026#34;, want: \u0026amp;Downloader{}, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } }   可以看到 IDE 還有紅色的字表示有問題\n  於是我們到 download.go 去觀察有什麼問題。\n  原來這邊告訴我們沒有定義 Download 的型別\n疑！奇怪，我們不是在 Test 已經定義過了嗎？\n是這樣的，在 test 定義過的 Download struct 是沒有辦法再不是 test 的 package 所使用的。\n所以我們需要把 Test 定義過的 Download struct 過移過來到這裡，看看還有什麼其他問題。\n  搬移過來了， Download 型別已經沒有出現紅字了，這時候 IDE 告訴我們 NewDownloader 這個 function 沒有寫 return 回傳值。那我們就寫一個給他吧。\n於是我們的 code 現在變成這樣子\ndownload.go\n1 2 3 4 5 6 7 8  package download type Downloader struct { } func NewDownloader() *Downloader { return \u0026amp;Downloader{} }   download_test.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  package download import ( \u0026#34;reflect\u0026#34; \u0026#34;testing\u0026#34; ) func TestNewDownloader(t *testing.T) { tests := []struct { name string want *Downloader }{ { name: \u0026#34;really init\u0026#34;, want: \u0026amp;Downloader{}, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := NewDownloader(); !reflect.DeepEqual(got, tt.want) { t.Errorf(\u0026#34;NewDownloader() = %v, want %v\u0026#34;, got, tt.want) } }) } }   這時候跑test 應該會看到一個完美的 Test 成功\ngo test . ok kata-go-example/download 0.239s 持續迭代 雖然你可能會覺得很煩，為什麼要寫這些有的沒的，這些步驟 IDE 可以幫你大量處理，不過當我們習慣之後 IDE 會變成輔助你哪裡忘記寫，記住一個要點 小步驟的迭代。\n好的有 Downloader 物件之後我們沒有一個真的下載的方法，所以我們需要一個 Download 的方法，但是 因為很重要所以要說很多次 但是 但是 但是 但是 ，我們通常會希望這個 Download 的方法 不是寫死的，保留一些抽象空間可以替換。\n這邊我提供幾種方法\n第一種 在結構注入 function type 現定義一個最簡單的download function ，我們預期輸入一串網址，下載器會幫我處理，這串網址對應的IP位置，最簡單function 可能會是長這樣子。\n1 2 3  func (d *Downloader) Download(url string) string { }   定義完成之後透過 IDE 幫忙補齊 Test function\n  IDE 建立完成的 Code 大概會長以下這樣子，也會期待你幫他補完test case 。\n  到這裡不知道大家有沒有發現還是還沒實作 download 的 function， test code 看起來除了要我們加入一些 test case 之外好像就沒有別的錯誤了。\n這邊需要使用到一些小技巧例如 function type ，我們可以定義一個 Download 的function type 如下所示\n1  type DownloadFunc func(url string) string   定義了 DownloadFunc 我們可能會希望這個 Function 在 Downloader 的struct 裡面讓 Downloader 的結構擁有外部的傳進來的下載方法，如此一來就不會強依賴 Downloader 裡面寫什麼。\n既然我改了 NewDownloader 勢必會導致 NewDownloader 的測試方法不會通過\n  這邊IDE就提示我們參數不正確，我們這邊就簡單地補上一個 Download function 就行，要訣修改到之前的 test code 需要先保證舊的 test code 是可以正確執行的！\n好，那就繼續回到 Download function 測試這個部分，我們在 struct 中新增了 DownloadFunc 就可以讓 Downloader 直接使用，code 可能會像是以下範例。\n1 2 3  func (d *Downloader) Download(url string) string { return d.downloadFunc(url) }   那麼 test code 會變成怎樣樣子呢？\n原本是長這樣子的test code\n  但我們新增了在 struct 裡新增了 DownloadFunc 讓 Download 的邏輯被抽離了，這邊我們只要簡單的驗證一下 DownloadFunc 是否正確被使用即可，記得最後還需要補上test case 呦！\n最後我們的Download Function 的test code 應該會長這樣子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  func TestDownloader_Download(t *testing.T) { type args struct { url string } fakeWant := \u0026#34;this is fake function\u0026#34; tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ \u0026#34;google.com\u0026#34;, }, want: fakeWant, }, } fakeDownloadFunc := func(url string) string { return fakeWant } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := NewDownloader(fakeDownloadFunc) if got := d.Download(tt.args.url); got != tt.want { t.Errorf(\u0026#34;Download() = %v, want %v\u0026#34;, got, tt.want) } }) } }   而我們 package 裡面的 code 應該會是長這樣子\n1 2 3 4 5 6 7 8 9 10 11 12 13  type DownloadFunc func(url string) string type Downloader struct { downloadFunc DownloadFunc } func NewDownloader(downloadFunc DownloadFunc) *Downloader { return \u0026amp;Downloader{downloadFunc} } func (d *Downloader) Download(url string) string { return d.downloadFunc(url) }   第二種 在方法注入 function type 第二種方法一樣是注入function type 不過跟第一種不一樣的地方在於我們這一次是在functnio 的參數注入。\n一樣定義一個 function type\n1  type DownloadFunc func(url string) string   這一次我們不在 Struct 裡面接收這個 function ，改用在functnion 裡面接收這個function ，例如說我們會定義一個 function 他要求攜帶一個 function 進來。\n1 2 3  func (d *Downloader) DownloadWithFunc(url string, downloadFunc DownloadFunc) string { }   這時候我們一樣用 IDE 幫我們撰寫 test code\n  長出來的code 大概是會如下方圖片所示，一樣要求你加入一些test case 。\n  這時候我們先來修正 package ，比如說我希望外面function 處理完成後之後我要加入一點東西，範例如下所示。\n1 2 3  func (d *Downloader) DownloadWithFunc(url string, downloadFunc DownloadFunc) string { return downloadFunc(url)+\u0026#34;WithFunc\u0026#34; }   改完了，那 test code 要怎麼寫？\n這時候我們跟 方法一 一樣依外部的function ，所以我們一樣要訂 fakeDownloadFunc。\n大致上的test code會像是下面範例的樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  func TestDownloader_Download(t *testing.T) { type args struct { url string } fakeWant := \u0026#34;this is fake function\u0026#34; tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ \u0026#34;google.com\u0026#34;, }, want: fakeWant, }, } fakeDownloadFunc := func(url string) string { return fakeWant } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := NewDownloader(fakeDownloadFunc) if got := d.Download(tt.args.url); got != tt.want { t.Errorf(\u0026#34;Download() = %v, want %v\u0026#34;, got, tt.want) } }) } } func TestDownloader_DownloadWithFunc(t *testing.T) { type args struct { url string downloadFunc DownloadFunc } fakeWant := \u0026#34;this is fake function\u0026#34; fakeDownloadFunc := func(url string) string { return fakeWant } tests := []struct { name string args args want string }{ { name: \u0026#34;Fake Download Func\u0026#34;, args: args{ downloadFunc: fakeDownloadFunc, url: \u0026#34;google.com\u0026#34;, }, want: fakeWant + \u0026#34;WithFunc\u0026#34;, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { d := \u0026amp;Downloader{} if got := d.DownloadWithFunc(tt.args.url, tt.args.downloadFunc); got != tt.want { t.Errorf(\u0026#34;DownloadWithFunc() = %v, want %v\u0026#34;, got, tt.want) } }) } }   ","description":"","id":5,"section":"posts","tags":["Go","TDD"],"title":"強迫使用 TDD 開發模式以Go為例-2","uri":"https://blog.jjmengze.website/posts/go/tdd-test2/"},{"content":"  connection 特性  當發送一個 http2 請求的時候會建立一個 connection (TCP connection) 發送第二個請求的時候會延續使用第一個請求所建立的Connection 若是有多個 stream 的情況，每個 stream 是可以在同一個 tcp connection 併發傳輸，稱做為多路復用(multiplexing)  stream 特性  一條 TCP 連線上可以有多個處於 Open 狀態的 stream（併發傳輸） Client 可以主動建立 stream (ID基數 1,3,5,7,9\u0026hellip;) Server 可以主動建立 stream (ID偶數 2,4,6,8,10\u0026hellip;) 任何一端都可以自主關閉 stream 同一條 stream 裡面的 frame 都是有序地排列  Client 向 Server 請求時 Stream ID 會為基數（1,3,5,7,9\u0026hellip;)\nServer 向 Client 主動推送 Stream ID 會為偶數（2,4,6,8,10\u0026hellip;)\nframe 特性  可以包含一到多個 Header (Header很小只要一個message ，到Header很大會被切成多個frame) 可以包含零到多個 Data!   想像一下 http1.1 時的 body 跟 header 都變成了 frame\n簡單的來說 body 也叫 frame , header 也叫 frame 只是裡面資料不同而已。\n 到這裡你能比較 http1.1 與 http2 的不同嗎 ？  在 http1.1 中 每一次請求都要重新建立一個 tcp connection 在 http1.1 中 每一次傳輸都要帶上重複的 header 在 http1.1 中 資料都是以純文本的方式傳輸 在 http1.1 中 server 無法主動推送資料給 client 在 http1.1 中 header 沒有進行壓縮  本篇文章還會再深入一點點解析 http2 stream 的相關知識\nstream status 可以觀察下圖 stream status 的狀態變化，所有的 stream 一開始會處於 idle 狀態，最終會結束於 closed 狀態。\n了解各個狀態監視如何切換的，需要知道 接收/發送 了什麼 flag，下方我列出了幾種會發送或是接收的 flag。\n HEADERS (H) PUSH_PROMISE (PP) END_STREAM (ES) RST_STREAM (RST)  這邊會簡單的說明一下幾種可能的狀況\n  idle 狀態轉換\n 當 stream 狀態處於 idle ， server 收到 head (h) flag 時後會把當前 stream 進行狀態的切換，從圖中可以我們從看到 idle 收 h flag 後切換成 open 狀態。 當 stream 狀態處於 idle 狀態，如果這時候 Server 想要推送消息給 Client 端，那Server 會發出 PUSH_PROMISE (PP) Flag 這時 Server 會處於 reserved 狀態，這時候就要準備發送後面的資料（Head+Data）。    Open 狀態轉換\n 當 stream 狀態處於 open 狀態，這時我們如果發送（send）或是接收(recv) END_STREAM (ES) flag 的話，都會進入一個所謂的半關閉狀態(half closed)這時stream會等待接受flag進而轉變成close狀態。    reserved 狀態轉換\n 當 stream 狀態處於 reserved 狀態，這時候Server 一但推送了第一筆資料的Head出去就會把狀態切換成半關閉狀態 (half closed) 等待接受flag進而轉變成close狀態。    half closed 狀態轉換\n 當 stream 狀態處於 half closed 狀態，這時我們如果發送（send）或是接收(recv) END_STREAM (ES) 、RST_STREAM (RST) flag 的話，會把該 Stream 的狀態切換成Closed。    Data source:http2-in-action/\nstream 的priorty順序 由於 http2 大量的運用在前後與端溝通上，就算工程師只關心後端服務之間的溝通也需要知道，關於 http2 stream 的 priorty 的相關知識可以幫助我們在設計服務的時候善用已經存在的欄位。\n+---------------+ |Pad Length? (8)| +-+-------------+-----------------------------------------------+ |E| Stream Dependency? (31) | +-+-------------+-----------------------------------------------+ | Weight? (8) | +-+-------------+-----------------------------------------------+ | Header Block Fragment (*) ... +---------------------------------------------------------------+ | Padding (*) ... +---------------------------------------------------------------+ 從上圖我們可以看到 E 、 Stream Dependency? 、 Weight? 三個欄位，這三個欄位就表示了 http2 stream 的 priorty 的狀態。\n ？表示是可以選的欄位\n  Stream Dependency : 表示依賴其他 Stream 當其他 Stream 完成才能換該 Stream Weight : 表示該 Stream 的權重，預設值為 16 ，那可以設定的範圍在 1~256。  wireshark 動手做看看 直接從 wireshark 抓取 http2 的封包來觀察 priorty，看看 http2 priorty ， Stream ID ， Frame 到底是怎麼一回事。\n下圖為 wireshark 抓取瀏覽器存取 https://www.nba.com/?37 的封包\nMagic SETTINGS WINDOW_UPDATE 可以看到第一個封包是 client (192.168.51.150) 送往 server (23.11.187.239) 封包的簡易描述是 Magic 類型的封包，同時包含了SETTINGS frame 與 WINDOW_UPDATE。\nMagic  點開magic stream 的話可以看到 client 發送了一個 PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n  SETTINGS  點開SETTINGS stream 的話可以看到 client 發送了好幾個設定值，告訴了Server 我要設定成這樣。  Header table size Max concurrent streams Max hader list size Unknown    WINDOW_UPDATE  點開WINDOW_UPDATE stream 的話可以看到 client 發送了一個設定，告訴Server最大我的資料只能傳輸多少。  Headers 可以看到 client 發送了好幾個 HEADS 給SERVER並且Stream ID 都是奇數，這表示Client 發送給Server 的請求Stream ID都會是奇數。接下來 Cleint 又分別要了 Head[3] , Head[5] , Head[7] 那這些 Header有什麼關聯？\nstream ID priority 點開 Head[3] 可以看到這個請求的權重，那跟 Head[5] 有什麼差別\n點開 Head[5] 可以看到 ，觀察後可以看到 Head[3] 的priority 大於 Head[5]，權重計算後 Head[3] 要比 Head[5] 的資料更快被接收。\nstream ID dependency 除了 priority 之外還有 stream ID dependency 可以從上圖觀察到這兩個請求都要依賴 stream ID 0，表示都要先接收完成stream ID 0才可以接收各自的資料。\nData 可以看到 Server 回傳了資料給 Client 端，如圖所示 Client 要了 Head[1]的資料， Server 就回傳了六筆資料給 Client 。\n小結 重新複習了 http2 的特性，以前常提到得多路復用（multiplexing），Stream 以及 Frame 到底是什麼又有什麼特性，以及當 Client/Server 發送或是接收請求後 connection 會處於什麼狀態 做一個簡單的分析與介紹。\n最後從 wireshark 觀察 http2 請求的內容，觀察 Stream ID 與 Stream ID priority ，共用同一個 Connection 有些請求需要先被執行才能在執行其他請求的依賴關係，除此之外 沒有依賴關係的請求需要按照 priority 的大小 優先處理。\n","description":"","id":6,"section":"posts","tags":["http2"],"title":"Http2_stream","uri":"https://blog.jjmengze.website/posts/protocol/http2/http2_stream/"},{"content":"  先寫測試 假設今天我們要寫一下計算長方形週長的函數，那我們可能會定義一個 Perimeter 的 function ，這個 function 會預期攜帶兩個參數一個是長度 length 一個是寬度 width ，以及他們的型態是浮點數的型態 float64 。\n我們這測試函數可能是長這樣子\n測試長什麼樣子 Go 在撰寫測試的時候習慣使用以下這種風格( table-drive)，這種測試模式相當的方便。\n我會先寫出測試的目標所需點三個數值並且包裝成一個結構體。\n 情境名稱 情境需要的參數 情境預期的答案  在情境所需要的參數部分，在計算週長的情境之下我們需要長寬，故我們也將包裝成一個結構體。\n所以本次測試的程式碼大概如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  func TestPerimeter(t *testing.T) { type args struct { width float64 height float64 } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ width: 4.0, height: 4.0, }, want: 16.00, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.width, tt.args.height); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } }   按照 TDD 的教戰手冊，就是撰寫最少的程式碼達到用測試推導出真正的函數要怎麼定義。\n這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4  go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:27:14: undefined: Perimeter FAIL go-tdd-example/perimeter [build failed]   可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們 undefined: Perimeter ，原因是我們在 main package 裡面還沒有定義 Perimeter function ，那就來定義吧！\n補充main package func Perimeter(width float64, height float64) float64 { return 2*(width + height) } 這時後再執行一次 go test 看看結果。\n1 2 3  go test PASS ok go-tdd-example/perimeter 0.006s   重構！ 測試寫完了也正確執行，我們可以思考一下 Perimeter 這個函數所代表的意思，應該是計算週長吧？\n那會不會有其他開發者，把計算長方行週長的函數拿來計算三角形的週長，我想這是有可能的吧xD\n所以我們需要把計算長方形的周長函數進行封裝，希望這個函數只能計算長方形週長。\n那我們會先定義長方形的結構體，希望計算週長的函數 Perimeter 只接受長方形的結構體，並且計算他的週長。\n1 2 3 4  type Rectangle struct { Width float64 Height float64 }   測試長什麼樣子 按照 TDD 的教戰手冊，就是撰寫最少的程式碼達到用測試推導出真正的函數要怎麼定義，簡單的說就是讓編譯器幫助我們建構正確的程式碼。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  func TestPerimeter(t *testing.T) { type args struct { Rectangle Rectangle } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Rectangle{ Width: 4.0, Height: 4.0, }, }, want: 16.00, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.Rectangle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } }   這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 5 6  go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:28:23: not enough arguments in call to Perimeter have (Rectangle) want (float64, float64) FAIL go-tdd-example/perimeter [build failed]   可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\nnot enough arguments in call to Perimeter have (Rectangle) want (float64, float64) FAIL go-tdd-example/perimeter [build failed] 補充main package 原因是我們 Perimeter function ，定義的是 (width, height float64) 現在需要改成接收(rectangle Rectangle)。\n那我們的 Perimeter function 就會修改成以下形式。\n1 2 3  func Perimeter(rectangle Rectangle) float64 { return 2 * (rectangle.Width + rectangle.Height) }   這時後再執行一次 go test 看看結果。\n1 2 3  go test PASS ok go-tdd-example/perimeter 0.006s   不停的重構 做一個合格的工程師一直收到新的需求也是很正常的，今天PM告訴我客戶想要計算圓形的週長，對於工程師來說能盡量少改code就少改code xD，這時候遵循 TDD 的原則先寫 test !\n對於圓形來說，他有的結構體應該只有半徑，那我們先定義出他的結構體吧！\n1 2 3  type Cycle struct { Radius float64 }   測試的 code 可能會長這樣子，應算很直覺的 test code ，因為我們現在 Perimeter 的計算函數，需要計算長方形跟圓形。\n 情境需要的參數自然就變成 Rectangle 加 Cycle 在 for 迴圈裡面 Perimeter function 有輸入 Rectangle 也有輸入 Cycle 看起來一切都很合理  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  func TestPerimeter(t *testing.T) { type args struct { Rectangle Rectangle Cycle Cycle } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Rectangle{ Width: 4.0, Height: 4.0, }, Cycle{ Radius: 16.0, }, }, want: 7.85, }, { name: \u0026#34;rectangles\u0026#34;, args: args{ \u0026amp;Cycle{ Radius: 1, }, }, want: 1.57, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Perimeter(tt.args.Rectangle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } if got := Perimeter(tt.args.Cycle); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } }   這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4  go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:34:31: cannot use tt.args.Cycle (type Cycle) as type Rectangle in argument to Perimeter FAIL go-tdd-example/perimeter [build failed]   可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\ncannot use tt.args.Cycle (type Cycle) as type Rectangle in argument to Perimeter 這個意思大概是 Perimeter function 要求輸入一個 Rectangle 的類別，但是我們輸入 Cycle 類別。\n明明 長方形（Rectangle） 也要計算周長， 圓形（Cycle）也要計算週長啊！我不服氣！xD\n找到共同點 從上面的問題我們找到 長方形 跟 圓形 的共同點，都需要計算週長！那我們需要借助 go 的 interface 來定義共同的方法，在本篇的例子就是 計算週長 Perimeter function。\n1 2 3  type Shape interface { Perimeter() float64 }   現在測試變成怎麼樣了？ 還記得前面提到的測試結構嗎？我會先寫出測試的目標所需點三個數值並且包裝成一個結構體。\n 情境名稱 情境需要的參數 情境預期的答案  這時候情境所需要的參數需要做變動，現在情境需要的是有計算週長能力的東西也就是interface Shape。\n簡單的說現在情需要的參數是一個有能力計算周長的物件，在這個情境下我會把物件拿去計算看是否能得到我預期的答案。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  func TestPerimeter(t *testing.T) { type args struct { shape Shape } tests := []struct { name string args args want float64 }{ { name: \u0026#34;rectangles\u0026#34;, args: args{ Rectangle{ Width: 4.0, Height: 4.0, }, Cycle{ Radius: 1, }, }, want: 1.57, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := tt.args.shape.Perimeter(); got != tt.want { t.Errorf(\u0026#34;Perimeter() = %.2f, want %.2f\u0026#34;, got, tt.want) } }) } }   這時候我們可跑一下測試看看，站在測試的角度我們真正的函數還缺少了什麼東西。\n1 2 3 4 5 6 7 8 9  go test # go-tdd-example/perimeter [go-tdd-example/perimeter.test] ./main_test.go:17:15: cannot use \u0026amp;Rectangle literal (type *Rectangle) as type Shape in field value: *Rectangle does not implement Shape (missing Perimeter method) ./main_test.go:27:11: cannot use \u0026amp;Cycle literal (type *Cycle) as type Shape in field value: *Cycle does not implement Shape (missing Perimeter method) FAIL go-tdd-example/perimeter [build failed]   可以看到在測試的過程中我們看到 Error ，這個 Error 是告訴我們\n./main_test.go:17:15: cannot use \u0026amp;Rectangle literal (type *Rectangle) as type Shape in field value: *Rectangle does not implement Shape (missing Perimeter method) ./main_test.go:27:11: cannot use \u0026amp;Cycle literal (type *Cycle) as type Shape in field value: *Cycle does not implement Shape (missing Perimeter method) FAIL go-tdd-example/perimeter [build failed] 會看到這兩個錯誤，第一個錯誤是在說 *Rectangle 沒有實作 Shape interface ，因為他卻少了 Perimeter function 。\n第二個錯是在說 *Cycle 沒有實作 Shape interface ，因為他卻少了 Perimeter function 。\n補充main package 因為在測試的時候編譯器告訴我們 Rectangle 沒有實作 Shape interface ，因為他卻少了 Perimeter function ，那我們就識做一個吧，這時候我們需要用到 go 的function reciver 來幫 Rectangle 新增一個 Perimeter function。\n1 2 3 4 5 6 7 8 9 10 11  type Shape interface { Perimeter() float64 } func (r *Rectangle) Perimeter() float64 { return 2 * (r.Width + r.Height) } func (r *Cycle) Perimeter() float64 { return 3.14 * r.Radius / 2 }   這時後再執行一次 go test 看看結果。\n1 2 3  go test PASS ok go-tdd-example/perimeter 0.006s   我們成功了！\n小結 本篇是一個 TDD 基本的範例，不斷嘗試執行這些 test code 並讓編譯器 指引 我們找到正確的方案。\n用 Go 語言實作了一個週長的計算函數，在最初的需求我們只需要一個 長方形的 計算方法，非常簡單就可以實作出來。 透過 go test 提示我們 真正的函數長什麼樣子，藉著我們去補齊內部的邏輯，當補齊內部邏輯後再次執行 test code 直到測試通過為止。\n當有新的需求（圓形周長計算），我們透過 test code 與軟體工程的概念抽離周長計算方法，使用 go 的 interface 與 function reciver 協助我們透過介面的方式進行測試與重構。\nTDD 在現今的需求之後變得非常的重要，我們可以透過 test 完成重構讓程式碼變得可以測試與能夠預期城市的行為。\n","description":"","id":7,"section":"posts","tags":["Go","TDD"],"title":"強迫使用 TDD 開發模式以Go為例","uri":"https://blog.jjmengze.website/posts/go/tdd-test/"},{"content":"  如何確認網頁是否支援 http2 plugins chrome 瀏覽器可以透過安裝 HTTP/2 and SPDY indicator 這個 plugins ，來觀察目前正在瀏覽的網站是否支援 http2 。\n如果當前瀏覽頁面啟用了 HTTP2 則為藍色，如果啟用了 SPDY 則為綠色。\n可以看一下我最常瀏覽的網頁他是支援什麼樣的 protocal\n當我瀏覽 Kubernetes.io 的時候可以看到右上角是藍色的閃電表示支援 http2\n  那今天什麼網頁支援 SPDY 呢？\n當我瀏覽 google 首頁的時候發現它支援 SPDY protocal\n  dev mod 可以透過 chrome 的開發者模式觀察 請求是過過什麼樣的協定去進行傳輸的。\n當我們開啟 dev mod 的時候存取 github 的頁面可以觀察到有許多的請求是透過http2完成的，如下圖所示。\n  從圖中我們可以觀察到 有http/1.1的請求也有 h2的請求，這兩種請求分別代表著前端網頁透過 http1.1 以及 基於 tls 的 http2 請求。\n蝦米 wireshark 抓不到 http2 的封包 抓不到 http2 封包 先來看一張圖，是我設定 wireshark filter 只顯示 http2 的封包，並且同時存取 kubernetes.io 。\n  從上圖會發現\u0026hellip;.什麼資料都沒有，那怎麼會這樣子？\n怎麼解決 原因出在瀏覽器存取服務的時候需要透過基於 TLS 的 Http2 protocal ，所有的資料都被加密了我們不能看到 Client server 之間到底溝通了什麼，解決的方法其實很簡單只要在瀏覽器 Chrome 設定 SSLKEYLOGFILE 就可以順利的從 Chrome 抓到 http2 封包了。\n由於我的作業系統是 MACOS 這邊就示範 MACOS 怎麼在 Chrome 上啟動 SSLKEYLOGFILE。\n 建立keylogfile.log 文件，更改權限。讓 chrome 啟動時能寫入攜帶sslkeylogfile  mkdir ~/sslkeylogfile touch ~/sslkeylogfile/keylogfile.log sudo chmod 777 ~/sslkeylogfile/keylogfile.log 設定環境變數  export SSLKEYLOGFILE=~/sslkeylogfile/keylogfile.log echo \u0026quot;alias chrome=\\\u0026quot;open -a 'Google Chrome'\\\u0026quot;\u0026quot; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 如果要實驗 HTTP2 的話需要從 terminal 開啟 Chrome  chrome 透過 terminal 啟動 chrome 的圖如下所示\n  可以看一下 sslkeylogfile 的資料，裡面寫什麼不是很重要 wireshark 看得懂就好xDDD\n1 2 3 4 5 6 7 8 9  cat ~/sslkeylogfile/keylogfile.log CLIENT_HANDSHAKE_TRAFFIC_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 c0dc0b38f983aa5f1dd82f4b555c6be6195570622e90c956444938027e0dedc5 SERVER_HANDSHAKE_TRAFFIC_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 52ff297d7a0b1705bf6a876cbb307836ecc462455b9455d5eac606a1a6b1cac1 CLIENT_TRAFFIC_SECRET_0 e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 ea654ba45d35cae4a0d81fa2eb3e795c930ea8e06caab92f4da992cba14c354b SERVER_TRAFFIC_SECRET_0 e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 ef0d144892cf974cbb9e8d6ddebf5bd80370951cbaf5cc1223e8c63084e1da20 EXPORTER_SECRET e379a0d9d454a5c80bf4d7b49dcbc6eb9055f07cda497306768b828b20380056 d4e350adc0738aea83d6035fe0a21792fccd1a317208b6f9af547ffa36ba8195 CLIENT_HANDSHAKE_TRAFFIC_SECRET f95d9c5cd50060635969f3a88c89c548a063e1fa74c8cdb59d3af8d55a7e4558 c27c5c111347c718a3875a1300fa7a7dc88bbe4229189d7e0f0b887a0c9ac8a0 SERVER_HANDSHAKE_TRAFFIC_SECRET f95d9c5cd50060635969f3a88c89c548a063e1fa74c8cdb59d3af8d55a7e4558 507c5b0b8040cf85364dd272cf7d26eff2072baee3b2ed32206bddba85a88bc0 ...   設定wireshark 開啟 wireshark 後點選wireshark 選擇 Preferences 打開設定頁面，如下圖所示。\n  接著選擇 Protocols -\u0026gt; TLS -\u0026gt; (Pre)-Master-Secret log filename 把~/sslkeylogfile/keylogfile.log 放進去。\n  驗證 wireshark 抓取瀏覽器存取 http2 的封包 現在開啟 wireshark filter 輸入 http2 接著透過瀏覽器存取 github的網站，觀察 wireshark 是否能抓取到封包，如下圖所示。\n  從圖中看起來都能夠順利地抓到封包，後續會針對 http2 的欄位進一步分析與複習相關的概念。\n","description":"","id":8,"section":"posts","tags":["http2"],"title":"怎麼透過 wireshark 抓取瀏覽器存取 Http2 的封包？","uri":"https://blog.jjmengze.website/posts/protocol/http2/http2_wireshark/"},{"content":"  ref:kubernetes scheduler\n從上圖可以看到會經過幾個步驟分別是排成（scheduing）綁定(binding)，每個步驟又分成好幾個 Task 每一個 Task 由一到多個plugins組合而成。\n下面將簡單敘述各個 Task 的作用，後續章節講到plugins時會比較了解在做什麼。\n先是排成(scheduing)\n Sort  用於對 scheduler queue 中的 Pod 進行排序。一次只能啟用一個 sort plugins 。   PreFilter  用來進行預先檢查 pod 的需要滿足的條件，若是處理失敗則離開調度週期，重新進入 scheduler queue 。   Filter  用來對節點進行過濾，只會留下滿足 pod 執行條件的節點。   PreScore  將 Filter 階段所產出的節點進行預評分工作，若是處理失敗則離開調度週期，重新進入 scheduler queue。   Score  將 Filter 階段所產出的節點進行評分   NormalizeScore  進行分數的正規化處理。   Reserve  這時Pod處於保留狀態，它將在綁定週期結束時觸發  失敗 : Unreserve plugins 成功 : PostBind插件。     Permit  Pod的調度週期結束時，做最後的把關用來Permit、deny或是wait這次的調度。    最後進行綁定(binding)\n PreBind  在Pod真正綁定前設定相關的volume並將其安裝在目標節點，若是處理失敗則離開綁定週期，重新進入scheduler queue。   Bind  將Pod綁定到節點。   PostBind  成功綁定Pod後，用來清理相關的資源。    結語 接下來將會簡單介紹一下每個階段的 plugins 在做什麼 ，以便我們了解整個 framwork 的架構。\n","description":"","id":9,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Scheduler context流水線略窺一二","uri":"https://blog.jjmengze.website/posts/kubernetes/scheduler/kubernetes-scheduler-context/"},{"content":"  kube-scheduler 是Kubernetes的預設調度程式，每個新建立的Pod，kube-scheduler需要幫他選擇一個最佳的節點，讓節點上的Kubelet可以把Pod執行起來。\n但是，每個Pod對資源的要求都不同，因此，需要根據特定的調度演算法對現有的節點進行過濾。\nkube-scheduler通過兩步操作為Pod選擇節點：\n 篩選  篩選步驟中，scheduler根據篩選規則，保留符合規則的節點，例如某些節點被打上taint的標記，Pod無法容忍這些被污染過(taint)的節點，scheduler根據篩選規則將合適的節點留下。   計分  計分步驟中，scheduler根據計分規則為每個對篩選完的節點打分數接著為每一個節點進行得分排名，藉以選擇最合適擺放的Pod節點。 如果存在多個得分相等的節點，則 scheduler 會隨機選擇其中之一。    了解上述的大方向後，後續的文章將source code來了解整個scheduler的架構與運作流程。\n","description":"","id":10,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Scheduler 探討 source code 前的準備","uri":"https://blog.jjmengze.website/posts/kubernetes/scheduler/kubernetes-scheduler/"},{"content":"  Openshift cluster monitoring operator 我才不告訴你勒 slide: https://hackmd.io/p/OonUQ9QKQ7-7JPBd1N9tOA?both\nWe have a collaborative session\nplease prepare laptop or smartphone to join!\nWho am I?  Jason Li SRE/Backend developer ❤ kubernetes Go Rust 🐈 lover 不斷的從入門到放棄  Agenda  Background Related Work Method Conclusion  Background Prometheus Operator, Prometheus, Prometheus Adapter, kube-state-metrics, \u0026hellip; e.t.c.\nIn order to manage such diverse components, a centralized management configuration file is required.\nRelated Work  UI Prometheus Metrics Thanos  UI  Grafana  Prometheus  Prometheus Operator Prometheus-k8s\n👎 - Prometheus-user-workload Alertmanager  Prometheus Operator   Provide Kubernetes native deployment and management related monitoring components.\n  automate the configuration of a Prometheus based monitoring stack for Kubernetes clusters.\n Prometheus Alertmanager Related components    Prometheus Operator(cont’d) Metrics  node-exporter kube-state-metrics openshift-state-metrics  👎 prometheus-adapter\n👎 Telemeter Client\n👎 configuration sharing\nnode-exporter  Node exporter for hardware and OS metrics exposed by *NIX kernels. We can scrape, including a wide variety of system metrics further down in the output (prefixed with node_).  1 2 3 4 5 6 7 8 9 10  # HELP node_network_transmit_queue_length transmit_queue_length value of /sys/class/net/\u0026lt;iface\u0026gt;. # TYPE node_network_transmit_queue_length gauge node_network_transmit_queue_length{device=\u0026#34;br0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;eth0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;lo\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;ovs-system\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;tun0\u0026#34;} 1000 node_network_transmit_queue_length{device=\u0026#34;veth24377b8e\u0026#34;} 0 node_network_transmit_queue_length{device=\u0026#34;veth58bd788d\u0026#34;} 0 ...   kube-state-metrics  Focused on the health of the individual Kubernetes components, such as deployments, nodes and pods. Exposes raw data unmodified from the Kubernetes API Designed to be consumed either by Prometheus  openshift-state-metrics  Expands upon kube-state-metrics by adding metrics for OpenShift specific resources. Expose cluster-level metrics for OpenShift specific resources  openshift-state-metrics (cont’d)  BuildConfig Metrics Build Metrics DeploymentConfig Metrics ClusterResourceQuota Metrics Route Metrics Group Metrics  ref: https://github.com/openshift/openshift-state-metrics\nThanos  Thanos Thanos Querier Thanos Ruler  Method    Component Key     Prometheus Operator prometheusOperator   Prometheus prometheusK8s   Alertmanager alertmanagerMain   kube-state-metrics kubeStateMetrics   openshift-state-metrics openshiftStateMetrics   Grafana grafana   Telemeter Client telemeterClient   Prometheus Adapter k8sPrometheusAdapter   Thanos Querier thanosQuerier    Method (cont’d)  Only Prometheus and Alertmanager have extensive configuration options. Other components usually provide only the nodeSelector field.  Method (cont’d) move components to the node\n1 2 3 4 5 6 7 8  data:config.yaml:| prometheusOperator:nodeSelector:foo:barprometheusK8s:nodeSelector:foo:bar  persistent volume claim\n1 2 3 4 5 6 7 8 9 10 11  data:config.yaml:| prometheusK8s:volumeClaimTemplate:metadata:name:localpvcspec:storageClassName:local-storageresources:requests:storage:40Gi  custom Alertmanager configuration  At this stage, cluster monitoring does not provide Alertmanager settings  Conclusion 💯 💪 🎉\nWrap up  Self-updating monitoring stack that is based on Prometheus wider eco-system Provides monitoring of cluster components Expect to manage each component through the configuration file🎉  Thank you! 🐑","description":"","id":11,"section":"posts","tags":["openshift","talk"],"title":"Cluster Monitoring Operator","uri":"https://blog.jjmengze.website/posts/ocp/cluster-monitoring-operator/"},{"content":"  圖片來源\nKubernetes Cluster 中 API Server 是管理整個叢集的大腦， API-Server 一般執行在 Controller Plane （Master Node）上，維運人員通常會透過 CLI kubectl 工具去管理 Kubernetes。\n但還有許多方式可以存取 Kubernetes API Server ，今天會建立一個特殊場景。有一個名為 Jean 的使用者，透過 Rest API 的方式存取 Kubernetes 的資源\n向 Kubernetes API Server 發起請求通常需要有一個有權限的 ServiceAccount ，使用這個 ServiceAccount 通過 ClusterRole 、 Role 、 ClusterRoleBinding 、 RoleBinding 賦予操作相關資源的權限， 與 API Server 的請求基本上是基於 TLS ，所以 User 發起請求的時候還需要自帶憑證，當然我們也可以透過非安全方式存取 API Server ， 但是不推薦在環境中使用，這邊就不再多做說明。\ncreate user create private key 為了簡單起見，本篇將 X.509 client certificates 與 OpenSSL 結合使用建立User：\n Create a private key for jean:  1  openssl genrsa -out jean.key 2048   create CSR 建立一個證書籤名請求（CSR）\n Without Group\n 1 2 3  openssl req -new -key jean.key \\ -out jean.csr \\ -subj \u0026#34;/CN=jean\u0026#34;   singup CSR 與 Kubernetes CA 簽署 CSR ， Kubernetes CA 通常在 /etc/kubernetes/pki/ 間單的授權100天。\n1 2 3 4 5  openssl x509 -req -in jean.csr \\ -CA /etc/kubernetes/pki/ca.crt \\ -CAkey /etc/kubernetes/pki/ca.key \\ -CAcreateserial \\ -out jean.crt -days 100   檢查一下當前資料夾的檔案，應該會有三個檔案分別是 jean.crt 、 jean.csr 、 jean.key\n1 2  ls jean.crt jean.csr jean.key   做完以上的步驟就簡單的建立了一個名為 Jean 的 user ，我們後續可以使用 rest api 或是 CLI kubectl 去存取 Kubernetes 資源。\naccess kubernetes api server check API Server IP 我們需要先確認 Master 在哪裡，這邊有兩種方式都可以存取到 Kubernetes Master Node 的IP。\n master NIC IP  1 2 3  kubectl get node jason-test -o go-template --template=\u0026#39;{{ (index .status.addresses 0).address }}\u0026#39; 172.18.0.5   API Server Service IP  1 2 3  kubectl get svc kubernetes -o go-template --template=\u0026#39;{{.spec.clusterIP}}\u0026#39; 10.96.0.1   第一種就是直接確認 master Node 的 IP ，第二種是可以透過 Kubernetes Service 上的 VIP 。\naccess pods list 例如我們可以透過 curl 指令直接存取 Master Node IP 上的 API Server\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  curl https://172.18.0.5:6443/api/v1/namespaces/default/pods/ --cacert /etc/kubernetes/pki/ca.crt --cert ./jean.crt --key ./jean.key { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;pods is forbidden: User \\\u0026#34;jean\\\u0026#34; cannot list resource \\\u0026#34;pods\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; in the namespace \\\u0026#34;default\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;code\u0026#34;: 403 }   但這裡會發現無法存取 Pod resource in \u0026quot;\u0026rdquo; group 這邊是因為沒有針對 jean 去設定 RBAC 的存取範圍。\n 新增一個讀取 default namespace 的 role  1 2 3 4 5 6 7 8 9  apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:defaultname:pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; indicates the core API groupresources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]   幫 Jean 綁定有能力讀取 Pod 的角色  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  apiVersion:rbac.authorization.k8s.io/v1# This role binding allows \u0026#34;jean\u0026#34; to read pods in the \u0026#34;default\u0026#34; namespace.# You need to already have a Role named \u0026#34;pod-reader\u0026#34; in that namespace.kind:RoleBindingmetadata:name:read-podsnamespace:defaultsubjects:# You can specify more than one \u0026#34;subject\u0026#34;- kind:Username:jane# \u0026#34;name\u0026#34; is case sensitiveapiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; specifies the binding to a Role / ClusterRolekind:Role#this must be Role or ClusterRolename:pod-reader# this must match the name of the Role or ClusterRole you wish to bind toapiGroup:rbac.authorization.k8s.io  完成以上綁定的動作之後 Jean 就有了讀取 default namespaces 的權限了\nreaccess kubernetes api server 一樣透過 curl 指令存取 Kubernetes api server ，最後透過 jq 幫忙整理輸出的內容。\n1 2 3 4 5 6 7 8 9 10 11  curl -s https://172.18.0.5:6443/api/v1/namespaces/default/pods/ --cacert /etc/kubernetes/pki/ca.crt --cert ./jean.crt --key ./jean.key | jq -r \u0026#39;.items[].metadata.name\u0026#39; greetings-run-1598345882182-pod-c7vbz h2c-client h2c-server-78bbb8f665-qz7kw hello-run-1598334732648-pod-ckvgk hello-run-1598342326047-pod-5hbj7 hello-run-1598342412495-pod-kbcsp hello-run-1598342606769-pod-xj6wd hello-run-1598344900394-pod-nc95n print-date-run-1598433322555-pod-m9lqf   小結 其實大多數的方法 例如透過 client-go 撰寫程式去存取 Kubernetes 資源又或是透過 CLI kubectl 去存取 Kubernetes 狀態，底層都是透過 Reset api ，學習怎麼使用 Reset api 去讀取資料也是一個很有用的方法\n像是 python client 如果沒有更新的話，我們可以使用 reset api 的方式取得當前 Kubernetes 資源近一步去修改。\n","description":"","id":12,"section":"posts","tags":["kubernetes"],"title":"使用reset api 存取 Kubernetes ","uri":"https://blog.jjmengze.website/posts/kubernetes/reset-api-server/"},{"content":"  簡單理解 Operator Lifecycle Manager - 2 (OLM) OpenShift Container Platform 4，使用了大量的 Operator 管理 Kubernetes Resource ，或許這跟 redhat 的推行Kubernetes政策有關吧 xD。\n但是這邊會延伸出蛋生雞雞生蛋的問題，那 Operator 要怎麼被管理呢？\nredhat 給出了一個解答那就是 Operator Lifecycle Manager 又稱 OLM，使用者透過 Declarative 的方式告訴 OLM ，要建立什麼樣子的 Operator 。\n:::info\n本篇文章的重點在於 OLM，是用來管理 Operator。\n朝著這方面去思考一且都會變得比較簡單！\n:::\n接續著上篇提到觀念本篇會示範如何在 cluster 中從 CatalogSources 下載一個 PackageManifest 並且安裝到環境。\nGUI 操作 可以從圖片看到在 administration 模式底下可以從左邊選取 Operators ，裡面有兩個分頁分別是 OperatorHub 以及 install Operators。\n我們可以透過 OCP Portal 直接下載想要的 Operator ，可以在右手邊看到有許多種類提供我們做選擇。\n這個部份留給有興趣的朋友，今天著重於 CLI 的操作。\nCRD - OperatorGroup 從 CatalogSources 下載一個 PackageManifest 之前我們要先建立一個project(namespaces)，讓操作都在這個 project 下。\n1  oc new-project playolm   建立完成 project 後我們還需要建立一個 OperatorGroup 確認之後安裝的 operator 只有操作這個 namespaces 的權限，對於 OperatorGroup 想要多瞭解一下可以參考olm book。\n1 2 3 4 5 6 7 8 9 10 11  cat \u0026lt;\u0026lt;EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: prometheus-operatorgroup namespace: playolm spec: targetNamespaces: - playolm EOF operatorgroup.operators.coreos.com/prometheus-operatorgroup created   透過指令確認一下剛剛部署上去的 operatorgroup\n1 2 3  oc get operatorgroup prometheus-operatorgroup NAME AGE prometheus-operatorgroup 114s   CRD - Subscription 剛剛設定好operatorgroup，現在要到 CatalogSources 下載一個 PackageManifest 還記得上一個章節有拿一個 community-operators 的 prometheus-exporter-operator yaml 簡單的看一下裡面的內容嗎？\n這邊在執行一次看一下裡面的 yaml 檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124  oc get packagemanifests prometheus-exporter-operator -o yaml apiVersion: packages.operators.coreos.com/v1 kind: PackageManifest metadata: creationTimestamp: \u0026#34;2020-06-20T07:23:23Z\u0026#34; labels: catalog: community-operators catalog-namespace: openshift-marketplace olm-visibility: hidden openshift-marketplace: \u0026#34;true\u0026#34; operatorframework.io/arch.amd64: supported operatorframework.io/os.linux: supported opsrc-datastore: \u0026#34;true\u0026#34; opsrc-owner-name: community-operators opsrc-owner-namespace: openshift-marketplace opsrc-provider: community provider: Red Hat provider-url: \u0026#34;\u0026#34; name: prometheus-exporter-operator namespace: default selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/prometheus-exporter-operator spec: {} status: catalogSource: community-operators catalogSourceDisplayName: Community Operators catalogSourceNamespace: openshift-marketplace catalogSourcePublisher: Red Hat channels: - currentCSV: prometheus-exporter-operator.v0.2.0 currentCSVDesc: annotations: alm-examples: |- [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;monitoring.3scale.net/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;PrometheusExporter\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example-memcached\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;dbHost\u0026#34;: \u0026#34;your-memcached-host\u0026#34;, \u0026#34;dbPort\u0026#34;: 11211, \u0026#34;grafanaDashboard\u0026#34;: { \u0026#34;label\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;autodiscovery\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;enabled\u0026#34; } }, \u0026#34;type\u0026#34;: \u0026#34;memcached\u0026#34; } } ] capabilities: Deep Insights categories: Monitoring certified: \u0026#34;false\u0026#34; containerImage: quay.io/3scale/prometheus-exporter-operator:v0.2.0 createdAt: \u0026#34;2020-06-08 00:00:00\u0026#34; description: Operator to setup 3rd party prometheus exporters, with a collection of grafana dashboards repository: https://github.com/3scale/prometheus-exporter-operator support: Red Hat, Inc. apiservicedefinitions: {} customresourcedefinitions: owned: - description: Configures a prometheus exporter to monitor a memcached instance displayName: PrometheusExporter kind: PrometheusExporter name: prometheusexporters.monitoring.3scale.net version: v1alpha1 description: | A Kubernetes Operator based on the Operator SDK to centralize the setup of 3rd party prometheus exporters on **Kubernetes/OpenShift**, with a collection of grafana dashboards. You can setup different prometheus exporters to monitor the internals from different databases, or even any available cloudwatch metric from any AWS Service, by just providing a few parameters like **dbHost** or **dbPort** (operator manages the container image, port, argument, command, volumes... and also prometheus **ServiceMonitor** and **GrafanaDashboard** k8s objects). Current prometheus exporters types supported, managed by same prometheus-exporter-operator: * memcached * redis * mysql * postgresql * sphinx * es (elasticsearch) * cloudwatch The operator manages the lifecycle of the following objects: * Deployment (one per CR) * Service (one per CR) * ServiceMonitor (optional, one per CR) * GrafanaDashboard (optional, one per Namespace) ### Documentation Documentation can be found on our [website](https://github.com/3scale/prometheus-exporter-operator#documentation). ### Getting help If you encounter any issues while using operator, you can create an issue on our [website](https://github.com/3scale/prometheus-exporter-operator) for bugs, enhancements, or other requests. ### Contributing You can contribute by: * Raising any issues you find using Prometheus Exporter Operator * Fixing issues by opening [Pull Requests](https://github.com/3scale/prometheus-exporter-operator/pulls) * Submitting a patch or opening a PR * Improving [documentation](https://github.com/3scale/prometheus-exporter-operator) * Talking about Prometheus Exporter Operator All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/prometheus-exporter-operator/issues). ### License Prometheus Exporter Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/prometheus-exporter-operator/blob/master/LICENSE) displayName: Prometheus Exporter Operator installModes: - supported: true type: OwnNamespace - supported: true type: SingleNamespace - supported: false type: MultiNamespace - supported: true type: AllNamespaces provider: name: Red Hat version: 0.2.0 name: alpha defaultChannel: alpha packageName: prometheus-exporter-operator provider: name: Red Hat   裡面的內容很多看起來心煩意亂的，不過我們真正要關心的只有以下四點。\n name: prometheus-exporter-operator defaultChannel: alpha catalogSource: community-operators catalogSourceNamespace: openshift-marketplace  有這幾個參數就可以告訴olm說我們要用的是哪個catalogsources以及是哪一個packagemanifests。\n從defaultChannel 我們可以看到我們要用到是 alpha 版，alpha 對應到的是operator prometheus-exporter-operator.v0.2.0 版。\n進入這個 CRD Subscription 的重頭戲，宣告我要用哪一個catalogsources以及是哪一個packagemanifests。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  cat\u0026lt;\u0026lt;EOF|ocapply-f-apiVersion:operators.coreos.com/v1alpha1kind:Subscriptionmetadata:name:prometheus-exporter-operatornamespace:playolmspec:channel:alphainstallPlanApproval:Manualname:prometheus-exporter-operatorsource:community-operatorssourceNamespace:openshift-marketplaceinstallPlanApproval:ManualEOFsubscription.operators.coreos.com/prometheus-exporter-operatorcreated  這時候可以透過指令看看這個project訂閱了哪些subscription。\n1 2 3  $ oc get subscription NAME PACKAGE SOURCE CHANNEL prometheus-exporter-operator prometheus-exporter-operator community-operators alpha   這邊的 subscription 可以把它當成 如果packagemanifests 有更新我們會收到更新的概念，說白了就是你訂閱youtuber有開啟小鈴鐺他就會通知你xD。\nCRD - Installplan 可以透過另外一個CRD看到目前 project 底下有訂閱（安裝？ 抱歉找不到一個適當的中文）哪些 Operator 他的版本是多少，現在安裝了嗎等資訊。\n1 2 3  oc get installplan NAME CSV APPROVAL APPROVED install-b5tmz prometheus-exporter-operator.v0.2.0 Manual false   從上署指令的結果來看這個 project 訂閱了prometheus-exporter-operator.v0.2.0 現在還沒有被 approved 所以還沒安裝到環境中。\n現在我們要手動上一個 patch 讓這個安裝包被 approved ，再來觀察環境的變化。\n1 2  oc patch installplan install-b5tmz --type=\u0026#39;json\u0026#39; -p \u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/approved\u0026#34;, \u0026#34;value\u0026#34;:true}]\u0026#39; installplan.operators.coreos.com/install-b5tmz patched   再透過指令看現在prometheus-exporter-operator.v0.2.0是否被approved了。\n1 2 3  oc get installplan NAME CSV APPROVAL APPROVED install-b5tmz prometheus-exporter-operator.v0.2.0 Manual true   CRD - ClusterServiceVersion (CSV) 可以透過 ClusterServiceVersion 觀察目前在 cluster 執行的 Operator 是哪一個版本，以及有哪些 operator 正在執行。\n:::warning\n這邊要注意，只有被approved了的installplan才會被clusterserviceversion紀錄。\n:::\n1 2 3  oc get clusterserviceversion NAME DISPLAY VERSION REPLACES PHASE prometheus-exporter-operator.v0.2.0 Prometheus Exporter Operator 0.2.0 Succeeded   看起來都有成功在執行，現在可以觀察 Kubernetes Cluster 裡面是否有安裝 prometheus 的相關元件。\n觀察prometheus CRD有沒有成功安裝\n1 2 3 4  oc get crd | grep prometheus prometheuses.monitoring.coreos.com 2020-06-20T07:24:12Z prometheusexporters.monitoring.3scale.net 2020-07-24T03:20:46Z prometheusrules.monitoring.coreos.com 2020-06-20T07:24:13Z   觀察 Prometheus CRD有沒有成功安裝\n1 2 3 4  oc get crd | grep prometheus prometheuses.monitoring.coreos.com 2020-06-20T07:24:12Z prometheusexporters.monitoring.3scale.net 2020-07-24T03:20:46Z prometheusrules.monitoring.coreos.com 2020-06-20T07:24:13Z   觀察 Prometheus ServiceAccount 有沒有成功安裝\n1 2  oc get sa | grep prometheus prometheus-exporter-operator 2 7m45s   觀察 Prometheus role rolebinding 有沒有成功安裝\n1 2 3  oc get role,rolebinding | grep prometheus role.rbac.authorization.k8s.io/prometheus-exporter-operator.v0.2.0-2nb8s 8m40s rolebinding.rbac.authorization.k8s.io/prometheus-exporter-operator.v0.2.0-2nb8s-prometheus-exporfkbqh 8m40s   最後看看 Prometheus Operator 有沒有成功安裝到 Kubernetes 上\noc get deployments NAME READY UP-TO-DATE AVAILABLE AGE prometheus-exporter-operator 1/1 1 1 9m38s 看起來一切都很正常，但魔鬼藏在細節裡是不是 prometheus-exporter-operator Deployment版本如我們預期的一樣\n1 2  oc get deployment prometheus-exporter-operator -o go-template --template \u0026#39;{{range .spec.template.spec.containers}} {{.image}}{{end}}\u0026#39; quay.io/3scale/prometheus-exporter-operator:v0.2.0   從上述輸出的結果來看版本跟 packagemanifests 所定義的是一樣的。\n結語 雖然感覺得出來 RedHat 立意良好，但是操作那麼多 CRD 情況真的有點麻煩，另外我在測試這幾個 CRD 的時候發現當刪除 ClusterServiceVersion 的時候，Operator的相關資源都會被刪除，感覺這個邏輯不怎麼正確。\n我個人覺得是不是 installplan 刪除或是 DISAPPROVED 才會刪除 ClusterServiceVersion ，使用者就算 誤刪 了也應該偵測到這件事情並重新生成一個 ClusterServiceVersion 才對。\n以上純屬個人見解，希望能與大家討論交流。\n","description":"","id":13,"section":"posts","tags":["openshift"],"title":"簡單理解 Operator Lifecycle Manager - 2 (OLM)","uri":"https://blog.jjmengze.website/posts/ocp/operator-lifecycle-manager-2/"},{"content":"  簡單理解 Operator Lifecycle Manager - 1 (OLM) OpenShift Container Platform 4，使用了大量的 Operator 管理 Kubernetes Resource ，或許這跟 redhat 的推行Kubernetes政策有關吧 xD。\n但是這邊會延伸出蛋生雞雞生蛋的問題，那 Operator 要怎麼被管理呢？\nredhat 給出了一個解答那就是 Operator Lifecycle Manager 又稱 OLM，使用者透過 Declarative 的方式告訴 OLM ，要建立什麼樣子的 Operator 。\n:::info\n本篇文章的重點在於 OLM，是用來管理 Operator。\n朝著這方面去思考一且都會變得比較簡單！\n:::\n我們先來看看 OLM 定義了哪些物件給我們使用\nOLM CRD Operator Lifecycle Manager 定義了六個自定義的資源(Custom Resource Definitions,CRD)分別是\n CatalogSource Subscription ClusterServiceVersion (CSV) PackageManifest InstallPlan OperatorGroup  如果手邊有 OpenShift 或是 OKD 環境的朋友可以試試看以下這個指令，可以看到環境中有這六個 CRD 存在。\n1 2 3 4 5 6 7  oc get crd | grep -E \u0026#39;catalogsource|subscription|clusterserviceversion|packagemanifest|installplan|operatorgroup\u0026#39; catalogsourceconfigs.operators.coreos.com 2020-06-20T06:57:36Z catalogsources.operators.coreos.com 2020-06-20T06:58:04Z clusterserviceversions.operators.coreos.com 2020-06-20T06:57:48Z installplans.operators.coreos.com 2020-06-20T06:57:53Z operatorgroups.operators.coreos.com 2020-06-20T06:58:12Z subscriptions.operators.coreos.com 2020-06-20T06:57:57Z   建立了 CRD 需要有相對應的 Controller 來觀察資源的變化，這六個 OLM CRD 被三個 Controller 所管理分別是 catalog-operator , olm-operator 以及packageserver，可以透過下面這個指令觀察到是否存在你的環境中。\n1 2 3 4 5  oc -n openshift-operator-lifecycle-manager get deploy NAME READY UP-TO-DATE AVAILABLE AGE catalog-operator 1/1 1 1 32d olm-operator 1/1 1 1 32d packageserver 2/2 2 2 32d   CRD - CatalogSources 簡單的來說 CatalogSources 就是 Operator 的 Repo Source，收集了四種類型的 Operator。\n分別是\n Certified Operators\n在這個 repo 內的 Operator 都有經過 Redhat 的認證，相關的說明可以從Red Hat OpenShift Operator Certification搜尋到。 Community Operators\n在這個 repo 內的都是社群提供的，各式各樣的 Operator 可以從 Github找到。 Redhat-marketplace\n由 Refhat 與 IBM 共同合作組織給 Enterprise 用的 Repo，相關資料可以查詢 marketplace。 Redhat-operators\n這些 Operator 由 Red Hat 發行  我們可以透過指令去看環境上有沒有這幾項 catalogsources 。\n1 2 3 4 5  NAME DISPLAY TYPE PUBLISHER AGE certified-operators Certified Operators grpc Red Hat 32d community-operators Community Operators grpc Red Hat 32d redhat-marketplace Red Hat Marketplace grpc Red Hat 32d redhat-operators Red Hat Operators grpc Red Hat 32d   有了這些 Repo 後那怎麼查看 Repo 上的 Operator 那就要用到另外一個 CRD packagemanifests 。\nCRD - Packagemanifests 簡單的來說 packagemanifests 就記載了 Operator 部署的相關訊息\n我們可以透過指令觀察上面提到的 catalogsources 裡面有多少個 operator 。\nCommunity Operators 1 2 3 4 5 6 7 8 9 10 11  oc get packagemanifests -l catalog=certified-operators NAME CATALOG AGE cyberarmor-operator-certified Certified Operators 32d kong-offline-operator Certified Operators 32d ibm-spectrum-symphony-operator Certified Operators 32d portshift-operator Certified Operators 32d storageos-1tb Certified Operators 32d joget-openshift-operator Certified Operators 32d aqua-operator-certified Certified Operators 32d node-red-operator-certified Certified Operators 32d ...   Community Operators oc get packagemanifests -l catalog=community-operators NAME CATALOG AGE kiali Community Operators 32d strimzi-kafka-operator Community Operators 32d planetscale Community Operators 32d nsm-operator-registry Community Operators 32d apicurito Community Operators 32d hazelcast-jet-operator Community Operators 32d ... Red Hat Marketplace 1 2 3 4 5 6 7 8  oc get packagemanifests -l catalog=redhat-marketplace NAME CATALOG AGE nxrm-operator-certified-rhmp Red Hat Marketplace 32d enterprise-operator-rhmp Red Hat Marketplace 32d storageos-rhmp Red Hat Marketplace 32d cortex-fabric-operator-rhmp Red Hat Marketplace 32d portshift-operator-rhmp Red Hat Marketplace 32d here-service-operator-certified-rhmp Red Hat Marketplace 32d   Red Hat Operators 1 2 3 4 5 6 7 8 9  oc get packagemanifests -l catalog=redhat-operators NAME CATALOG AGE rhsso-operator Red Hat Operators 32d amq-broker-rhel8 Red Hat Operators 32d advanced-cluster-management Red Hat Operators 32d jaeger-product Red Hat Operators 32d fuse-online Red Hat Operators 32d eap Red Hat Operators 32d kubevirt-hyperconverged Red Hat Operators 32d   這邊拿一個 community-operators 的 prometheus-exporter-operator yaml 簡單的看一下裡面的內容。\n大致上描述了以下幾點\n catalog channels\n主要描述了要管理的Operator的相關訊息，詳細內容會在下一篇做介紹  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124  ocgetpackagemanifestsprometheus-exporter-operator-oyamlapiVersion:packages.operators.coreos.com/v1kind:PackageManifestmetadata:creationTimestamp:\u0026#34;2020-06-20T07:23:23Z\u0026#34;labels:catalog:community-operatorscatalog-namespace:openshift-marketplaceolm-visibility:hiddenopenshift-marketplace:\u0026#34;true\u0026#34;operatorframework.io/arch.amd64:supportedoperatorframework.io/os.linux:supportedopsrc-datastore:\u0026#34;true\u0026#34;opsrc-owner-name:community-operatorsopsrc-owner-namespace:openshift-marketplaceopsrc-provider:communityprovider:RedHatprovider-url:\u0026#34;\u0026#34;name:prometheus-exporter-operatornamespace:defaultselfLink:/apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/prometheus-exporter-operatorspec:{}status:catalogSource:community-operatorscatalogSourceDisplayName:CommunityOperatorscatalogSourceNamespace:openshift-marketplacecatalogSourcePublisher:RedHatchannels:- currentCSV:prometheus-exporter-operator.v0.2.0currentCSVDesc:annotations:alm-examples:|- [{\u0026#34;apiVersion\u0026#34;: \u0026#34;monitoring.3scale.net/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;: \u0026#34;PrometheusExporter\u0026#34;,\u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;example-memcached\u0026#34;},\u0026#34;spec\u0026#34;: {\u0026#34;dbHost\u0026#34;: \u0026#34;your-memcached-host\u0026#34;,\u0026#34;dbPort\u0026#34;: 11211,\u0026#34;grafanaDashboard\u0026#34;: {\u0026#34;label\u0026#34;: {\u0026#34;key\u0026#34;: \u0026#34;autodiscovery\u0026#34;,\u0026#34;value\u0026#34;: \u0026#34;enabled\u0026#34;}},\u0026#34;type\u0026#34;: \u0026#34;memcached\u0026#34;}}]capabilities:DeepInsightscategories:Monitoringcertified:\u0026#34;false\u0026#34;containerImage:quay.io/3scale/prometheus-exporter-operator:v0.2.0createdAt:\u0026#34;2020-06-08 00:00:00\u0026#34;description:Operatortosetup3rdpartyprometheusexporters,withacollectionofgrafanadashboardsrepository:https://github.com/3scale/prometheus-exporter-operatorsupport:RedHat,Inc.apiservicedefinitions:{}customresourcedefinitions:owned:- description:ConfiguresaprometheusexportertomonitoramemcachedinstancedisplayName:PrometheusExporterkind:PrometheusExportername:prometheusexporters.monitoring.3scale.netversion:v1alpha1description:| A Kubernetes Operator based on the Operator SDK to centralize the setup of 3rd party prometheus exporters on **Kubernetes/OpenShift**, with a collection of grafana dashboards.Youcansetupdifferentprometheusexporterstomonitortheinternalsfromdifferentdatabases,orevenanyavailablecloudwatchmetricfromanyAWSService,byjustprovidingafewparameterslike**dbHost**or**dbPort**(operatormanagesthecontainerimage,port,argument,command,volumes...andalsoprometheus**ServiceMonitor**and**GrafanaDashboard**k8sobjects).Current prometheus exporters types supported, managed by same prometheus-exporter-operator:*memcached*redis*mysql*postgresql*sphinx*es(elasticsearch)*cloudwatchThe operator manages the lifecycle of the following objects:*Deployment(oneperCR)*Service(oneperCR)*ServiceMonitor(optional,oneperCR)*GrafanaDashboard(optional,oneperNamespace)### DocumentationDocumentationcanbefoundonour[website](https://github.com/3scale/prometheus-exporter-operator#documentation).### Getting helpIfyouencounteranyissueswhileusingoperator,youcancreateanissueonour[website](https://github.com/3scale/prometheus-exporter-operator)forbugs,enhancements,orotherrequests.### ContributingYou can contribute by:*RaisinganyissuesyoufindusingPrometheusExporterOperator*Fixingissuesbyopening[PullRequests](https://github.com/3scale/prometheus-exporter-operator/pulls)*SubmittingapatchoropeningaPR*Improving[documentation](https://github.com/3scale/prometheus-exporter-operator)*TalkingaboutPrometheusExporterOperatorAllbugs,tasksorenhancementsaretrackedas[GitHubissues](https://github.com/3scale/prometheus-exporter-operator/issues).### LicensePrometheusExporterOperatorislicensedunderthe[Apache2.0license](https://github.com/3scale/prometheus-exporter-operator/blob/master/LICENSE)displayName:PrometheusExporterOperatorinstallModes:- supported:truetype:OwnNamespace- supported:truetype:SingleNamespace- supported:falsetype:MultiNamespace- supported:truetype:AllNamespacesprovider:name:RedHatversion:0.2.0name:alphadefaultChannel:alphapackageName:prometheus-exporter-operatorprovider:name:RedHat  結語 大致上能夠了解 OLM 是用來管理 Operator 的生命週期，加上透過六個自定義的資源( Custom Resource Definitions , CRD )以及三個 Controller去管理整個 Operator 的生態。\n使用者可以透過 CatalogSources 所提供的 Repo 下載各式各樣的 PackageManifest (Operator)。\n下一篇會針對 OLM 剩下的 CRD 進行間單的介紹，以及從 CatalogSources 下載一個 PackageManifest (Operator) 並且部署到環境中。\n","description":"","id":14,"section":"posts","tags":["openshift"],"title":"簡單理解 Operator Lifecycle Manager - 1 (OLM)","uri":"https://blog.jjmengze.website/posts/ocp/operator-lifecycle-manager-1/"},{"content":"  就想看看你對我做什麼壞壞的事 在操作 Kubernetes Cluster 的時候系統管理員會發給各個開發者、使用者又或是 Robot 帳號，這時好多人在操作 Cluster 有沒有什麼方法可以看看誰對 Cluster 做了什麼，到時候炸鍋的時候比較好找兇手（Ｘ）。\n好拉除了找兇手之外還可以除錯計費做很多我目前還想不到的功能，如果有大大看到這篇文章有想到 audit 還可以做什麼可以跟我分享與討論呦！\n我們先看看 Kubernetes 支援哪些 audit 處理的方法。\n Log backend, which writes events to a disk Webhook backend, which sends events to an external API Dynamic backend, which configures webhook backends through an AuditSink API object.  目前 Kubernetes 支援這三種方法處理 audit 資料，下文會三種方法都會使用一次作為範例，在看看實際的範例之前我們可以先來看看 audit 資料可以記錄哪些東西。\n 發生了什麼事情（What 什麼時候發生的（When 誰讓事情發生了（Who  雖然還有紀錄其他可以的東西，不過我認為這三個最為重要，有興趣的朋友可以看看官方的文件。\n我們了解到 Kubernetes audit 會記錄什麼後，接著就是 audit 紀錄什麼時候會被觸發，這邊分為四個階段。\n  RequestReceived\n events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain.（簡單來說就是 apiserver 收到請求的階段）    ResponseStarted\n Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g. watch).(這個階段我不是非常了解，看起來像是 watch 之類的才會觸發這個階段)    ResponseComplete\n The response body has been completed and no more bytes will be sent.（這個階段代表 apiserver 的回應）    Panic\n Events generated when a panic occurred.(發生 panic 才會觸發)    目前我們知道 Kubernetes audit 會記錄事情發生的內容，事情發生的階段。那有可不可以分門別類，例如我只想要紀錄 pods 的事件、 configmaps 的事件 或是某個 user 的行為呢？\n答案是可以的！每當我看完 Kubernetes 的設計真的覺得社群考慮的相當的彈性，繼續項開源專案學習，只有站在巨人的肩膀上才能看得更遠。好廢話不多說，剛剛談到的分門別類在 audit 裡稱為 policy ， policy 分為以下四個級別。\n None  (不要記錄與此規則匹配的事件)   Metadata  (記錄請求的 metadata 例如 user timestamp resource verb e.t.c. 但不記錄請求的內容，有點類似只把 header 紀錄一下來 body 隨它去的感覺。)   Request  (記錄整個事件請求的內容， body 跟 header 都要存的意思)   RequestResponse  (記錄整個事件回應的內容， body 跟 header 都要存的意思)    官方有給出一個範例讓我們可以依照自己的需求進行修改，範例如下所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  apiVersion:audit.k8s.io/v1# This is required.kind:Policy# Don\u0026#39;t generate audit events for all requests in RequestReceived stage.omitStages:- \u0026#34;RequestReceived\u0026#34;rules:# Log pod changes at RequestResponse level- level:RequestResponseresources:- group:\u0026#34;\u0026#34;# Resource \u0026#34;pods\u0026#34; doesn\u0026#39;t match requests to any subresource of pods,# which is consistent with the RBAC policy.resources:[\u0026#34;pods\u0026#34;]# Log \u0026#34;pods/log\u0026#34;, \u0026#34;pods/status\u0026#34; at Metadata level- level:Metadataresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;pods/log\u0026#34;,\u0026#34;pods/status\u0026#34;]# Don\u0026#39;t log requests to a configmap called \u0026#34;controller-leader\u0026#34;- level:Noneresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;controller-leader\u0026#34;]# Don\u0026#39;t log watch requests by the \u0026#34;system:kube-proxy\u0026#34; on endpoints or services- level:Noneusers:[\u0026#34;system:kube-proxy\u0026#34;]verbs:[\u0026#34;watch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;endpoints\u0026#34;,\u0026#34;services\u0026#34;]# Don\u0026#39;t log authenticated requests to certain non-resource URL paths.- level:NoneuserGroups:[\u0026#34;system:authenticated\u0026#34;]nonResourceURLs:- \u0026#34;/api*\u0026#34;# Wildcard matching.- \u0026#34;/version\u0026#34;# Log the request body of configmap changes in kube-system.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;configmaps\u0026#34;]# This rule only applies to resources in the \u0026#34;kube-system\u0026#34; namespace.# The empty string \u0026#34;\u0026#34; can be used to select non-namespaced resources.namespaces:[\u0026#34;kube-system\u0026#34;]# Log configmap and secret changes in all other namespaces at the Metadata level.- level:Metadataresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;secrets\u0026#34;,\u0026#34;configmaps\u0026#34;]# Log all other resources in core and extensions at the Request level.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API group- group:\u0026#34;extensions\u0026#34;# Version of group should NOT be included.# A catch-all rule to log all other requests at the Metadata level.- level:Metadata# Long-running requests like watches that fall under this rule will not# generate an audit event in RequestReceived.omitStages:- \u0026#34;RequestReceived\u0026#34;  上面大致上瞭解了 Kuberetes audit 的大方向如，當 event 發生的時候他會記錄什麼，有哪幾個 stage 會觸發 event 以及 event 可以看照 policy 分門別類。\n了解這幾項東西之後就可以來進行實驗，我會透過 kubeadm 建立一個 all in one 的 kubernetes 測試 audit 紀錄在 disk 上以及 透過 webhook 的方式把 event 傳出來進行額外的處理。\nKubeadm 安裝測試環境 kubeadm 安裝方法這邊不多談 Google 有很多安裝的方法，在本次實驗需要設定 audit 的相關參數，kubeadm 目前還沒有完全支援 audit 的 feature gate 除了設定 kubeadm config 之外還需要手動設定部分參數。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  apiVersion:kubeadm.k8s.io/v1beta2kind:ClusterConfiguration#featureGates:# not support DynamicAuditing# DynamicAuditing: trueapiServer:extraArgs:audit-log-path:/home/ubuntu/audit.logaudit-policy-file:/etc/kubernetes/addon/audit-policy.yaml# not support DynamicAuditing# runtime-config=auditregistration.k8s.io/v1alpha1: \u0026#34;true\u0026#34;# audit-dynamic-configuration:extraVolumes:- name:audithostPath:/etc/kubernetes/addon/audit-policy.yamlmountPath:/etc/kubernetes/addon/audit-policy.yamlreadOnly:truepathType:File- name:audit-loghostPath:/home/ubuntumountPath:/home/ubuntupathType:DirectoryOrCreate  1 2 3 4 5 6  cat \u0026lt;\u0026lt;EOF | \u0026gt;/etc/kubernetes/addon/audit-policy.yaml # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: Metadata   可以透過上述這個 kubeadm config 去設定部分的 audit 參數，再由 kubeadm init 啟動的時候將它自動地帶入 kube-apiserver中。\n1 2 3 4 5 6  kubeadm init --config admin-apiserver.yml W0629 07:46:16.046453 86387 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.5 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster ...   由於本次測試環境只有一個節點所以需要進行untaint的動作，讓 pod 可以在 master node 上開啟。\nkubectl taint node jason-test node-role.kubernetes.io/master- 當 kubeadm 執行完也設定完 CNI 後需要修改/etc/kubernetes/manifests/kube-apiserver 相關參數，讓 apiserver 支援 audit-dynamic-configuration 需要加入以下參數。\n1 2 3  - --audit-dynamic-configuration- --feature-gates=DynamicAuditing=true- --runtime-config=auditregistration.k8s.io/v1alpha1=true  修改完成後，可以透過 kubectl 指令檢查是否開啟 auditregistration.k8s.io/v1alpha1 的資源。\n1 2  kubectl api-resources | grep AuditSink auditsinks auditregistration.k8s.io false AuditSin   完成以上步驟後就完成了 kubernetes audit 的測試環境搭建。\nLog Backend 事實上剛剛的流程已經把 Backend 設定好了，位置就在 audit-log-path: /home/ubuntu/audit.log ，只要觸發 audit 就會把紀錄儲存在這個位置上。那我們來看看設定完 Kubernetes 之後會儲存哪寫資料。\n1 2 3 4  tail -f audit.log {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;caa00b7a-e564-486c-837f-219eade633dd\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;RequestReceived\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-controller-manager\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-controller-manager\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;} {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;caa00b7a-e564-486c-837f-219eade633dd\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-controller-manager\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.2.4\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-controller-manager\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.083898Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-07-04T15:19:21.085030Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;system:kube-controller-manager\\\u0026#34; of ClusterRole \\\u0026#34;system:kube-controller-manager\\\u0026#34; to User \\\u0026#34;system:kube-controller-manager\\\u0026#34;\u0026#34;}} ...   這邊會看到許多觸發 audit 後記錄的訊息，這邊需要注意的地方有 requestURI 、 verb 、 user 、 sourceIPs 、 userAgent。\n我先執行一個簡單的 kubectl 指令來觀察 audit 有沒有攔截到這個請求。\n1  kubectl get pod   再去看 audit 所記錄的 log 有沒有儲存這個操作\ncat /home/ubuntu/audit/log | grep {\u0026quot;kind\u0026quot;:\u0026quot;Event\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;audit.k8s.io/v1\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;Metadata\u0026quot;,\u0026quot;auditID\u0026quot;:\u0026quot;0595bb26-d8a2-4b59-b3d1-7d538c2e131f\u0026quot;,\u0026quot;stage\u0026quot;:\u0026quot;RequestReceived\u0026quot;,\u0026quot;requestURI\u0026quot;:\u0026quot;/api/v1/namespaces/default/pods?limit=500\u0026quot;,\u0026quot;verb\u0026quot;:\u0026quot;list\u0026quot;,\u0026quot;user\u0026quot;:{\u0026quot;username\u0026quot;:\u0026quot;kubernetes-admin\u0026quot;,\u0026quot;groups\u0026quot;:[\u0026quot;system:masters\u0026quot;,\u0026quot;system:authenticated\u0026quot;]},\u0026quot;sourceIPs\u0026quot;:[\u0026quot;10.0.2.4\u0026quot;],\u0026quot;userAgent\u0026quot;:\u0026quot;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026quot;,\u0026quot;objectRef\u0026quot;:{\u0026quot;resource\u0026quot;:\u0026quot;pods\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;requestReceivedTimestamp\u0026quot;:\u0026quot;2020-07-04T15:26:20.895927Z\u0026quot;,\u0026quot;stageTimestamp\u0026quot;:\u0026quot;2020-07-04T15:26:20.895927Z\u0026quot;} {\u0026quot;kind\u0026quot;:\u0026quot;Event\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;audit.k8s.io/v1\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;Metadata\u0026quot;,\u0026quot;auditID\u0026quot;:\u0026quot;0595bb26-d8a2-4b59-b3d1-7d538c2e131f\u0026quot;,\u0026quot;stage\u0026quot;:\u0026quot;ResponseComplete\u0026quot;,\u0026quot;requestURI\u0026quot;:\u0026quot;/api/v1/namespaces/default/pods?limit=500\u0026quot;,\u0026quot;verb\u0026quot;:\u0026quot;list\u0026quot;,\u0026quot;user\u0026quot;:{\u0026quot;username\u0026quot;:\u0026quot;kubernetes-admin\u0026quot;,\u0026quot;groups\u0026quot;:[\u0026quot;system:masters\u0026quot;,\u0026quot;system:authenticated\u0026quot;]},\u0026quot;sourceIPs\u0026quot;:[\u0026quot;10.0.2.4\u0026quot;],\u0026quot;userAgent\u0026quot;:\u0026quot;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026quot;,\u0026quot;objectRef\u0026quot;:{\u0026quot;resource\u0026quot;:\u0026quot;pods\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;responseStatus\u0026quot;:{\u0026quot;metadata\u0026quot;:{},\u0026quot;code\u0026quot;:200},\u0026quot;requestReceivedTimestamp\u0026quot;:\u0026quot;2020-07-04T15:26:20.895927Z\u0026quot;,\u0026quot;stageTimestamp\u0026quot;:\u0026quot;2020-07-04T15:26:20.897187Z\u0026quot;,\u0026quot;annotations\u0026quot;:{\u0026quot;authorization.k8s.io/decision\u0026quot;:\u0026quot;allow\u0026quot;,\u0026quot;authorization.k8s.io/reason\u0026quot;:\u0026quot;\u0026quot;}} 整理 audit log 的資訊得到以下重點\n \u0026ldquo;requestURI\u0026rdquo;:\u0026quot;/api/v1/namespaces/default/pods?limit=500\u0026rdquo; \u0026ldquo;verb\u0026rdquo;:\u0026ldquo;list\u0026rdquo; \u0026ldquo;user\u0026rdquo;:{\u0026ldquo;username\u0026rdquo;:\u0026ldquo;kubernetes-admin\u0026rdquo;,\u0026ldquo;groups\u0026rdquo;:[\u0026ldquo;system:masters\u0026rdquo;,\u0026ldquo;system:authenticated\u0026rdquo;]} \u0026ldquo;sourceIPs\u0026rdquo;:[\u0026ldquo;10.0.2.4\u0026rdquo;] \u0026ldquo;userAgent\u0026rdquo;:\u0026ldquo;kubectl/v1.18.5 (linux/amd64) kubernetes/e6503f8\u0026rdquo;  可以清楚看到 kubernetes-admin 這個 user 執行了這個操作 ，操作是 /api/v1/namespaces/default/pods?limit=500 執行的動作是 list ，這個操作是來自 10.0.2.4 的 kubectl。\n看到以上這些重點之後，大致上可以推測初使用者在 10.0.2.4 透過 kubectl執行get pod --namespace default的動作。\nDynamic Backend 接著讓我們來看看 Dynamic Backend 是怎麼一回事，可以通過 AuditSink API 設定相對應的 Webhook 對資料進行預處理再送往其他地方。\n預設的 kubernetes 不會幫你設定 AuditSink API ，需要手動去開啟。在前面的章節已經有先開啟了，這邊我們可以回頭去看一下設定了哪些東西。\n \u0026ndash;audit-dynamic-configuration \u0026ndash;feature-gates=DynamicAuditing=true \u0026ndash;runtime-config=auditregistration.k8s.io/v1alpha1=true  這三個設定在 /etc/kubernetes/manifests/kube-apiserver.yaml 裡可以看到，還沒有修改的小夥伴可以現在加上去。\n確定完設定檔後我們可以透過 kubectl 去檢查這個資源是不是被開啟。\n1 2  kubectl api-resources | grep AuditSink auditsinks auditregistration.k8s.io false AuditSin   接著我們要去部署一個 webhook server 讓 kubernetes audit 可以把資料送上來，由於要建立 webhook 需要撰寫 code 以及需要設定 CA 這邊我提供實驗的 repo 給大家試試看。\n使用上非常簡單，只要透過 kubectl apply -f pod.yml -f service.yml這樣就把 webhook 建立好了。\n1 2 3 4 5 6 7 8 9 10 11 12  k apply -f pod.yml -f service.yml pod/webhook created service/admissionwebhook created ... k get pod,svc NAME READY STATUS RESTARTS AGE pod/webhook 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/admissionwebhook ClusterIP 10.97.173.35 \u0026lt;none\u0026gt; 443/TCP 7s service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2d14h   建立好 webhook pod 後我們需要建立 AuditSink 物件讓 audit 事件把資料往這個 webhook 送，\n1 2  kubectl apply -f auditSink.yml auditsink.auditregistration.k8s.io/mysink created   這一個 auditSink.yml 描述了，限定 audit 觸發的事件以及要往哪個地方送資料。\napiVersion: auditregistration.k8s.io/v1alpha1 kind: AuditSink metadata: name: mysink spec: policy: level: Metadata stages: - RequestReceived webhook: clientConfig: service: namespace: default name: admissionwebhook path: /sink port: 443 caBundle: \u0026quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2RENDQWFRQ0NRRHVXMXNYUVhqUjdEQU5CZ2txaGtpRzl3MEJBUXNGQURBZk1SMHdHd1lEVlFRRERCUkIKWkcxcGMzTnBiMjRnVjJWaWFHOXZheUJEUVRBZ0Z3MHlNREEzTURjd05UQTNOVEJhR0E4ek1ERTVNVEV3T0RBMQpNRGMxTUZvd0h6RWRNQnNHQTFVRUF3d1VRV1J0YVhOemFXOXVJRmRsWW1odmIyc2dRMEV3Z2dFaU1BMEdDU3FHClNJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUMvWjZ1U1UrNlgrR3htN0NuYXR0RXBvUTN0VjJIb01TNGcKeCtGdEV4Q1Nqb0FjTk5wdHI1cjdTVjVIck1zS09wNDhrZFQ5NVI0YytWZUFnOGlqbVdpSEdQSXFyb1dIRys5cQpibzhoM0FZWk9MeWxQVjBUVWVSb0hQSkRmRmZsV054WlFnRHRXa2p0c2VRUVdsVisyVE5KVXF1cGtBMUMvZTJWCnVEckVkRzZTRFZaWDZMYTdTd3ViOXA4UnVaY21TMURrWlB3bFBmMEt2UVp5UHlMUXl4TVRjeVdJM3ZnNzlENWsKcTJPNFB0N080bm9MM0YzRGRyMHU3aGZwQjlJUUFUelZnTWRvQjNPRmV1NjRTZVRydmdmeG1XK1FZNFA0Z05aRQova21TTnNmaklnT202VnF1eEZHTEpsdUE3WlJoQkdadG1ON1N6dktTb21OZjlhTHJkWERSQWdNQkFBRXdEUVlKCktvWklodmNOQVFFTEJRQURnZ0VCQUtpMDBFRUZBdTFKL1M0ejBpVTVUYlJvOXI1WjRTZk5Ub3ZhS1U4bHA3YTAKaUUwWXRSVUdSMjhkRjRrRE12OXp4dTRCYy82N0ptb3g0SGtZMTFIU0RtOXVUUUs0T1dHMGo5MnIvOGY2RlRZMQpNSk1VNUJ0dy90Ym5hV2ZNWE5Xa0R5TnVhQXA1U3hMV1luODE4OWNqM0Zyd2NIL1VoODl4WFhDQnpzQUNOZGNiClRiN3ZKdnBGdWgyOXpNWUJGNTBPNHNnTi9UMWhNbTk3Y2xBbU9OZ1JoSTQ3cVdDamtlNlJKb2I0MnF6Ri8yK0gKK3k3eWlqQktkU2pQOVJOS3VreXM5VlY4eEN5TlpTSUFGYzM1dldMUDFLUEY1aFVTMWo1c0o2V3JncjdGVDU2ZQpZMC9rdGZkcWpSNXJVbWhzUW9GeHhJMzFHY1hjYktWTFNoRldtS2pMMERvPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\u0026quot; 這時候有觸發 audit 的事件就會往 webhook service送，可以透過 kubectl 指令去確認相關 log 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  k log -f webhook ... I0707 05:47:44.205612 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:4b6cb6dd-456b-4b77-9e63-90634a6d93eb Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=10s Verb:update User:{Username:system:kube-controller-manager UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335980 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:42.212444 +0000 UTC StageTimestamp:2020-07-07 05:47:42.212444 +0000 UTC Annotations:map[]} I0707 05:47:44.206093 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:3d52b965-d65f-4060-b839-8e5ffe874b7f Stage:RequestReceived RequestURI:/api/v1/persistentvolumeclaims?allowWatchBookmarks=true\u0026amp;resourceVersion=492314\u0026amp;timeout=8m47s\u0026amp;timeoutSeconds=527\u0026amp;watch=true Verb:watch User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/scheduler ObjectRef:0xc000335a80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:43.363791 +0000 UTC StageTimestamp:2020-07-07 05:47:43.363791 +0000 UTC Annotations:map[]} I0707 05:47:44.206530 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:d1a20f9d-0204-43ae-8ab1-82b2c09858ed Stage:RequestReceived RequestURI:/apis/apps/v1/replicasets?allowWatchBookmarks=true\u0026amp;resourceVersion=492314\u0026amp;timeout=5m20s\u0026amp;timeoutSeconds=320\u0026amp;watch=true Verb:watch User:{Username:system:kube-controller-manager UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-controller-manager/v1.18.5 (linux/amd64) kubernetes/e6503f8/shared-informers ObjectRef:0xc000335b80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:43.610833 +0000 UTC StageTimestamp:2020-07-07 05:47:43.610833 +0000 UTC Annotations:map[]} I0707 05:47:44.206956 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:5d48dc35-f089-4e76-a2f4-098e5628b055 Stage:RequestReceived RequestURI:/api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335c80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.024424 +0000 UTC StageTimestamp:2020-07-07 05:47:44.024424 +0000 UTC Annotations:map[]} I0707 05:47:44.207483 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:e2b5f015-d0e0-48ab-85a8-c9a42121cdfd Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335d80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.026864 +0000 UTC StageTimestamp:2020-07-07 05:47:44.026864 +0000 UTC Annotations:map[]} I0707 05:47:44.207995 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:0070ecce-3119-4e44-850d-5914e45dcde6 Stage:RequestReceived RequestURI:/api/v1/namespaces/kube-system/endpoints/kube-scheduler?timeout=10s Verb:update User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335e80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.028928 +0000 UTC StageTimestamp:2020-07-07 05:47:44.028928 +0000 UTC Annotations:map[]} I0707 05:47:44.208582 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:ed450b92-27cc-449d-95cf-c2822fd0e380 Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:get User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000335f80 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.032618 +0000 UTC StageTimestamp:2020-07-07 05:47:44.032618 +0000 UTC Annotations:map[]} I0707 05:47:44.209296 1 main.go:34] this event is {TypeMeta:{Kind: APIVersion:} Level:Metadata AuditID:9deb9b72-4a27-4a36-b6f8-74e976aee3cf Stage:RequestReceived RequestURI:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-scheduler?timeout=10s Verb:update User:{Username:system:kube-scheduler UID: Groups:[system:authenticated] Extra:map[]} ImpersonatedUser:nil SourceIPs:[10.0.2.4] UserAgent:kube-scheduler/v1.18.5 (linux/amd64) kubernetes/e6503f8/leader-election ObjectRef:0xc000126080 ResponseStatus:nil RequestObject:nil ResponseObject:nil RequestReceivedTimestamp:2020-07-07 05:47:44.034645 +0000 UTC StageTimestamp:2020-07-07 05:47:44.034645 +0000 UTC Annotations:map[]} [GIN] 2020/07/07 - 05:47:44 | 200 | 105.108µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; [GIN] 2020/07/07 - 05:47:44 | 200 | 97.207µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; [GIN] 2020/07/07 - 05:47:44 | 200 | 150.111µs | 10.32.0.1 | POST \u0026#34;/sink?timeout=30s\u0026#34; ...   可以看到 webhook 的 log 已經收到許多 audit 事件的資料，我們可以去修改 webhook 的程式碼讓 webhook 收到資料後進行預處理後再往其他地方發送例如：做Alert 。\n後記 目前大多數看到的解決方案是直接採用 Log Backend ，把資料直接記載在 master node 上再透過 logstash or fluentd 直接把資料過濾並且丟到 Elasticsearch ，可以利用 audit 紀錄使用者行為看到 cluster 內發生什麼事情。\n","description":"","id":15,"section":"posts","tags":["kubernetes"],"title":"kubernetes Audit 查起來","uri":"https://blog.jjmengze.website/posts/kubernetes/audit/"},{"content":"  前面說了一些基礎概念，本篇文章就實際來動手做做看用 OpenSSL 、 Nginx 加上 GO 開發一個簡單的 TLS 範例。\n建立 CA key 由於我們要自己簽 SSL 所以需要透過 OpenSSL 先建立一個 CA 的私鑰。\n1 2 3 4 5 6 7 8  openssl genrsa -out ca.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ........+++++ ......+++++ e is 65537 (0x010001) root@internalTest:/home/ca# ls ca.key   建立 CA CRT 建立完 CA 的私鑰後，我們還需要建立 CA 的身分證 CA CRT ，這個身分證就代表 CA 公開出去的所有人都看得到。\n1 2 3 4 5  openssl req -x509 -new -nodes -key ca.key -days 10000 -out ca.crt -subj \u0026#34;/CN=playwithca\u0026#34; root@internalTest:/home/ca# ls ca.key ca.crt   建立 Server Key 建立 server 的私鑰 ， 用來給 nginx server 的 TLS private key。\n1 2 3 4 5 6  openssl genrsa -out server.key 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ...................+++++ .............................................................................................+++++ e is 65537 (0x010001)   Server CSR 建立一個 CSR 用來向 CA 說請幫我簽名，簽的 Domain 是 example.com 。\n可以想像你去戶政事務所填寫申請身分證的表單\n1  openssl req -new -key server.key -out server.csr -subj \u0026#34;/CN=example.com\u0026#34;   Server CRT 透過剛剛產出的 CSR 給 CA 簽名，這邊一般來說是用買的，你給 CA 商 CSR 他會幫你簽名出一個 CRT ， 這個 CRT 就是這個 server 的身分證。\n從你填寫表單的內容戶政事務所發給你對應的身分證\n1 2 3 4 5 6 7 8 9  openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 -out server.crt -days 365 Signature ok subject=CN = example.com Getting CA Private Key root@internalTest:/home/ca# ls ca.crt ca.key ca.srl server.crt server.csr server.key   這邊簡單設定一下 nginx 的 SSL ，讓 nginx 可以使用指定的證書，並且開啟 443 port 提供其他人連線。\nnginx config ... server { listen 443 ssl default_server; listen [::]:443 ssl default_server; ssl_certificate /home/ca/server.crt; ssl_certificate_key /home/ca/server.key; } ... 設定好 nginx 別忘了重新載入 nginx 設定檔（題外話我覺得 nginx graceful reload做得滿好的，有興趣的朋友可以去研究一下）\n1  nginx -s reload   由於是測試環境沒有實際的 domain ，就先以修改 hostname 作為替代方案。\n1 2 3 4 5 6 7 8 9  cat /etc/hosts ## # Host Database # # localhost is used to configure the loopback interface # when the system is booting. Do not change this entry. ## 52.163.243.191 example.com ...   修改完成後測試一下能不能透過 https 連線到 nginx 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  curl -k https://example.com \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully i ...   可以順利連線到，那接著可以撰寫一個簡單的 Client 去打打看 Server 囉！\nSample Client 這邊會以一個單向 TLS 認證作為示範，單向 TLS 認證大致上的動作是， Client 向 Server 發起請求。這時候 Client 會拿到 Server 的 證書( crt ) ，Client 會拿著這組 Server crt 跟 CA 的 crt 進行驗證 ，看 Server 是不是被 CA 授權的還是假冒的。\n如果是假冒的那麼 Client 會拒絕連線，反之 Server 若是被 CA 認證的那 Client 就會繼續往 Server 發請求。\n可以想得簡單一點，今天所有人來向你買東西，看到你身分證都馬上拿去問戶政事務所問這個身分證是不是合法的公民，如果不是合法的身分證就不進行交易。\n以下是一個用 go 開發的單向 TLS 請求的 Client 。\n1 2 3 4 5 6 7 8  tree /go/src/dev/ . ├── 2.0 │ ├── client.crt │ ├── client.key │ └── root-ca.crt └── main.go   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;io\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { tr := \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ RootCAs: loadCA(\u0026#34;/Users/jason/go/src/dev/example-tls/2.0/root-ca.crt\u0026#34;), }, } c := \u0026amp;http.Client{Transport: tr} if resp, e := c.Get(\u0026#34;https://example.com\u0026#34;); e != nil { log.Fatal(\u0026#34;http.Client.Get: \u0026#34;, e) } else { defer resp.Body.Close() io.Copy(os.Stdout, resp.Body) } } func loadCA(caFile string) *x509.CertPool { pool := x509.NewCertPool() if ca, e := ioutil.ReadFile(caFile); e != nil { log.Fatal(\u0026#34;ReadFile: \u0026#34;, e) } else { pool.AppendCertsFromPEM(ca) } return pool }   以下是單向 TLS 請求的 Client 向 Server 請求的結果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  go run main.go \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; ...   結束語 這篇文章分享了如何用 nginx 、 OpenSSL 以及 Go 做一個自簽的 TLS 服務，雖然是一個簡單的範例但我們可以從中瞭解單向 TLS ， Client 只要向 CA 驗證 Server 的身份即可，那這樣安全嗎？這個討論我會保留到下一章再來跟大家分享。\n","description":"","id":16,"section":"posts","tags":["ci/cd"],"title":"CA 動手玩","uri":"https://blog.jjmengze.website/posts/ca/ca-one-way-ssl/"},{"content":"  前面有一篇文章，在描述 Github Action 做 CI/CD 的好處，對於 Github Action 還不太了解的朋友可以參考Github Action 起步走第一式了解一下最基礎的東西怎麼用吧～\n在 CI/CD 總有一個階段會需要啊我們應應用服務部署到 Server 上提供對應的服務，對於初期快速發展的公司來說 Heroku 是一個滿好用的平台，可以直接把 source code push 上去， Heroku 會提供免費帳戶每個月 450 個小時的，開通信用卡，還會額外增加 550 個小時的免費時數，此外 Heroku 會幫你設定相關的 Url 讓外部可以讀取你的服務，這讓開發人員不用擔心 infrastructure 的問題，可以更加專注在 application 的開發。\n這篇文章主要是一個小範例，我們可以在 Github Action 中嵌入一個 Job 讓服務部署到 Heroku上～\n申請 Heroku 帳號 首先我們要去申請 Heroku 帳號 ，這邊非常簡單把他打米字號的欄位填一填寫一寫就沒問題了～\n  完成這個完成這個申請帳號的步驟之後，接著開一個 Heroku 的應用程式，透過介面上的 New 的 Create new app 建立一個應用程式，需要注意的是App name 只能用小寫的英文，而且是沒有被使用過的，基本上這樣就完成了跟在 Github 上建立一個 Repository 一樣簡單。\n  安裝 Heroku CLI 安裝 Heroku CLI 這個部分也相當的簡單安裝過程可以參考官方的教學，我這邊以 MACOS 作為示範有其他作業系統需求的請麻煩請參考官網囉～\nbrew tap heroku/brew \u0026amp;\u0026amp; brew install heroku Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 3 taps (homebrew/core, homebrew/cask and homebrew/cask-fonts). ... ... ==\u0026gt; Installing dependencies for heroku/brew/heroku: heroku/brew/heroku-node ==\u0026gt; Installing heroku/brew/heroku dependency: heroku/brew/heroku-node 🍺 /usr/local/Cellar/heroku-node/12.16.2: 3 files, 42.2MB, built in 3 seconds ==\u0026gt; Installing heroku/brew/heroku ... 測試一下 Heroku CLI 是不是已經裝好了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  heroku CLI to interact with Heroku VERSION heroku/7.41.1 darwin-x64 node-v12.16.2 USAGE $ heroku [COMMAND] COMMANDS access manage user access to apps addons tools and services for developing, extending, and operating your app apps manage apps on Heroku auth check 2fa status authorizations OAuth authorizations autocomplete display autocomplete installation instructions buildpacks scripts used to compile apps certs a topic for the ssl plugin ci run an application test suite on Heroku clients OAuth clients on the platform config environment variables of apps container Use containers to build and deploy Heroku apps domains custom domains for apps drains forward logs to syslog or HTTPS features add/remove app features git manage local git repository for app help display help for heroku keys add/remove account ssh keys labs add/remove experimental features local run Heroku app locally logs display recent log output maintenance enable/disable access to app members manage organization members notifications display notifications orgs manage organizations pg manage postgresql databases pipelines manage pipelines plugins list installed plugins ps Client tools for Heroku Exec psql open a psql shell to the database redis manage heroku redis instances regions list available regions for deployment releases display the releases for an app reviewapps manage reviewapps in pipelines run run a one-off process inside a Heroku dyno sessions OAuth sessions spaces manage heroku private spaces status status of the Heroku platform teams manage teams update update the Heroku CLI webhooks list webhooks on an app   測試 Heroku 這邊以一個簡單的 go server 作為範例，程式碼如下所示。當我們對這個 server 發起請求他會回應你 現在 server 是使用哪一個 port 跟你做溝通。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  package main import ( \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html; charset=utf-8\u0026#34;) p:=os.Getenv(\u0026#34;PORT\u0026#34;) w.Write([]byte(p)) }) http.ListenAndServe(\u0026#34;0.0.0.0:\u0026#34;+os.Getenv(\u0026#34;PORT\u0026#34;), nil) }   透過 Heroku CLI 登入 Heroku ，驗證完身份後，就能夠透過 Git 把source code 推到 Heroku 上囉～\n1 2 3 4 5 6  heroku login heroku: Press any key to open up the browser to login or q to exit: Opening browser to https://cli-auth.heroku.com/auth/cli/browser/5e83cd9d-51cc-4676-b0be-d240a1f5c480 Logging in... done ...   接著設定 heroku git 把git remote url 設定好\nheroku git:remote -a example-githubaction set git remote heroku to https://git.heroku.com/example-githubaction.git 先看看 Heroku 有沒有加到 git remote url\n1 2 3 4 5  git remote -v heroku\thttps://git.heroku.com/example-githubaction.git (fetch) heroku\thttps://git.heroku.com/example-githubaction.git (push) origin\thttps://github.com/jjmengze/heroku-go.git (fetch) origin\thttps://github.com/jjmengze/heroku-go.git (push)   最後透過git push 指令將source code上傳到 Heroku\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  git push heroku master Enumerating objects: 10, done. Counting objects: 100% (10/10), done. Delta compression using up to 8 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (10/10), 1.25 KiB | 640.00 KiB/s, done. Total 10 (delta 0), reused 0 (delta 0) remote: Compressing source files... done. remote: Building source: remote: remote: -----\u0026gt; Go app detected ... ... remote: -----\u0026gt; Launching... remote: Released v3 remote: https://example-githubaction.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/example-githubaction.git * [new branch] master -\u0026gt; master   完成後回到之前在 Heroku 上建立的應用程式，並且在上方的選項點選 Settings\n  最下方 Heroku 會告訴你這個應用程式的 Domain ，一般來說會是 https://\u0026lt;appname\u0026gt;.herokuapp.com/，所以以本篇的例子會是 https://example-githubaction.herokuapp.com/ 。到頁面下方檢查一下 Domain是不是這個～\n  可以請求一下這個 domain 看看得到的結果是什麼\n1 2  curl https://example-githubaction.herokuapp.com/ 59048   表示目前 server 是用 59048 的 port 跟你進行溝通，這邊我就想到壞壞的事情惹，那我能不能用 Heroku 提供的服務 在裡面啟用一個 sshd ，另類 VM 的概念xD (不過那又是另外一個故事了)\n改到 Github Action 既然純手動 ( manually setup ) 可以完成，那我相信透過 CI/CD 自動化機制也可以完成！\n這邊可以到 Github Marketplace 去找找有沒有人做相關的 Action ，好的工程師第一步要學會怎麼找資料 xD\n這邊可以看到有不少 Action 都有在做 HeroKu 相關的部署，可以參考人家怎麼實作在試試看能不能用更簡單的方法做一個出來。\n  這邊我直接給出一個方案所有程式碼與內容可以參考我在 Github 的範例專案，有興趣的可以試試看，基本上的思路是透過 Heroku 提供的 container 服務直接把 source code 包成 container 並且上傳到 heroku image registry 上，相關的部署以及詳細動作可以參考官方的範例\n這邊我直接上github action 整合Heroku container image 後的 ci yaml給大家參考，大家也可以依照個做變化做成自己想要的樣子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  push:name:Pushruns-on:ubuntu-lateststeps:- name:CheckoutcodeintotheGomoduledirectoryuses:actions/checkout@v2- name:LogintoHerokuContainerregistryenv:HEROKU_API_KEY:${{secrets.HEROKU_API_KEY}}run:herokucontainer:login- name:Buildandpushenv:HEROKU_API_KEY:${{secrets.HEROKU_API_KEY}}run:herokucontainer:push-a${{env.APP_NAME}}web- name:Releaseenv:HEROKU_API_KEY:${{secrets.HEROKU_API_KEY}}run:herokucontainer:release-a${{env.APP_NAME}}web  其中可以看到 push 這個 stage 執行了四個 steps ，比較要關注的是後面那三個。分別是執行\n heroku container:login heroku container:push -a ${{ env.APP_NAME }} web heroku container:release -a ${{ env.APP_NAME }} web  這三部分別對應的是\n 登入 heroku 構建 Container image 並將其推送到 Heroku Container Registry 將 Heroku Container Registry 上的 Container image 進行部署與發佈  HEROKU_API_KEY 以及 APP_NAME 都可以在 Github 的 CI/CD 服務直接做環境變數的編輯，透過這一種方式我們可以很簡單的把服務部署到Heroku上面。開發人員可以快速的部署與測試自己的服務～ CI/CD 帶來的好處多多，希望大家可以多多嘗試。\n結束語 今天非常簡單的介紹了 Heroku 與 Github Action 的整合的功能，以及怎麼快速得使用 Heroku 給外部做存取，下次有機會再來介紹他跟docker怎麼整合以及怎麼撰寫自己的 action 貢獻給 Github Action Marketplace 讓大家來使用。\n","description":"","id":17,"section":"posts","tags":null,"title":"Github Action 妳各位，注意 Heroku ！","uri":"https://blog.jjmengze.website/posts/cicd/github/github-action-2/"},{"content":"  記錄一下之前在工作上遇到的 Gitlab domain 沒有簽憑證，但是外部服務(Azure Kubernetes Service)又需要存取 Gitlab Container Registry 會遇到的問題。\n發生了什麼？   Gitlab 的 domain 沒有買 SSL （由於政策問題也不能用 letsencrypt)\n  Kubernetes 拉取 Gitlab Container Registry 需要帳密\n  Kubernetes 拉取 Gitlab Container Registry 憑證不被信任\n  檢視各項問題 Gitlab domain 沒有ssl   如果我們在 host 上透過 docker 拉取該 Container Registry 的 image 可能會出現的狀況。\n先看一下我使用的 docker 測試環境\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  $docker version Client: Version: 19.03.6 API version: 1.40 Go version: go1.12.17 Git commit: 369ce74a3c Built: Fri Feb 28 23:45:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.6 API version: 1.40 (minimum version 1.12) Go version: go1.12.17 Git commit: 369ce74a3c Built: Wed Feb 19 01:06:16 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu1~18.04.2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit:   接著我透過 Docker CLI 去拉 Gitlab Container Registry 上的 private image，由於\u0026hellip;一些公司政策問題我把公司的位置替換掉了請見諒。\n1 2  $docker pull registry-gitlab.com.tw/repo/image-cli:v4.4.0 Error response from daemon: Get registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates   直接垃取會出現 x509: certificate is not authorized to sign other certificates，這個部分非常容易解決 Google 一下就可以很快的拿到解，只要docker login 一下就沒問題了。\n在那之前我們先來看一下相關的 docker 設定檔\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  $cat /lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from \u0026#34;Service\u0026#34; to \u0026#34;Unit\u0026#34; in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process [Install] WantedBy=multi-user.target   上面可以看到 Docker systemD 的相關設定 ，看起來都相當的簡單而且沒有任何繞過 insecure 的選項。\n也同時在 /etc 底下看看 Docker 的設定\n1 2  $ls /etc/docker/ key.json   這邊我們可以看到 Docker 完全沒有設定任何繞過 insecure 的選項後，使用docker login 後再看看有沒有任何的改變。\n1 2 3 4  docker login registry-gitlab.com.tw Username (test): Password: Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates   這時候我們又觀察到新的問題了，在 login 的時候發現我們的Gitlab Registry 的憑證有問題，在繼續 Google 會看到原來要把憑證有問題的 domain 設定到 /etc/docker/daemon.json 下面，這邊我很快的設定起來，並且重啟dockerd。\n1 2 3 4 5  cat \u0026lt;\u0026lt;EOF | \u0026gt;\u0026gt; /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;:[\u0026#34;registry-gitlab.com.tw\u0026#34;] } systemctl restart docker   接著再嘗試透過docker login 登入並且拉取 Gitlab container registry 的 image。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  $docker login registry-gitlab.com.tw Username (test): Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded $docker pull registry-gitlab.com.tw/repo/image-cli:v4.4.0 v4.4.0: Pulling from registry-gitlab.com.tw/repo/image-cli:v4.4.0 23302e52b49d: Pulling fs layer cf5693de4d3c: Download complete 0bdf97977791: Downloading [=\u0026gt; ] 110.1kB/3.493MB 8b8c7ad8f3fb: Waiting 40eb930bd6b2: Waiting   到這邊我們終於解決在 docker 拉取沒有 SSL Container registry 上的 private image 了，接著我們要來看在公有雲(Azure Kubernetes Service)上出了什麼問題。\nKubernetes pull image og Gitlab Container Registry 接著來看看在公有雲環境中( Azure Kubernetes Service , AKS )會出現的問題，我在 AKS 部署一個三個節點的環境，Kubernetes 版本為 1.15.11。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  bash-5.0# kubectl get node NAME STATUS ROLES AGE VERSION aks-agentpool-20139558-vmss000000 Ready agent 11m v1.15.11 aks-agentpool-20139558-vmss000001 Ready agent 11m v1.15.11 aks-agentpool-20139558-vmss000002 Ready agent 11m v1.15.11 bash-5.0# kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-698c77c5d7-69rch 1/1 Running 0 11m kube-system coredns-698c77c5d7-vsrzb 1/1 Running 0 14m kube-system coredns-autoscaler-5bd7c6759b-jx9dt 1/1 Running 0 14m kube-system kube-proxy-46phd 1/1 Running 0 11m kube-system kube-proxy-fgb6f 1/1 Running 0 11m kube-system kube-proxy-gxck8 1/1 Running 0 11m kube-system kubernetes-dashboard-74d8c675bc-j25fz 1/1 Running 0 14m kube-system metrics-server-7d654ddc8b-bljdd 1/1 Running 0 14m kube-system omsagent-rs-c45c944df-5crtb 1/1 Running 0 14m kube-system omsagent-w5b8l 1/1 Running 1 11m kube-system omsagent-xfndn 1/1 Running 1 11m kube-system omsagent-xhnbv 1/1 Running 0 11m kube-system tunnelfront-98c8b5dc6-b56d5 1/1 Running 0 14m   在AKS的環境上我直接透過kubectl CLI 建立一個非常簡單的Deployment Resource，測試拉取 Container registry 上的 private image 會有什麼問題。\n1  bash-5.0# kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash   接著觀察 pod 是否有成功被建立起起來。\n1 2 3  bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s   可以看到 pod 出現 ErrImagePull 的 status 此時，需要更進步一分析出現ErrImagePull的原因。\n1 2 3 4 5 6 7 8  bash-5.0# kubectl describe pod test ... Normal BackOff 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Back-off pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ImagePullBackOff Normal Pulling 4s (x3 over 46s) kubelet, aks-agentpool-20139558-vmss000000 Pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Failed to pull image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ErrImagePull   這邊可以觀察到問題\nPulling image \u0026ldquo;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026rdquo; 時發生\nx509: certificate is not authorized to sign other certificates\nGoogle 可以搜尋到很多 Kubernetes pull private 的解決方法，這邊直接使用看看還會遇到什麼問題。大致上就是設定 kubernetes 的某個 namespace 如果要拉某一個位置上的 private image 可以透過這一個使用者帳密去執行。\nkubectl create secret docker-registry gitlab-registry \\ --docker-username=jason \\ --docker-password=RmgmeG4K-1oD4j3XW5A- \\ --docker-email=jason@hello.com.tw \\ --docker-server=registry-gitlab.com.tw secret/gitlab-registry created 這一步做完我們可以再來看看是不是可以成功pull到image，我們先把原本的 pod delete 再重新 執行一個。\n1 2 3 4 5  bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ...   這時候持續觀察 pod 的狀態是不是running\n1 2 3  bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s   什麼竟然還不是 running 那我們繼續順藤摸瓜看看 pod 到底錯了什麼。\n1 2 3 4 5 6 7 8  bash-5.0# kubectl describe pod test ... Normal BackOff 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Back-off pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 16s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ImagePullBackOff Normal Pulling 4s (x3 over 46s) kubelet, aks-agentpool-20139558-vmss000000 Pulling image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34; Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Failed to pull image \u0026#34;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 4s (x3 over 45s) kubelet, aks-agentpool-20139558-vmss000000 Error: ErrImagePull   怎麼還是樣的錯，這時候不得不去看看 pod 的 spec 到底定義了什麼有沒有使用我們指定的 image pull secret 。\n1 2 3 4 5 6 7 8 9 10 11  bash-5.0# kubectl get pod test -o yaml ... name: default-token-wvgsq readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: aks-agentpool-20139558-vmss000000 nodeSelector: node-role.kubernetes.io/agent: \u0026#34;\u0026#34; priority: 0 ...   怎麼沒有 image pull secret 呢\u0026hellip;，找了一下文件 pod 再啟動時都會去用 default service account 的一些設定。\nref:https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account\n這邊就偷懶讓 default namesapce 都用同一個 image pull secret 吧 xDD\nkubectl patch serviceaccount default -p '{\u0026quot;imagePullSecrets\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;gitlab-registry\u0026quot;}]}' 這邊再把 Pod 刪掉再重新測試一次看看 image 能不能正確的拉取到。\n1 2 3 4 5  bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ...   這時候持續觀察 pod 的狀態是不是running\n1 2 3  bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 0/1 ErrImagePull 0 12s   還是不行啊，繼續透過kubectl CLI 找尋錯誤的原因。\nbash-5.0# kubectl describe pod test ... Normal BackOff 21s kubelet, aks-agentpool-20139558-vmss000002 Back-off pulling image \u0026quot;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026quot; Warning Failed 21s kubelet, aks-agentpool-20139558-vmss000002 Error: ImagePullBackOff Normal Pulling 10s (x2 over 23s) kubelet, aks-agentpool-20139558-vmss000002 Pulling image \u0026quot;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026quot; Warning Failed 10s (x2 over 22s) kubelet, aks-agentpool-20139558-vmss000002 Failed to pull image \u0026quot;registry-gitlab.com.tw/repo/image-cli:v4.4.0\u0026quot;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-gitlab.com.tw/v2/: x509: certificate is not authorized to sign other certificates Warning Failed 10s (x2 over 22s) kubelet, aks-agentpool-20139558-vmss000002 Error: ErrImagePull 這次看到一樣的錯誤資訊，這時候想到 unssl 的 image registry 不是要去設定 docker/daemon.json ，這樣 docker 去拉 image 的時候才不會認證他的憑證。\n因為在公有雲(Azure Kubernetes Server,AKS)上碰不到這幾台worker 主機，只好用奇門遁甲的方式 mount 他的 filesystem 了。\n這邊我解決的方法透過 daemonset 讓 pod 部署到所有的 node 上，該 pod 把 host 的 /etc mount 出來。讓我可以觀察AKS host docker 的 config。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  apiVersion:apps/v1kind:DaemonSetmetadata:name:registry-canamespace:kube-systemlabels:k8s-app:config-dockerspec:selector:matchLabels:name:config-dockertemplate:metadata:labels:name:config-dockerspec:containers:- name:config-dockerimage:nginx:1.18command:[\u0026#39;sh\u0026#39;]args:[\u0026#39;-c\u0026#39;,\u0026#39;tail -f /dev/null\u0026#39;]volumeMounts:- name:etc-dockermountPath:/etc/dockersecurityContext:privileged:trueterminationGracePeriodSeconds:30hostPID:truevolumes:- name:etc-dockerhostPath:path:/etc/docker  這邊眼尖得觀眾可能會發現為什麼我用了，securityContext以及hostPID這邊後續會講為什麼我要這樣做。\n好的廢話不多說，部署完這個 daemset 後直接進入 pod 裡面看他的設定。\nroot@registry-ca-rdzbv:/# cat /etc/docker/daemon.json { \u0026quot;live-restore\u0026quot;: true, \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;50m\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;5\u0026quot; } } 發現！他竟然沒有設定 insecure-registries 這邊我只好手動幫他做設定，設定完後直接送給 host pid reload dockerd。這邊你會好奇說為什麼可動到 host 上的 pid ，因為上面的 yaml 有設定 securityContext 以及 hostPID ，所以 container 可以直皆碰到 host 的 process 並且可以下一些需要 security 的指令。\n1 2 3  root@registry-ca-rdzbv:/#pidof dockerd 3322 root@registry-ca-rdzbv:/#kill -1 3322   這邊完成後，直接delete test pod 看看重新 run 一個能不能成功pull 到 private image 。\n1 2 3 4 5  bash-5.0# kubectl delete pod test pod \u0026#34;test\u0026#34; deleted kubectl run -ti --rm test --image registry-gitlab.com.tw/repo/image-cli:v4.4.0 bash ...   這時候持續觀察 pod 的狀態是不是running\n1 2 3  bash-5.0# kubectl get pod NAME READY STATUS RESTARTS AGE test 1/1 Running 0 30s   此時還有一個問題，剛剛只有修改一個節點如果 Pod 今天部署到其他節點上，還是會出現x509: certificate is not authorized to sign other certificates的問題。有沒有方式可以一勞永逸呢？我這邊直接沿用剛剛的daemset 並且修改一下 yaml file讓這個 pod 可以處理這個問題。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  apiVersion:apps/v1kind:DaemonSetmetadata:name:registry-canamespace:kube-systemlabels:k8s-app:config-dockerspec:selector:matchLabels:name:config-dockertemplate:metadata:labels:name:config-dockerspec:containers:- name:config-dockerimage:nginx:1.18command:[\u0026#39;sh\u0026#39;]args:[\u0026#39;-c\u0026#39;,\u0026#39;cp --remove-destination /home/core/daemon.json /etc/docker/daemon.json \u0026amp;\u0026amp; kill -1 $(pidof dockerd) \u0026amp;\u0026amp; tail -f /dev/null\u0026#39;]volumeMounts:- name:etc-dockermountPath:/etc/docker- name:docker-configmountPath:/home/coresecurityContext:privileged:trueterminationGracePeriodSeconds:30hostPID:truevolumes:- name:etc-dockerhostPath:path:/etc/docker- name:docker-configconfigMap:name:docker-insecure  這邊可以看到直接套了一個 configmap 以及透過 runtime 的 args 把configmap 的資料複製到 /etc/docker/daemon.json 最後重啟了 dockerd 的 process。\n以上是解決在Azure Kubernetes Service 遇到的 unssl image registry 的思路與解決方法。\n","description":"","id":18,"section":"posts","tags":["kubernetes"],"title":"Azure Kubernetes Service pull unssl  image registry","uri":"https://blog.jjmengze.website/posts/azure/private-image/"},{"content":"  ref: https://docs.openshift.com/container-platform/4.4/authentication/identity_providers/configuring-gitlab-identity-provider.html\nGitlab 管控使用者 為什麼要使用 Gitlab 進行平台的管控\ndentity providers use OpenShift Container Platform Secrets in the openshift-config namespace to contain the client secret, client certificates, and keys.\nYou can define an OpenShift Container Platform Secret containing a string by using the following command.\nCreating the Application OAuth 第一步，先去Gitlab上建置相關的application，並且設定該application所需要的權限。\n 如圖所示先選擇Gitlab網頁上右上角自己的頭像，並選擇Setting。    選擇完Setting後，到左方選擇Applications，進入到設定Application的頁面。    進到設定Application的頁面後，會看到以下畫面。這邊有幾個部分需要留意。    Redirect URI\n需要特定格式進行填寫，https://oauth-openshift.apps.\u0026lt;cluster-name\u0026gt;.\u0026lt;cluster-domain\u0026gt;/oauth2callback/\u0026lt;idp-provider-name\u0026gt;\ncluster name 與 cluster-domain 可以在 openshift console 的網址上看到，例如:console-openshift-console.apps.ocp4demo.jjmengze.website\n其中 ocp4demo 就是我們的 cluster name ，而 jjmengze.website 就是 cluster-domain。\n  openid\n因為 Openshift 需要透過 OpenID Connect (OIDC)驗證 Gitlab 使用者的登入資訊，所以我們要把 openid 給勾選起來。\n    填寫完成後按下 Save application 可以看到以下結果。    Create secret 要在 OCP 上面建立一個 secret 後續讓驗證的時候可以攜帶這個 secret 資訊。\n這裡的 clientSecret 的數值就是剛剛在 Gitlab 上建立 Application 的 Secret 的數值呦！\n$kubectl create secret generic gitlab-app-oath --from-literal=clientSecret=\u0026quot;1368992d4100943938f4896d47419b4bba3c5598dce0fa15f8c4ca50fc16758f\u0026quot; -n openshift-config Create Comfigmap 接著建立 configmap ，這裡的 configmap 是 Gitlab 的憑證，只要修改一下憑證的路徑在按照下面的命令執行就可以把憑證放到 OCP 上了。\n$kubectl create configmap gitlab-ca --from-file=ca.crt=/mnt/gitlab-ca.crt -n openshift-config Create OAuth 上面的步驟做完之後，我們還需要建立一個 yaml 案來告訴 OCP 說現在要多提供一種登入及認證方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion:config.openshift.io/v1kind:OAuthmetadata:name:clusterspec:identityProviders:- name:\u0026lt;登入選項會出現的名稱\u0026gt; mappingMethod: claim type:GitLabgitlab:clientID:\u0026lt;Gitlab上取得的applicationID\u0026gt; clientSecret: name:gitlab-app-oathurl:\u0026lt;Gitlab的位置\u0026gt; ca: name:gitlab-ca  de re mi so ，我們可以 apply 這個設定檔案到 OCP 上囉！\n$kubectl apply -f oath-gitlab.yml 就著我們就可以到openshift console 看剛剛的設定是不是大成功了！\n  快樂地點進去之後，可以看看。可以看到 OCP 向你的 Gitlab 要權限登入囉。\n  按下 Authorize 後就可進入到 Openshift 的頁面，接著開始操作你的環境囉！\n  ","description":"","id":19,"section":"posts","tags":["openshift"],"title":"OpenShift 燈紅酒綠與 Gitlab identity","uri":"https://blog.jjmengze.website/posts/ocp/gitlab-ocp/"},{"content":"  為什麼要使用？ 這邊我直接點出我個人的觀點，歡迎大家來探討到底需不需要導入 tracing 的概念在專案中。\n就我個人的理解導入 tracing 有幾個好處，能提供系統運作時有更全面的理解與追縱，包括延遲，錯誤以及效能等問題，我們可以透過 tracing 查看整個系統或是單個應用程式是如何處理某一項業務操作 並且追蹤此項操作的流程與相關訊息。\n這樣帶來了什麼好處？\n如果可以透過圖表等方式顯示我們跟蹤的結果，可以加速後續維運人員排除故障的速度並且可以幫助開發者找到效能問題的癥結點，此外我們還可以看到一條業務邏輯實際的流向是如何被處理的。\n那什麼是Opentracing 再談談 Opentracing 之前我們可以將時間回推倒90年代，當時 Google 發表了一篇關於分散是追蹤的論文\u0026ldquo;Dapper, a Large-Scale Distributed Systems Tracing Infrastructure\u0026rdquo;，除了該篇論文外 Google 還發表了另外一篇分散式追蹤的的問題所在\u0026ldquo;Uncertainty in Aggregate Estimates from Sampled Distributed Traces\u0026rdquo;，兩篇論文都非常的精采說明了為什麼需要做分散式追蹤以及追蹤帶來的效益已提以及追蹤帶來的效益與其困難點。\n隨後各家廠商跟著這幾篇論文的引導開發出一系列相關的工具例如， Dapper 、 Zipkin 、 Appdash 等等，越來越多廠商到了戰場上爭奪這一塊 tracing 的大餅，不過就使用者而言就相當的頭痛了，對於各廠商提供的 API 與系統支援度都不相同的情況之下，使用者幾乎沒有辦法無痛得轉移所使用的 tracing System。\n下一個時代來臨必須要有人統一群雄打造一個輕量級且標準化的中介層統一上下層之間的隔閡， CNCF 底下的 OpenTracing 願景為提供一個標準的 tracing API 所有使用者只要在乎 OpenTracing tracing 所提供的 API ，不需要再了解各家廠商如 Zipkin 、 Appdash 的流程，這點我認為他跟 Kubernetes 的 CNI 、 CRI 、 CSI有異曲同工之妙。\n名词解释 在了解 Opentracing 的由來之後，現在來介紹他的架構與相關的名詞，這有助於我們後續開發上的 API 使用。\nTrace 一個 trace 在 Opentracing 代表一個事件在系統的執行過程，我們可以從圖中看到藍色的代表一個事件在 Opentracing 稱為一個 trace ，底下經過紅色綠色以及灰色的處理。\n比較學術的說法唯有向無環圖Directed Acyclic Graph （ DAG )，有興趣的可以去查查這是什麼。\nSpan span 代表在 Opentracing 中各個工作單元。一個 span 可以包含其他 span 的 reference ，如此一來可以形成一個父子關係圖，進而形成一個完整的 trace 。\n其中 span 會攜帶一些資訊方便使用者閱讀例如：\n reference span 名稱 span 開始與結束時間 tag log span Context baggage Items\n  ChildOf 先來看看流程結構時序圖，Childof 我會歸類成父節點依賴子節點的結果。例如 Server Span 透過 RPC等方式 去叫了 register Span ， register Span 處理了一些資料又去呼叫 SQL Span 做 insert 的動作，SQL Span 對 SQL 存入資料後會返回給 register Span 回報 insert 動作成功，接著 register Span 又會返回給 Server Span 註冊成功的消息。\n這種父節點依賴子節點的行為稱為ChildOf。\n [-Parent Server Span--------------] [-Child register Span A----] [-Child Span B----] FollowsFrom 先來看看流程結構時序圖，父節點不依賴任何子節點的結果我把他歸類為 FollowsFrom ，例如我發送一條訊息，訊息是否成功被處理與發訊息的事件沒有關聯，這時就稱為 FollowsFrom 。\n[-Parent Span-] [-Child Span-] Span-tag 在 OpenTracing 的規範之下，每一個 span 都可以有 tag 屬性以 key-value 的形式出現。 tag 我們可以理解成某一種屬性例如圖中的error tag ，我們可以利用這格屬性過濾掉error =false 的span，方便我們查詢我追蹤。\nSpan-log 除了 tag 之外 Opentracing 還規範了 log ，開發者可以在 span 中加入 log ，他以以 key-value 的形式出現，方便開發者或是維運人員快速地檢視該 span 執行的相關訊息。如圖所示可以看到在 external service api 中的 log event 顯示time out 我們可以很快瞭解在執行這個操作的時候他的撞快為何。\nSpan Context 我認為這個比較難解釋，可以參考著官方的說明對照著看，簡單的說就是span會傳遞 context 資訊給下一個 span ，例如 spanID 、traceID等。\n The SpanContext carries data across process boundaries. Specifically, it has two major components:\n An implementation-dependent state to refer to the distinct span within a trace\n i.e., the implementing Tracer’s definition of spanID and traceID\n Any Baggage Items\n These are key:value pairs that cross process-boundaries.\nThese may be useful to have some data available for access throughout the trace.\n   Baggage Items 從字面上來看 baggage 就是行李的意思，一個 trace 開始到結束，可能會從某一個 span 攜帶行李到下一個 span 進行處理，不過這裡有一個非常重要的點就是行李在整個 trace 的過程都不會被丟棄，直到 trace 結束為止。\n從範例來看App A 是一個 span ，攜帶了一些資料到如ot-baggage-environment:production 到 APP B 如果後面還有 APP C 、APP D，這一個資料會傳到整個trace結束為止。\n小結 上面我們討論了 Opentracing 的前世今生，為什麼要有 tracing 以及相關的論文。同時針對 Opentracing 會出現的名詞做一個初步的解說，可以以幫助我們後續再使用 Opentracing 提供的 API 時更能了解他所代表的意義以及正確的使用姿勢。下一篇我會續繼分享Opentracing with jaeger 的用法。\n","description":"","id":20,"section":"posts","tags":["devops"],"title":"前方高能之Opentracing","uri":"https://blog.jjmengze.website/posts/go/opentracing/"},{"content":"  Github Action 起步走 由於最近在使用 Github 時不論是 Create 一個新的 Repository 或是 Fork 別人現有的 Project 都會跳出以下提示，為了滿足好奇心就花了一點時間研究了 Github Action 這個新功能。\n  GitHub Actions 是 GitHub 在美國11/13號全面開放的 CI/CD workflow系統，我們先看看官方的描述。\n官方的描述:\nGitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. 可以從官方的介紹看出來 GitHub Actions 是一個自動化 CI/CD 的功能，可以直接從GitHub deploy 以及 test 你的 source code。\n為什麼要用？ 在沒有用 Github Action 之前維運人員可能會透過 WebHook 之類的方式去得知當前 Repository 的狀態，例如 Kubernetes 社群就開發了一個機器人kubernetes/test-infra專門去處理相關的狀態改變並且觸發後續的流程，如 unit test 、intregration test 或是 e2e test(end to end test) \u0026hellip;.等等，或是透過其他 CI/CD 工具 如 Jenkins 、 Argo 等等其他工具去建構一個 pipline，Github 開發了Githbu Action 可以幫這我們在 Github Repository 上直接建立自動化部屬以及測試的流程，除此之外 Github 提供了一些吸引開發者去使用的特點。\n Discovering actions Notifications for workflow runs Disabling or limiting GitHub Actions for your repository or organization  \u0026quot;Managing a workflow run\u0026quot; \u0026quot;Events that trigger workflows\u0026quot; \u0026quot;Virtual environments for GitHub-hosted runners\u0026quot; \u0026quot;Managing billing for GitHub Actions\u0026quot; 常見的用法說明 GitHub Actions有許多常見的用詞，這邊稍微做一些簡單的解說。(這邊我個人認為我解釋得不好，不過還是聽聽吧xD)\n Continuous integration (CI):  我們可以透過 CI 去 build 以及 test Repository 上的 Source code。可以從 test 得過程快速地檢測錯誤的邏輯（這邊可以包含整個unit test 、intregration test 或是 e2e test(end to end test) \u0026hellip;.等等）。\n想要了解更多可以上wiki之類的去了解它的定義\n  Continuous deployment (CD)\nCD通常是建築於CI的結果之上，當CI執行成功後會觸發CD，這個步驟基本上是將CI打包好的\nbinary檔案部署到各種不同的環境如Test 環境 Staging 等等。客戶的使用者或是測試人員可以快速地得到部署完的環境，並且進行業務邏輯的測試。\n  Workflow file:\n是定義這個 Repository CI/CD 的工作流程(workflow/pipline)的一個 YAML 檔，這個檔案會放在 RepoRepository 跟目錄底下的 .github/workflows 資料夾中。\n  Action:\nAction 組合了多個 Step ，我可以將 Action 打包好並且分享到分享到 GitHub Action 社群。\n  Step:\n是一個執行一個獨立的指令或是執行別人打包好的Action，一個 Job 裡面會包含一到多個Step，Job 裡面的Step 是共用同一個environment與filesystem。\n  Job:\n由一個或是多個Step組成，當有多個Job的時候我們可以定義Job的相依關係與執行流程。Job 可以並行(parallel)執行或是按順序(sequentially)執行。例如Repository 有建立Build Job 以及Test Job ，Test Job 依賴Build Job 的執行結果，如果今天Build Job 執行失敗Test Job 則不會執行。\n  通常Job執行在不同的Runner(虛擬環境virtual environment).\n Runner:\nRunner是在執行Job的虛擬環境，我們可以自己建立Runner或是使用Github 代管的Runner ， Runner 會去去監聽 Github Job 的資料，當有 Job 資料產生的時候 Github 會將 Job 下放給某一個 Runner 去執行，一但 Runner 執行完 Job 會將執行結果回報給Github。  Runner 一次只能執行一個 Job 。\n  Artifact:\nArtifacts 就是把某一個 Job 執行完的結果（可能是 binary 檔、 log 檔等等），可以把這一個 Job 執行完的結果送到下一個 Job 。\n  Event:\n當某一個事件產生的時候如 create PR 、create issue 或是 create branch等等，Gitlab 會收到這個消息並且且進行處理。\n  要怎麼上手使用 Github 有提供一些預設的 Workflow templates給開發者做使用，開發者可以針對不同語言套用不同的樣板。\nGithub還會分析你Repository 的source code並且推薦你適合的Workflow templates，例如我的Repository 包含Golang的source code ，我們會收到Github 的建議Golang Workflow templates 我們可以以這個模型堆疊我們需要的Workflow。\n可以參考actions/starter-workflows查詢Github 官方維護的 Workflow templates。\n起手式 先創一個專案再說，我這邊先用 Golang 建立一個非常簡單的WebServer Repo。\n如圖所示，我們可以在 Github 上面可以看到這一個提示，問我們要不要建立一個 GitHub Actions 。\n這邊點選Set Up Action會跳轉到這個頁面，選擇想要建立的Workflow templates。\n  這邊可以直接點擊Set up workerflow，點進去後會跳轉到Workerflow 編輯頁面，頁面上可以看見預設的job、steps等資訊。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  name:Goon:push:branches:[master]pull_request:branches:[master]jobs:build:name:Buildruns-on:ubuntu-lateststeps:- name:SetupGo1.13uses:actions/setup-go@v1with:go-version:1.13id:go- name:CheckoutcodeintotheGomoduledirectoryuses:actions/checkout@v2- name:Getdependenciesrun:| go get -v -t -d ./...if[-fGopkg.toml];thencurlhttps://raw.githubusercontent.com/golang/dep/master/install.sh|shdepensurefi- name:Buildrun:gobuild-v.  稍微幾解釋一下這個yaml所代表的意義\n name\n表示這個Workflow的名字 on\n表示這個什麼時候會觸發這個Workerflow  push\n表示推到這個某一個branch的時候會觸發，這裡預設是master pull_request\n表示像這個某一個branch發出PR的時候會觸發，這裡預設是master   job\n裡面會有要執行的步驟與環境設定  build  name\n表示這個job用來建構程式碼 runs-on\n表示這個這個job要執行在什麼虛擬環境上，這裡是ubuntu，可以改成更小的image   steps\n表示這個job有多少步驟要執行  name\n描述這個這一步要做什麼，這邊預設是Set up Go 1.13 uses\n這邊的意思是使用別人寫的action，預設是actions/setup-go@v1 with\n可以指定action的參數，這邊指定go用1.13版go-version: 1.13 id\n可以在setup上加上一些標記，這邊預設是go run\n如果不要用別人的action想要執行自己的腳本可以用這個參數，如go build -v .       [color=#FF0000]這邊非常粗淺的帶過這些參數所代表的意義，如果想進行更複雜的設定可以參考GitHub Help\n 這邊設定好自己想要的Workerflow流程後可以按Start commit 提交這一個commit。\n  Commit 之後可以看到Github Action 被觸發，我們可以點進去看一下CI/CD有沒有過。\n這邊就可以看到我在Workflow Build的階段就失敗了xD，可以點進去看為什麼失敗，判斷是不是code寫錯還是哪裡有問題。\n  點進去後我們發現，原來是go build -v .出錯拉～那我們這邊做簡單的修改再重新提交一次。\n  將 Build 那一段 run 改成正確的語法，再重新commit。\n1 2  - name:Buildrun:gobuildcmd/main.go  再次檢查Github Action 的Workflow是不是都亮綠燈通過拉～\n  結束語 今天非常簡單的介紹了Github Action的功能，以及怎麼快速的上手使用這個新功能，下次有機會再來介紹他跟docker怎麼整合以及怎麼撰寫自己的action貢獻給Github Action Marketplace 讓大家來使用。\n","description":"","id":21,"section":"posts","tags":["ci/cd"],"title":"Github Action 起步走第一式","uri":"https://blog.jjmengze.website/posts/cicd/github/github-action-1/"},{"content":"  在 Kubernetes 資源是怎麼被回收的？ 是由 kubernetes master node 上的 kube-controller-manger 中的 garbage-collection controller 進行回收管理的。\n我們先不管 kubernetes 底層是怎麼回收這些垃圾物件的，先把焦點放在我們要怎麼設定yaml檔，畢竟我們是yaml工程師嘛(笑)，本篇文章會用進行幾個實驗來觀察各種 kubernetes 回收策略是如何進行的。\n當我們刪除物件時，可以指定該物件底下關聯的子物件是否也跟著自動刪除。\n 自動刪除附屬的行為也稱為 級聯刪除（Cascading Deletion）  Kubernetes 中有兩種 Cascading Deletion (聯集) 刪除分別是：\n 後台（Background） 模式 前台（Foreground） 模式。  Foreground 條件  物件的 metadata.finalizers 被設定為 foregroundDeletion 物件處於 deletion in progress 狀態（deletionTimestamp被建立）  行為  需要等到物件所有的關聯的子物件被删除完之後，才可以删除該物件 如何確定關聯的子物件的父親是誰？  透過子物件的 ownerReferences 來確定    Background kubernetes 立刻 馬上 删除物件， garbage-collection controller 會在後台(背景)删除該物件的子物件。\n除了在背景刪除子物件的行為外還有一種是不刪除子物件，讓子物件變成孤兒(Orphan)。\n實驗 propagation Policy (Foreground) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的deployment ReplicaSet 以及pod的狀態\n1 2 3 4 5 6 7 8 9  kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s   取得ReplicaSet與pod的ownerReferences 用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:597d36f5-968a-4025-8621-b24f17f7f3a6]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:97fb6974-6882-4459-b6b0-b39357a7650b]]   destroy Foreground 透過指定的刪除模式來刪除物件，這裡是透過Foreground 的方式刪除物件。\ncurl -X DELETE curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\ -d '{\u0026quot;kind\u0026quot;:\u0026quot;DeleteOptions\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;propagationPolicy\u0026quot;:\u0026quot;Foreground\u0026quot;}' \\ -H \u0026quot;Content-Type: application/json\u0026quot; 狀態 取的deployment 以及pod的狀態觀察刪除狀態。\n從這個狀態可以看到所有的 pod 都在 Terminating 的狀態， ReplicaSet 以及 deployment 都沒有先移除。\n1 2 3 4 5 6 7 8 9 10 11  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-nkbrb 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-zh5g7 0/1 Terminating 0 133m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 0/3 0 0 133m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 0 0 133m   比較差異 比較刪除前後的差異\nDeployment Deployment狀態的差異\n從 diff 的狀態可以看出來，在 metadata 的部分修改了 finalizers 並且指定了 foregroundDeletion 的模式表示使用 foreground 刪除模式，另外新增了 deletionTimestamp 的時間確定了物件的刪除時間。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  --- a/dpeloy.yaml +++ b/dpeloy.yaml - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2  namespace: default - resourceVersion: \u0026#34;1480766\u0026#34; + resourceVersion: \u0026#34;1499040\u0026#34;  ... status: - availableReplicas: 3  conditions: - - lastTransitionTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - lastUpdateTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - message: Deployment has minimum availability. - reason: MinimumReplicasAvailable - status: \u0026#34;True\u0026#34; - type: Available  ... - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 - updatedReplicas: 3 + - lastTransitionTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + lastUpdateTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + message: Deployment does not have minimum availability. + reason: MinimumReplicasUnavailable + status: \u0026#34;False\u0026#34; + type: Available + observedGeneration: 2 + unavailableReplicas: 3    ReplicaSet 觀察ReplicaSet的變化也是確定了刪除的時間 deletionTimestamp 以及修改時間 time ，以及是透過 foregroundDeletion 的策略進行刪除。\n在狀態欄的地方也能看出現在狀態replicas的數量被縮減為0個。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  --- a/rs.yaml +++ b/rs.yaml  creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2 ... - time: \u0026#34;2020-08-10T06:10:18Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:55Z\u0026#34; ... ownerReferences: @@ -86,7 +87,7 @@ metadata:  kind: Deployment name: nginx-deployment uid: 597d36f5-968a-4025-8621-b24f17f7f3a6 - resourceVersion: \u0026#34;1480765\u0026#34; + resourceVersion: \u0026#34;1499039\u0026#34; ... terminationGracePeriodSeconds: 30 status: - availableReplicas: 3 - fullyLabeledReplicas: 3 - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 + observedGeneration: 2 + replicas: 0   Pod pod 的部分也想當的簡單，因為沒有子物件所以只要確定刪除時間 deletionTimestamp 以及修改時間 time 。\npod的狀態會被修改成 pending 以及相關的資源都會被移除例如: pod ip。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  --- a/pod.yaml +++ b/pod.yaml  creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T08:23:25Z\u0026#34; ... - time: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:57Z\u0026#34;  uid: 97fb6974-6882-4459-b6b0-b39357a7650b - resourceVersion: \u0026#34;1480754\u0026#34; + resourceVersion: \u0026#34;1499048\u0026#34; status: \u0026#34;True\u0026#34; type: Initialized - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34;  type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; ... containerStatuses: - - containerID: docker://f20bbce2b58ac42426b61fc21e3f1a61938a51a4dc30277f50e9ac7aea88aa3d - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34;  lastState: {} name: nginx - ready: true + ready: false  restartCount: 0 - started: true + started: false  state: - running: - startedAt: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + waiting: + reason: ContainerCreating  hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.5 - podIPs: - - ip: 10.32.0.5 + phase: Pending   實驗propagationPolicy (Background) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的Deployment ReplicaSet 以及Pod的狀態\n1 2 3 4 5 6 7 8 9 10 11  kubectl get deploy,rs,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 55s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 55s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-6qvvg 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-plsn7 1/1 Running 0 55s   取得ReplicaSet與pod的ownerReferences用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  kubectl get replicaset.apps/nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:8a2be904-c306-426c-881e-c6914415c5fe]] kubectl get pod nginx-deployment-6b474476c4-6qvvg -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:565540ad-fbf1-4d74-8841-f0475e12a200]]   destroy background 1 2 3 4  curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Background\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34;   狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在Terminating 的狀態，但是replicaset 以及 deployment 都先被移除。\n1 2 3 4 5  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-6qvvg 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-plsn7 0/1 Terminating 0 31m   比較差異 deployment deployment狀態的差異\n可以看到deployment 直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  --- a/dpeloy.yaml +++ b/dpeloy.yaml -apiVersion: apps/v1 -kind: Deployment -metadata: - annotations: - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - kubectl.kubernetes.io/last-applied-configuration: | - {\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3,\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;containerPort\u0026#34;:80}]}]}}}} - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - managedFields: - - apiVersion: apps/v1 ... ...   replicaset 可以看到 replicaset 也是直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  --- a/rs.yaml +++ b/rs.yaml -apiVersion: apps/v1 -kind: ReplicaSet -metadata: - annotations: - deployment.kubernetes.io/desired-replicas: \u0026#34;3\u0026#34; - deployment.kubernetes.io/max-replicas: \u0026#34;4\u0026#34; - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - pod-template-hash: 6b474476c4 - managedFields: - - apiVersion: apps/v1 - fieldsType: FieldsV1 - fieldsV1: - f:metadata: - f:annotations: - .: {} - f:deployment.kubernetes.io/desired-replicas: {} - f:deployment.kubernetes.io/max-replicas: {} - f:deployment.kubernetes.io/revision: {} ... ...   pod 可以看到 pod 是緩慢的回收 ， 可以看到被設定了移除的時間以及相關狀態。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  --- a/pod.yaml +++ b/pod.yaml - time: \u0026#34;2020-08-10T10:05:25Z\u0026#34; + time: \u0026#34;2020-08-10T10:08:20Z\u0026#34;  + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T10:08:20Z\u0026#34;  generateName: nginx-deployment-6b474476c4- - resourceVersion: \u0026#34;1513382\u0026#34; + resourceVersion: \u0026#34;1513717\u0026#34; ... - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34;  type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; - - containerID: docker://bb34d6af8dbe1c72c423fece3d9d797ec8a5a0b62fd82f1f46bfcf5d67157be1 - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34;  lastState: {} name: nginx - ready: true + ready: false  restartCount: 0 - started: true + started: false  state: - running: - startedAt: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + waiting: + reason: ContainerCreating  hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.6 - podIPs: - - ip: 10.32.0.6 + phase: Pending ...   實驗 propagation Policy (Orphan) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的deployment replicaset 以及pod的狀態\n1 2 3 4 5 6 7 8 9  kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s   取得replicaset與pod的ownerReferences確定物件之間的關係\n1 2 3 4 5 6 7  kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:b1d1a61d-8b51-4511-8c91-de44aaa2cdd0]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:b052f80f-d72a-4e32-a4c4-f598274f2b07]]   destroy Orphan 1 2 3  curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34;   狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在 Running 的狀態，ReplicaSet 以及 Pod 都沒有被移除，只有 Deployment 被殺掉而已。\n1 2 3 4 5 6 7 8 9  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-d6wns 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-d7dtf 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-m7rbz 1/1 Running 0 12m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 12m   小結 從上面三個實驗可以看到不同的移除方式會有不同的結果\n  Front Ground\n需要等到關聯的子物件被刪除後才會進行清理的動作(打上deletionTimestamp)\n  BackGround\n先刪除物件(打上deletionTimestamp)，再慢慢回收子物件(打上deletionTimestamp)\n  3.Orphan\n直接把物件刪除（打上deletionTimestamp），所有子物件不做任何動作。\n","description":"","id":22,"section":"posts","tags":["kubernetes"],"title":"學習Kubernetes Garbage Collection機制","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-garbage-collection/"},{"content":"  Istio 簡介 Istio提供了一個完整的微服務應用解決方案，透過為整個服務網路提供行為監控和細粒度的控制來滿足微服務應用程序的多樣化需求。\n Istio提供了非常簡單的方式建立具有監控(monitor)、負載平衡(load balance)、服務間的認證（service-to-service authentication）\u0026hellip;等功能的網路功能，而不需要對服務的程式碼進行任何修改。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role RAM CPUs IP Address     Master 16G 8Cores 10.20.0.154   Node1 16G 8Cores 10.20.0.164   Node2 16G 8Cores 10.20.0.174    這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.11』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and docker and other package dependencies:\n1 2 3 4 5 6 7 8  $apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl  \nKubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0  \n透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker  \n將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf  \n在master節點上使用kubeadm進行kubernetes叢集的初始化\n1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16  \n會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4  You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config  \n在master 安裝Kubernetes CNI，這邊採用的是Calico。\n1 2  $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml  \n等所有的pod都完成，在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.11.1 node-1 Ready \u0026lt;none\u0026gt; 2m v1.11.1 node-2 Ready \u0026lt;none\u0026gt; 2m v1.11.1  \n在master節點上，下載並且安裝helm\n1 2 3  $wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz $tar zxvf helm-v2.9.1-linux-amd64.tar.gz $mv linux-amd64/helm /usr/bin  \n為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role，最後在初始化helm 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF $helm init --service-account tiller Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming!  \n完後成後可透過kubectl指令確認\n1 2 3 4 5 6  $kubectl get pod,svc -l app=helm -n kube-system NAME READY STATUS RESTARTS AGE pod/tiller-deploy-759cb9df9-b7n7j 1/1 Running 0 4m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tiller-deploy ClusterIP 10.107.71.110 \u0026lt;none\u0026gt; 44134/TCP 4m  \n安裝Istio 透過官方提供的腳本，下載Istio並安裝istioctl binary\n1 2 3  $curl -L https://git.io/getLatestIstio | sh - $cd istio-1.0.0/ $cp bin/istioctl /usr/bin/  \n在helm version 2.10.0以前的版本Istio還是需要手安裝Istio CRD\n1 2  $kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml $kubectl apply -f install/kubernetes/helm/istio/charts/certmanager/templates/crds.yaml  \n透過kubectl指令檢查Istio是否安裝成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  $kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7d8f9748c5-zd4vb 1/1 Running 0 4m istio-egressgateway-676c8546c5-fq55b 1/1 Running 0 4m istio-galley-5669f7c9b-q98ld 1/1 Running 0 4m istio-ingressgateway-5475685bbb-5jfwv 1/1 Running 0 4m istio-pilot-5795d6d695-2vfq9 2/2 Running 0 4m istio-policy-7f945bf487-brtxn 2/2 Running 0 4m istio-sidecar-injector-d96cd9459-ws647 1/1 Running 0 4m istio-statsd-prom-bridge-549d687fd9-mb99f 1/1 Running 0 4m istio-telemetry-6c587bdbc4-tzblq 2/2 Running 0 4m prometheus-6ffc56584f-xcpsj 1/1 Running 0 4m root@sdn-k8s-b4:/home/ubuntu|⇒ kubectl get pod,svc NAME READY STATUS RESTARTS AGE pod/app-debug-6b4f85c9cc-gq7nx 1/1 Running 1 28d pod/config 1/1 Running 0 1d pod/hello-55f998cd56-d2q8n 1/1 Running 0 1d pod/hellogo-bb9bd67f7-ckmkc 1/1 Running 0 1d pod/myapp-pod 0/1 Completed 0 1d pod/myapp-pod2 0/1 Completed 0 1d pod/network-controller-server-tcp-7x6vm 1/1 Running 0 1d pod/network-controller-server-tcp-kjtm9 1/1 Running 0 1d pod/network-controller-server-unix-2rnfv 1/1 Running 0 1d pod/network-controller-server-unix-lh8qh 1/1 Running 0 1d pod/nginx-6f858d4d45-vrlgb 1/1 Running 0 1d pod/onos-65df85486f-tvk5t 1/1 Running 0 1d pod/skydive-agent-9x2n4 1/1 Running 0 6h pod/skydive-agent-rww57 1/1 Running 0 6h pod/skydive-analyzer-5f9556687f-gvdbj 2/2 Running 0 6h $kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-citadel ClusterIP 10.106.166.113 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 5m istio-egressgateway NodePort 10.100.183.7 \u0026lt;none\u0026gt; 80:31607/TCP,443:31053/TCP 5m istio-galley ClusterIP 10.109.104.146 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 5m istio-ingressgateway NodePort 10.101.66.117 \u0026lt;none\u0026gt; 80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:32388/TCP,8060:32100/TCP, 15030:30847/TCP,15031:32749/TCP 5m istio-pilot ClusterIP 10.102.202.205 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 5m istio-policy ClusterIP 10.97.181.32 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP 5m istio-sidecar-injector ClusterIP 10.96.165.139 \u0026lt;none\u0026gt; 443/TCP 5m istio-statsd-prom-bridge ClusterIP 10.101.82.72 \u0026lt;none\u0026gt; 9102/TCP,9125/UDP 5m istio-telemetry ClusterIP 10.108.94.224 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 5m prometheus ClusterIP 10.104.3.226 \u0026lt;none\u0026gt; 9090/TCP 5m  \n範例：Bookinfo Application 這邊示範Istio官方提供的範例：Bookinfo Application\n1 2 3 4 5 6 7 8 9 10 11  $kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) service/details created deployment.extensions/details-v1 created service/ratings created deployment.extensions/ratings-v1 created service/reviews created deployment.extensions/reviews-v1 created deployment.extensions/reviews-v2 created deployment.extensions/reviews-v3 created service/productpage created deployment.extensions/productpage-v1 created  \n透過kubectl指令確認安裝的pod及service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-fc9649d9c-tqbcn 2/2 Running 0 1m productpage-v1-58845c779c-2lqxg 2/2 Running 0 27m ratings-v1-6cc485c997-zqvqt 2/2 Running 0 1m reviews-v1-76987687b7-hfrn2 2/2 Running 0 1m reviews-v2-86749dcd5-ffmvs 2/2 Running 0 1m reviews-v3-7f4746b959-zr4ml 2/2 Running 0 1m $kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.101.208.129 \u0026lt;none\u0026gt; 9080/TCP 1m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 27m productpage ClusterIP 10.101.11.13 \u0026lt;none\u0026gt; 9080/TCP 1m ratings ClusterIP 10.105.132.197 \u0026lt;none\u0026gt; 9080/TCP 1m reviews ClusterIP 10.103.199.76 \u0026lt;none\u0026gt; 9080/TCP 1m  \n建立一個Gateway讓叢及外部可以存取\n1  $kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  \n接著我們透過瀏覽器去存取我們服務，在網址的地方輸入http://\u0026lt;node or master ip\u0026gt;:31380/productpage，網頁會顯下圖的網站內容。\n  Bookinfo Application 網頁服務\n  不斷重新整理該服務的網頁，會發現網頁上的星星的部分有改變。從沒星星==\u0026gt;黑星星==\u0026gt;紅星星切換，分別對應到pod中的三個版本的review,預設的載平衡功能是輪詢（Round Robin）\n設定destination rule，此時還是輪詢法\n1  $kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml  \nIntelligent Routing Istio 提供了智能路由（Intelligent Routing），這邊示範如何使用Istio管理各種服務的流量\n依照版本進行路由管理 1 2 3 4 5  $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml virtualservice.networking.istio.io/productpage created virtualservice.networking.istio.io/reviews created virtualservice.networking.istio.io/ratings created virtualservice.networking.istio.io/details created   這時候我們再回到瀏覽器查看Bookinfo Application 網頁服務，不管重新整理多少次網頁，都看不到網頁上有星星的標示，因為此時所有的請求都被轉送到review v1版本上\n依照用戶進行路由管理 1 2  $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml virtualservice.networking.istio.io/reviews create   我們回到瀏覽器上，按下右上角的Sign in 的按鈕。帳號密碼都為jason，登入後會發現不管怎麼更新頁面，網頁上都是呈現黑星星的標示。\n當我們登出後，也進行更新頁面的動作，網頁上不會出現任何星星的標示。\n因為當我們登入jason後，所有的路由都請求都會被轉發到review v2版本上。\nFault injection 有時候我們的程式碼裡面會有bug，可以透過注入故障的方式發現這些潛伏在裡面的bug。\nBookinfo特別示範一個http延遲的例子，為了測BookInfo的微服務，在reviews:v2和ratings以及用戶jason之間注入七秒延遲。此測試將發Bookinfo故意塞進去的一個錯誤。\n剛剛的路由規則，如果已經被您刪除掉。請再利用以下指令加回他的路由規則。\n1 2  $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml  \n使用以下指令注入一個http delay ，該指令在reviews:v2和ratings以及用戶jason之間注入七秒http delay\n1  $kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml  \n這時我們去存取網頁會拋出一個異常的錯誤，因為我們把服務與服務之間的存取的時間拉長，我們可以透過注入錯誤的方式發現一個存取延遲的bug。\n  注入延遲後的Bookinfo Application網頁服務\n  結論 玩過 istio 之後發現功能十分強大，但架構過於複雜當有問題出現時，維運與開發人員難以排查狀況與問題的發生點，但目前 istio 還在 1.0 版本 未來發展起來應該是一頭猛獸 ，會持續關注 service mesh 的相關議題。\nistio 背後撐腰的公司非常可怕 ，可以觀察這個專案後續的走向，作為學習的方向！！\n","description":"","id":28,"section":"posts","tags":["servicemesh","kubernetes"],"title":"service mesh 之 Istio 1.0 安裝及極度簡易操作","uri":"https://blog.jjmengze.website/posts/istio/istio1.0-install/"},{"content":"  Serverless 在進入Kubeless之前先科普一下什麼是Serverless。\n就字面上的意思來說就是沒有Server，那沒有Server又是什麼意思？\n先從設計開發到部署開始！\n我們想要做出一套系統軟體，勢必會經歷設計開發部署\u0026hellip;等流程。\n最後總會部署到單台或是多台的Server上面，當把服務架設到Server上時有許多方案可以提供我們參考。\n例如: Google 所提供的 GKE，AWS 所提供的 EC2 亦或是 Microsoft 所提供的 Azure。\n選擇完使用哪個雲服務商所提供的Cloud後，還要考慮這套系統究竟需要多少台Server、多大空間Stroage\u0026hellip;等問題。\n在微服務的架構之下，我們的每個服務會運行在各個Server上（先不考慮Container XD）\n在你的業務擴展後，我們就需要更多更多的資源，也就是更多更多的Server。\n很多微服務事實上被call到的機會很少，但他還是佔了一台Server的資源及空間。同時這些很少被使用到的微服務也一點一滴的花掉你的錢（開Server可是要錢的啊！！！）\n有了Serverless又或是稱為FaaS的出現後，我們再也不需要去關心那些Server，只要把微服務source code提交給雲服務商，雲服務商能提供一個運行的環境給你，總而言之就是開發者不需要再去關心底下Infrastructure Layer。\nKubeless 簡介 Kubeless是一個Kubernetes-native 無伺服器(Serverless)框架，只要撰寫程式（Function），而不需要了解底層基礎建設（Infrastructure Layer）。Kubeless利用Kubernetes資源提供自動擴展，API路由，監控，故障排除\u0026hellip;等功能。\n環境配置 本次安裝是使用OpenStack上的三台虛擬機器去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role RAM CPUs IP Address     Master 8G 4Cores 172.24.0.2   Node1 8G 4Cores 172.24.0.2   Node2 8G 4Cores 172.24.0.4    部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝套件 安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7  $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl  \n關閉Swap Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0  \n重啟Docker 透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker  \nIPv4 forward設定 將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf  \nCluster 初始化 在master節點上使用kubeadm進行kubernetes叢集的初始化\n1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16   1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16   會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4 5  ... You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5   在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config  \n安裝Calico CNI 1 2  $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml   檢查環境 在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.10.0 node-1 Ready \u0026lt;none\u0026gt; 2m v1.10.0 node-2 Ready \u0026lt;none\u0026gt; 2m v1.10.0  \n安裝Kubeless 安裝CLI 安裝操作 Kubeless 的 CLI\n1 2 3 4  $wget https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.7/kubeless_linux-amd64.zip apt install -y unzip unzip kubeless_linux-amd64.zip $sudo mv ~/bundles/kubeless_linux-amd64 /usr/local/bin/  \n檢查CLI是否安裝完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  Serverless framework for Kubernetes Usage: kubeless [command] Available Commands: autoscale manage autoscale to function on Kubeless completion Output shell completion code for the specified shell. function function specific operations get-server-config Print the current configuration of the controller help Help about any command topic manage message topics in Kubeless trigger trigger specific operations version Print the version of Kubeless Flags: -h, --help help for kubeless Use \u0026#34;kubeless [command] --help\u0026#34; for more information about a command.  \n安裝Kubeless Kubeless官方針對不同Kubernetes環境提供了多種腳本（非RBAC，RBAC和openshift），這邊我採用的是有RBAC的部署。\n1 2 3 4 5 6 7 8 9 10 11 12 13  $kubectl create ns kubeless namespace/kubeless created $kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml configmap/kubeless-config created deployment.apps/kubeless-controller-manager created serviceaccount/controller-acct created clusterrole.rbac.authorization.k8s.io/kubeless-controller-deployer created clusterrolebinding.rbac.authorization.k8s.io/kubeless-controller-deployer created customresourcedefinition.apiextensions.k8s.io/functions.kubeless.io created customresourcedefinition.apiextensions.k8s.io/httptriggers.kubeless.io created customresourcedefinition.apiextensions.k8s.io/cronjobtriggers.kubeless.io created  \n可以透過kubetl指令觀察kubeless是否有被部署起來\n1 2 3 4 5 6 7 8 9 10 11 12  kubectl get pods -n kubeless NAME READY STATUS RESTARTS AGE kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 1m $kubectl get deployment -n kubeless NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubeless-controller-manager 1 1 1 1 2m $kubectl get crd NAME CREATED AT cronjobtriggers.kubeless.io 2018-08-24T12:44:57Z functions.kubeless.io 2018-08-24T12:44:57Z httptriggers.kubeless.io 2018-08-24T12:44:57Z   ### 安裝Kubeless UI 官方提供了操作Kubeless的Dashboard，可以透過該介面操作。 ```bash $kubectl create -f https://raw.githubusercontent.com/kubeless/kubeless-ui/master/k8s.yaml 可以透過kubetl指令觀察Kubeless的Dashboard是否有被部署起來\n1 2 3 4 5 6 7  kubectl -n kubeless get pod,svc NAME READY STATUS RESTARTS AGE pod/kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 39m pod/ui-6d868664c5-kr99m 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ui NodePort 10.100.15.46 \u0026lt;none\u0026gt; 3000:30722/TCP 1m   可以透Browser瀏覽 Kubeless UI(172.24.0.2:30722)\n  Kubeless Dashboard\n  建立及測試Function 這邊示範一個簡單的質數判斷的Function，以Go為例。\n這邊會示範從UI上操作以及使用CLI操作\nUI操作Kubeless 透過Browser連進Kubeless Dashboard 後點選 Create Function\n  建立一個Function\n  按下Create後，就可以撰寫我們判斷質數的程式碼。\n程式的撰寫畫面會如下圖所示\n  在function內撰寫我們想要的功能\n  撰寫完，質數判斷的function大致上是這個樣子\n  質數判斷的function\n  測試Function 接著可以輸入右側的Request，填入你想要判斷的數值。如：177,9487\u0026hellip;等數值。\n填完數值後按下Run fuction，發現底下會沒有輸出！！！\n 因為kubeless UI有RBAC的問題，這邊偷懶直接給cluster-admin。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubeless-ui-default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: User name: system:serviceaccount:kubeless:ui-acct apiGroup: rbac.authorization.k8s.io EOF     這邊還會產生另外一個問題，在Kubeless UI 所產生的Request只能是String型態的。\n在這個範例之中會看到以下輸出。\n   只能得到must be a positive integer的回應\n  要解決這個問題我們可以藉由Kubeless CLI發送請求給特定的function。\n1 2 3 4 5 6  $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數   結束實驗後我們可以透過Kubeless UI刪除掉剛剛建立的checkprime function\nCLI操作Kubeless 建立一個checkprime.go，並且撰寫質數判斷的function。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  package kubeless import ( \u0026#34;strconv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/kubeless/kubeless/pkg/functions\u0026#34; ) func IsPrime(event functions.Event, context functions.Context) (string, error) { num, err := strconv.Atoi(event.Data) if err != nil { fmt.Println(err) return \u0026#34;must be a positive integer\u0026#34;, nil } if num \u0026lt;= 1 { return \u0026#34;must be a positive integer\u0026#34;, nil } for i := 2; i \u0026lt; num; i++ { if num%i == 0 { return \u0026#34;非質數\u0026#34;, nil break } } return \u0026#34;質數\u0026#34;, nil }   透過kubeless CLI幫我們建立Function到環境上\n1 2 3 4  $kubeless function deploy checkprime --runtime go1.10 --from-file checkprime.go --handler checkprime.IsPrime INFO[0000] Deploying function... INFO[0000] Function checkprime submitted for deployment INFO[0000] Check the deployment status executing \u0026#39;kubeless function ls checkprime\u0026#39;   透過Kubeless CLI 、 Kubectl 去確認Function 有沒有成功的被建立到環境上：\n1 2 3 4 5 6 7 8 9 10 11  $kubectl get functions NAME AGE checkprime 1m $kubeless function ls NAME NAMESPACE\tHANDLER RUNTIME\tDEPENDENCIES\tSTATUS checkprime\tdefault checkprime.IsPrime\tgo1.10 1/1 READY $kubectl get po NAME READY STATUS RESTARTS AGE checkprime-7d4f$b7b64c-k47w9 1/1 Running 0 37s   測試Function 透過Kubeless CLI去測試剛剛撰寫的質數判斷是否正確\n1 2 3 4 5 6  $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數   另外也能透過kubernetes把該function proxy 出來進行測試\n1 2 3 4 5 6 7 8 9  $kubectl proxy -p 34567 \u0026amp; [1] 3533 Starting to serve on 127.0.0.1:34567 $curl --data 9487\\  --header \u0026#34;Content-Type:application/json\u0026#34; \\  localhost:34567/api/v1/namespaces/default/services/checkprime:8080/proxy/ 非質數   結束實驗後我們可以透過Kubeless CLI刪除掉剛剛建立的checkprime function\n1 2 3 4 5  $kubeless function delete checkprime $kubeless function ls NAME\tNAMESPACE\tHANDLER\tRUNTIME\tDEPENDENCIES\tSTATUS   小結 還有很多開源的 FAAS 框架可以直接套用在 kubernetes 上，本篇文章只針對 kubeless 做一個非常簡單的 Demo ，有機會的話還會玩玩看 Knative , fission \u0026hellip;等框架。\n","description":"","id":29,"section":"posts","tags":["kubernetes","serverless"],"title":"FAAS之kubeless的新滋味","uri":"https://blog.jjmengze.website/posts/kubeless/kubeless/"},{"content":"  kolla是為了提供production-ready的 OpenStack Cloud 之container和deployment tools。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role OpeStack Role RAM CPUs IP Address     Master Controller 16G 8Cores 10.0.0.190   Node1 Compute1 16G 8Cores 10.0.0.191   Node2 Compute2 16G 8Cores 10.0.0.192    部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7 8 9  $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt; kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo cp -aR kubernetes.list /etc/apt/sources.list.d/kubernetes.list $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl   Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0   透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker   確認Docker是否支援Cgroup Driver或是Systemd Driver，進一步修改 kubelet 設定參數。\n1 2  $CGROUP_DRIVER=$(sudo docker info | grep \u0026#34;Cgroup Driver\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) $sudo sed -i \u0026#34;s|KUBELET_KUBECONFIG_ARGS=|KUBELET_KUBECONFIG_ARGS=--cgroup-driver=$CGROUP_DRIVER|g\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf   將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf   設置Kubernetes Server CIDR的DNS位置。\n1  $sudo sed -i \u0026#39;s/10.96.0.10/10.3.3.10/g\u0026#39; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf   利用以下重新載入kubelet相關的啟動參數\n1 2 3 4  $sudo systemctl daemon-reload $sudo systemctl stop kubelet $sudo systemctl enable kubelet $sudo systemctl start kubelet   在master節點上使用kubeadm進行kubernetes 叢集的初始化\n1  $sudo kubeadm init --feature-gates CoreDNS=true --pod-network-cidr=10.1.0.0/16 --service-cidr=10.3.3.0/24   會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4  You can now join any number of machines by running the following on each node as root: kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481   在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481   在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config   在master 安裝Kubernetes CNI，這邊採用的是Canal。\n1 2 3 4 5  $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/rbac.yaml $kubectl apply -f rbac.yaml $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml $sed -i \u0026#34;s@10.244.0.0/16@10.1.0.0/16@\u0026#34; canal.yaml $kubectl apply -f canal.yaml   因為我們要把kubernetes master當作openstack controller的角色，我們將kubernetes master 的節點污染拿掉。\n1  $kubectl taint nodes --all=true node-role.kubernetes.io/master:NoSchedule-   在CNI安裝完成後可透過下列指令來檢查，Node \u0026amp; Pod是否都準備好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  $kubectl get pod,node -o wide --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/canal-297sw 3/3 Running 0 2m kube-system pod/canal-f82qj 3/3 Running 0 2m kube-system pod/canal-zxfbk 3/3 Running 0 2m kube-system pod/coredns-7997f8864c-cglcr 1/1 Running 0 9m kube-system pod/coredns-7997f8864c-sxf2c 1/1 Running 0 9m kube-system pod/etcd-node1 1/1 Running 0 8m kube-system pod/kube-apiserver-node1 1/1 Running 0 8m kube-system pod/kube-controller-manager-node1 1/1 Running 0 8m kube-system pod/kube-proxy-6gwws 1/1 Running 0 6m kube-system pod/kube-proxy-9xbdq 1/1 Running 0 6m kube-system pod/kube-proxy-z54k4 1/1 Running 0 9m kube-system pod/kube-scheduler-node1 1/1 Running 0 8m NAME STATUS ROLES AGE VERSION node1 Ready master 8m v1.10.3 node2 Ready master 8m v1.10.3 node3 Ready master 8m v1.10.3   利用BusyBox ，驗證Kubernetes 環境例如DNS 是否有通。\n1 2 3 4 5 6 7 8 9  $kubectl run -i -t $(uuidgen) --image=busybox --restart=Never $nslookup kubernetes Server: 10.3.3.10 Address 1: 10.3.3.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.3.3.1 kubernetes.default.svc.cluster.local   為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF   接著我們安裝Kubernetes Helm，用來管理Helm package的元件。\n1 2 3 4  $curl -L https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh $chmod 700 get_helm.sh $./get_helm.sh $helm init --service-account tiller   由於kolla kubernetes 需要用到ansible git ，我們在master的節點需要安裝ansible ，其他節點需安裝python。\n1 2 3  $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y software-properties-common git python python-pip $sudo apt-add-repository -y ppa:ansible/ansible $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y ansible   建立一個資料夾方便我們接下來的步驟\n1 2  $mkdir kolla-bringup $cd kolla-bringup   下載 kolla-kubernetes \u0026amp; kolla ansible\n1 2 3 4 5 6 7  $git clone http://github.com/openstack/kolla-ansible $git clone http://github.com/openstack/kolla-kubernetes $cd kolla-kubernetes $git checkout 22ed0c232d7666afb6e288001b8814deea664992 $cd ../kolla-ansible $git checkout origin/stable/pike $cd ..   使用pip 安裝kolla-kubernetes \u0026amp; kolla-ansible所需要的套件\n1  $sudo pip install -U kolla-ansible/ kolla-kubernetes   把相關設定檔製到/etc底下\n1 2  $cp -Ra kolla-kubernetes/etc/kolla/ /etc $cp -Ra kolla-kubernetes/etc/kolla-kubernetes/ /etc   pip剛剛安裝的項目這時候可以幫我們生成default passwords\n1  $sudo kolla-kubernetes-genpwd   建立一個Kubernetes namespaces來隔離Kolla deployment\n1  $kubectl create namespace kolla   使用Label標記要成為Controller及Compute的節點，我們這邊將node1當成Controller，node2 node3當成Compute\n1 2 3  $kubectl label node node1 kolla_controller=true $kubectl label node node2 kolla_compute=true $kubectl label node node3 kolla_compute=true   將/etc/kolla/globals.yml設定檔，修改成與自己環境相符\n將/etc/kolla/globals.yml中的network_interface設置為Management interface name。例如：eth1\n將/etc/kolla/globals.yml中的neutron_external_interface設置為neutron external interface name。例如：eth2\n將相關的openstack設定加入/etc/kolla/globals.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  $cat \u0026lt;\u0026lt;EOF \u0026gt; add-to-globals.yml kolla_install_type: \u0026#34;source\u0026#34; tempest_image_alt_id: \u0026#34;{{ tempest_image_id }}\u0026#34; tempest_flavor_ref_alt_id: \u0026#34;{{ tempest_flavor_ref_id }}\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; api_interface_address: 0.0.0.0 tunnel_interface_address: 0.0.0.0 orchestration_engine: KUBERNETES memcached_servers: \u0026#34;memcached\u0026#34; keystone_admin_url: \u0026#34;http://keystone-admin:35357/v3\u0026#34; keystone_internal_url: \u0026#34;http://keystone-internal:5000/v3\u0026#34; keystone_public_url: \u0026#34;http://keystone-public:5000/v3\u0026#34; glance_registry_host: \u0026#34;glance-registry\u0026#34; neutron_host: \u0026#34;neutron\u0026#34; keystone_database_address: \u0026#34;mariadb\u0026#34; glance_database_address: \u0026#34;mariadb\u0026#34; nova_database_address: \u0026#34;mariadb\u0026#34; nova_api_database_address: \u0026#34;mariadb\u0026#34; neutron_database_address: \u0026#34;mariadb\u0026#34; cinder_database_address: \u0026#34;mariadb\u0026#34; ironic_database_address: \u0026#34;mariadb\u0026#34; placement_database_address: \u0026#34;mariadb\u0026#34; rabbitmq_servers: \u0026#34;rabbitmq\u0026#34; openstack_logging_debug: \u0026#34;True\u0026#34; enable_heat: \u0026#34;no\u0026#34; enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; enable_cinder_backend_iscsi: \u0026#34;yes\u0026#34; enable_cinder_backend_rbd: \u0026#34;no\u0026#34; enable_ceph: \u0026#34;no\u0026#34; enable_elasticsearch: \u0026#34;no\u0026#34; enable_kibana: \u0026#34;no\u0026#34; glance_backend_ceph: \u0026#34;no\u0026#34; cinder_backend_ceph: \u0026#34;no\u0026#34; nova_backend_ceph: \u0026#34;no\u0026#34; EOF $cat ./add-to-globals.yml | sudo tee -a /etc/kolla/globals.yml   接下來透過ansible幫我們產生OpneStack設定檔\n1  $ansible-playbook -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla kolla-kubernetes/ansible/site.yml   使用官方提供的腳本幫助我們快速的把OpenStack的password，加到Kubernetes secrets\n1  $kolla-kubernetes/tools/secret-generator.py create   使用之前在pip安裝的檔案，快速的把OpenStack的設定檔，加入Kubernetes config maps裡\n1 2 3 4 5 6 7 8 9 10  $kollakube res create configmap \\  mariadb keystone horizon rabbitmq memcached nova-api nova-conductor \\  nova-scheduler glance-api-haproxy glance-registry-haproxy glance-api \\  glance-registry neutron-server neutron-dhcp-agent neutron-l3-agent \\  neutron-metadata-agent neutron-openvswitch-agent openvswitch-db-server \\  openvswitch-vswitchd nova-libvirt nova-compute nova-consoleauth \\  nova-novncproxy nova-novncproxy-haproxy neutron-server-haproxy \\  nova-api-haproxy cinder-api cinder-api-haproxy cinder-backup \\  cinder-scheduler cinder-volume iscsid tgtd keepalived \\  placement-api placement-api-haproxy   透過官方提供的腳本建立Kolla Helm Chart\n1  $kolla-kubernetes/tools/helm_build_all.sh .   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  $cat \u0026lt;\u0026lt;EOF \u0026gt; cloud.yaml global: kolla: all: docker_registry: docker.io image_tag: \u0026#34;4.0.0\u0026#34; kube_logger: false external_vip: \u0026#34;192.168.7.105\u0026#34; base_distro: \u0026#34;centos\u0026#34; install_type: \u0026#34;source\u0026#34; tunnel_interface: \u0026#34;docker0\u0026#34; keystone: all: admin_port_external: \u0026#34;true\u0026#34; dns_name: \u0026#34;192.168.7.105\u0026#34; public: all: port_external: \u0026#34;true\u0026#34; rabbitmq: all: cookie: 67 glance: api: all: port_external: \u0026#34;true\u0026#34; cinder: api: all: port_external: \u0026#34;true\u0026#34; volume_lvm: all: element_name: cinder-volume daemonset: lvm_backends: - \u0026#39;192.168.7.105\u0026#39;: \u0026#39;cinder-volumes\u0026#39; ironic: conductor: daemonset: selector_key: \u0026#34;kolla_conductor\u0026#34; nova: placement_api: all: port_external: true novncproxy: all: port: 6080 port_external: true openvswitch: all: add_port: true ext_bridge_name: br-ex ext_interface_name: enp1s0f1 setup_bridge: true horizon: all: port_external: true EOF   將該數值修改成環境上Management interface的IP。例如10.0.0.178\n1  $sed -i \u0026#34;s@192.168.7.105@10.0.0.178@g\u0026#34; ./cloud.yaml   將該數值修改成環境上ext_interface_name的interface。例如：eth1\n1  $sed -i \u0026#34;s@enp1s0f1@eth1@g\u0026#34; ./cloud.yaml   將該數值修改成環境上Management interface。例如：eth2\n1  $sed -i \u0026#34;s@docker0@eth2@g\u0026#34; ./cloud.yaml   在這邊建立一個給Openstack rbac ，讓後來helm啟動的元件去取得Kubernetes資源不會有權限問題（OpenStack官方RBAC這一點還沒修正\u0026hellip;）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kolla EOF   一個一個把openstack的服務使用helm啟動起來。（例如mariadb,rabbitmq,memcached\u0026hellip;等)\n1 2 3 4 5 6 7 8 9 10 11  $helm install --debug kolla-kubernetes/helm/service/mariadb --namespace kolla --name mariadb --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/rabbitmq --namespace kolla --name rabbitmq --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/memcached --namespace kolla --name memcached --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/keystone --namespace kolla --name keystone --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/glance --namespace kolla --name glance --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/cinder-control --namespace kolla --name cinder-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/horizon --namespace kolla --name horizon --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/openvswitch --namespace kolla --name openvswitch --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/neutron --namespace kolla --name neutron --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-control --namespace kolla --name nova-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml   這邊有可能遇到第一個問題，也就是nova-compute init 會卡在這個畫面。\n1  nova-api-create-cell-r288q 0/1 Init:2/3 0 5min   這邊發生了一些問題，官方沒有去修正他。先把這個helm chart 刪除掉，我在這邊修正了helm的設定檔案，去修改keystone的url。\n1 2 3 4  $helm delete --purge nova-compute $vim kolla-bringup/kolla-kubernetes/helm/microservice/nova-api-create-simple-cell-job/templates/nova-api-create-cell.yaml {{- $keystonePort := include \u0026#34;kolla_val_get_str\u0026#34; (dict \u0026#34;key\u0026#34; \u0026#34;port\u0026#34; \u0026#34;searchPath\u0026#34; $keystoneSearchPath \u0026#34;Values\u0026#34; .Values )| default \u0026#34;5000\u0026#34; }}   再重新build一次helm chart\n1  $kolla-kubernetes/tools/helm_build_all.sh .   再重新run一次修正過後的chart.\n1  helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml   確認所有服務運作正常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  $ kubectl get pod -n kolla NAME READY STATUS RESTARTS AGE cinder-api-5d6fd874b5-tzwlr 3/3 Running 0 11h cinder-create-db-x552q 0/2 Completed 0 11h cinder-create-keystone-endpoint-admin-7xhjp 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv2-252bg 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv3-4zcv7 0/1 Completed 0 11h cinder-create-keystone-endpoint-internal-s2n77 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv2-9wr7f 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv3-2srh2 0/1 Completed 0 11h cinder-create-keystone-endpoint-public-7mksf 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv2-pqhms 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv3-8z6xg 0/1 Completed 0 11h cinder-create-keystone-service-4sbp6 0/1 Completed 0 11h cinder-create-keystone-servicev2-9h88v 0/1 Completed 0 11h cinder-create-keystone-servicev3-p89wk 0/1 Completed 0 11h cinder-create-keystone-user-4whfz 0/1 Completed 0 11h cinder-manage-db-hgppr 0/1 Completed 0 11h cinder-scheduler-0 1/1 Running 0 11h glance-api-6f649fbf8d-9hwzn 1/1 Running 0 11h glance-create-db-76lwc 0/2 Completed 0 11h glance-create-keystone-endpoint-admin-m4mxm 0/1 Completed 0 11h glance-create-keystone-endpoint-internal-q9whd 0/1 Completed 0 11h glance-create-keystone-endpoint-public-stszm 0/1 Completed 0 11h glance-create-keystone-service-hcznf 0/1 Completed 0 11h glance-create-keystone-user-9f2g7 0/1 Completed 0 11h glance-manage-db-ch6rp 0/1 Completed 0 11h glance-registry-684d9cc765-d5g5p 3/3 Running 0 11h horizon-7bc45d8df6-8ndt6 1/1 Running 0 11h keystone-b55d658-4bmpf 1/1 Running 0 12h keystone-create-db-rb9wq 0/2 Completed 0 12h keystone-create-endpoints-65m86 0/1 Completed 0 12h keystone-fernet-setup-job-knfm8 0/1 Completed 0 12h keystone-manage-db-x4m2n 0/1 Completed 0 12h mariadb-0 1/1 Running 0 12h mariadb-init-element-ndbxt 0/1 Completed 0 12h memcached-7b95fd6b69-v8f4v 2/2 Running 0 12h neutron-create-db-w2hqk 0/2 Completed 0 11h neutron-create-keystone-endpoint-admin-hkg8p 0/1 Completed 0 11h neutron-create-keystone-endpoint-internal-cwzrt 0/1 Completed 0 11h neutron-create-keystone-endpoint-public-bjzzk 0/1 Completed 0 11h neutron-create-keystone-service-q7ms9 0/1 Completed 0 11h neutron-create-keystone-user-zvqnw 0/1 Completed 0 11h neutron-dhcp-agent-l5qkg 1/1 Running 0 11h neutron-l3-agent-network-64v9x 1/1 Running 0 11h neutron-manage-db-5dqkn 0/1 Completed 0 11h neutron-metadata-agent-network-ttf5v 1/1 Running 0 11h neutron-openvswitch-agent-network-j6llm 1/1 Running 0 11h neutron-server-6d74c78c98-xzdd8 3/3 Running 0 11h nova-api-7d5cf595bc-rxg4k 3/3 Running 0 11h nova-api-create-cell-r288q 0/1 Completed 0 11h nova-api-create-db-5w2lg 0/2 Completed 0 11h nova-api-manage-db-wd4b8 0/1 Completed 0 11h nova-cell0-create-db-5bz6v 0/2 Completed 0 11h nova-compute-wn5lb 1/1 Running 0 11h nova-compute-xkv8r 1/1 Running 0 11h nova-conductor-0 1/1 Running 0 11h nova-consoleauth-0 1/1 Running 0 11h nova-create-db-476gl 0/2 Completed 0 11h nova-create-keystone-endpoint-admin-xbt8x 0/1 Completed 0 11h nova-create-keystone-endpoint-internal-58dvx 0/1 Completed 0 11h nova-create-keystone-endpoint-public-8c56c 0/1 Completed 0 11h nova-create-keystone-service-jngxg 0/1 Completed 0 11h nova-create-keystone-user-4gc62 0/1 Completed 0 11h nova-libvirt-kbcrl 1/1 Running 0 11h nova-libvirt-n2nnj 1/1 Running 0 11h nova-novncproxy-79bf74796f-9p7ct 3/3 Running 0 11h nova-scheduler-0 1/1 Running 0 11h openvswitch-ovsdb-network-rwwrz 1/1 Running 0 11h openvswitch-vswitchd-network-6q9w9 1/1 Running 0 11h placement-api-create-keystone-endpoint-admin-bg9ct 0/1 Completed 0 11h placement-api-create-keystone-endpoint-internal-v998h 0/1 Completed 0 11h placement-api-create-keystone-endpoint-public-p6mvf 0/1 Completed 0 11h placement-api-fc8f68544-rvhwc 1/1 Running 0 11h placement-create-keystone-service-blj57 0/1 Completed 0 11h placement-create-keystone-user-tw5k4 0/1 Completed 0 11h rabbitmq-0 1/1 Running 0 12h rabbitmq-init-element-gtvgw 0/1 Completed 0 12h   使用官方的tool建立admin的openrc file，建立完成的檔案會存在當前使用者的home目錄下。\n1  $kolla-kubernetes/tools/build_local_admin_keystonerc.sh ext   接者安裝OpenStack clients套件\n1 2 3  $sudo pip install \u0026#34;python-openstackclient\u0026#34; $sudo pip install \u0026#34;python-neutronclient\u0026#34; $sudo pip install \u0026#34;python-cinderclient\u0026#34;   使用openstack語法去產生image,network\u0026hellip;等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $source ~/keystonerc_admin $IMAGE_URL=http://download.cirros-cloud.net/0.3.5/ $IMAGE=cirros-0.3.5-x86_64-disk.img $IMAGE_NAME=cirros $IMAGE_TYPE=linux EXT_NET_CIDR=\u0026#39;172.24.10.1/24\u0026#39; EXT_NET_RANGE=\u0026#39;start=172.24.10.10,end=172.24.10.200\u0026#39; EXT_NET_GATEWAY=\u0026#39;172.24.10.1\u0026#39; $curl -L -o ./${IMAGE} ${IMAGE_URL}/${IMAGE} $openstack image create --disk-format qcow2 --container-format bare --public \\  --property os_type=${IMAGE_TYPE} --file ./${IMAGE} ${IMAGE_NAME} $openstack network create --external --provider-physical-network physnet1 \\  --provider-network-type flat public1 $openstack subnet create --no-dhcp \\  --allocation-pool ${EXT_NET_RANGE} --network public1 \\  --subnet-range ${EXT_NET_CIDR} --gateway ${EXT_NET_GATEWAY} public1-subnet openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tiny openstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.small openstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.medium openstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.large openstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge   可以使用openstack horizon操作openstack dashboard，使用admin user的帳號、密碼進行登入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  $kubectl get service -n kolla NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cinder-api ClusterIP 10.3.3.125 10.0.0.182 8776/TCP 12h glance-api ClusterIP 10.3.3.141 10.0.0.182 9292/TCP 12h glance-registry ClusterIP 10.3.3.15 \u0026lt;none\u0026gt; 9191/TCP 12h horizon ClusterIP 10.3.3.11 10.0.0.182 80/TCP 12h keystone-admin ClusterIP 10.3.3.35 10.0.0.182 35357/TCP 12h keystone-internal ClusterIP 10.3.3.228 \u0026lt;none\u0026gt; 5000/TCP 12h keystone-public ClusterIP 10.3.3.124 10.0.0.182 5000/TCP 12h mariadb ClusterIP 10.3.3.98 \u0026lt;none\u0026gt; 3306/TCP 12h memcached ClusterIP 10.3.3.140 \u0026lt;none\u0026gt; 11211/TCP 12h neutron-server ClusterIP 10.3.3.21 10.0.0.182 9696/TCP 12h nova-api ClusterIP 10.3.3.150 10.0.0.182 8774/TCP 12h nova-metadata ClusterIP 10.3.3.217 \u0026lt;none\u0026gt; 8775/TCP 12h nova-novncproxy ClusterIP 10.3.3.4 10.0.0.182 6080/TCP 12h nova-placement-api ClusterIP 10.3.3.159 10.0.0.182 8780/TCP 12h rabbitmq ClusterIP 10.3.3.66 \u0026lt;none\u0026gt; 5672/TCP 12h rabbitmq-mgmt ClusterIP 10.3.3.37 \u0026lt;none\u0026gt; 15672/TCP 12h $cat keystonerc_admin unset OS_SERVICE_TOKEN export OS_USERNAME=admin export OS_PASSWORD=kQHEss3THBmHCQWdqNd2b51U8xRB3hKPH6KD4kx3 export OS_AUTH_URL=http://10.0.0.182:5000/v3 export PS1=\u0026#39;[\\u@\\h \\W(keystone_admin)]$ \u0026#39; export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_VOLUME_API_VERSION=2    登入即可看到openstack的操作畫面。\n  -- 如果需要拆除你的OpenStack Kolla Kubernetes環境 在你的master上，使用helm拆除OpenStack相關元件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $helm install --debug ~/kolla-bringup/kolla-kubernetes/helm/service/nova-cleanup --namespace kolla --name nova-cleanup --values ~/kolla-bringup/cloud.yaml $helm delete mariadb --purge $helm delete mariadb --purge $helm delete rabbitmq --purge $helm delete memcached --purge $helm delete keystone --purge $helm delete glance --purge $helm delete cinder-control --purge $helm delete horizon --purge $helm delete openvswitch --purge $helm delete neutron --purge $helm delete nova-control --purge $helm delete nova-compute --purge $helm delete nova-cell0-create-db-job --purge $helm delete cinder-volume-lvm --purge   在每個節點上拆除相關的OpenStack volume\n1  $sudo rm -rf /var/lib/kolla/volumes/*   在每個節點上刪除Kubernetes\n1 2 3 4  $sudo kubeadm reset $sudo rm -rf /etc/kolla $sudo rm -rf /etc/kubernetes $sudo rm -rf /etc/kolla-kubernetes   ","description":"","id":30,"section":"posts","tags":["openStack","kubernetes"],"title":"Kolla Kubernetes","uri":"https://blog.jjmengze.website/posts/openstack/kolla-kubernetes/"},{"content":"  TODO","description":"","id":31,"section":"posts","tags":["GRPC","proto"],"title":"GRPC 與 Proto buffer 附身合體","uri":"https://blog.jjmengze.website/posts/protocol/grpc-server-client/"},{"content":"  什麼是Protocol Buffer Protocol Buffer 是由是 Google 所推出的一種輕量且高效的結構化資料存儲格式。\n將結構化的資料進行序列化，實現資料的傳輸以及資料的儲存。\nProtocol Buffer 比 XML、Json 更小、更快、使用及維護上更加的簡單（可以將 api speic 直接轉換成魏應的程式語言）！\n優點  體積小 跨平台 跨語言 傳輸速度快 維護成本低 序列化速度快  定義資料結構 結構的定義很簡單，檔案以 .proto 作為後輟。\nCoding style可以參考這裡：https://developers.google.com/protocol-buffers/docs/style\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  syntax = \u0026#34;proto3\u0026#34;;package tutorial;message Employee { string name = 1; int32 id = 2; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 3;}  第一行指定您正在使用proto3語法\n接下來package定義在golang裡所屬於的package\nmessage定義傳輸的訊息\n訊息內容有name、id、枚舉以及一組內部phone number的message\n編譯所撰寫好的.proto 安裝編譯時所需要用的套件，也可以參考官方的方法決定安裝的方式。\n1  go get -u github.com/golang/protobuf/protoc-gen-go   \n\n  \n\n我們使用剛剛安裝的套件進行編譯成 go library 或是可以編成 java python \u0026hellip;. 等的 library 以達到跨平台跨語言的支持，生成的 library 直接幫我們寫好壓縮以及解壓縮的方法開發者只要專心撰寫自己的業務邏輯即可，本篇文章以 go 作為範例。\n1  protoc --go_out=. *.proto   編譯完成後的檔案會是樣的命名方式：employee.pb.go\n檔案內容會如同下面的範例所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170  // Code generated by protoc-gen-go. DO NOT EDIT. // source: employee.proto  package tutorial import proto \u0026#34;github.com/golang/protobuf/proto\u0026#34; import fmt \u0026#34;fmt\u0026#34; import math \u0026#34;math\u0026#34; // Reference imports to suppress errors if they are not otherwise used. var _ = proto.Marshal var _ = fmt.Errorf var _ = math.Inf // This is a compile-time assertion to ensure that this generated file // is compatible with the proto package it is being compiled against. // A compilation error at this line likely means your copy of the // proto package needs to be updated. const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package  type Employee_PhoneType int32 const ( Employee_MOBILE Employee_PhoneType = 0 Employee_HOME Employee_PhoneType = 1 Employee_WORK Employee_PhoneType = 2 ) var Employee_PhoneType_name = map[int32]string{ 0: \u0026#34;MOBILE\u0026#34;, 1: \u0026#34;HOME\u0026#34;, 2: \u0026#34;WORK\u0026#34;, } var Employee_PhoneType_value = map[string]int32{ \u0026#34;MOBILE\u0026#34;: 0, \u0026#34;HOME\u0026#34;: 1, \u0026#34;WORK\u0026#34;: 2, } func (x Employee_PhoneType) String() string { return proto.EnumName(Employee_PhoneType_name, int32(x)) } func (Employee_PhoneType) EnumDescriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } type Employee struct { Name string `protobuf:\u0026#34;bytes,1,opt,name=name\u0026#34; json:\u0026#34;name,omitempty\u0026#34;` Id int32 `protobuf:\u0026#34;varint,2,opt,name=id\u0026#34; json:\u0026#34;id,omitempty\u0026#34;` Phones []*Employee_PhoneNumber `protobuf:\u0026#34;bytes,3,rep,name=phones\u0026#34; json:\u0026#34;phones,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee) Reset() { *m = Employee{} } func (m *Employee) String() string { return proto.CompactTextString(m) } func (*Employee) ProtoMessage() {} func (*Employee) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0} } func (m *Employee) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee.Unmarshal(m, b) } func (m *Employee) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee.Marshal(b, m, deterministic) } func (dst *Employee) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee.Merge(dst, src) } func (m *Employee) XXX_Size() int { return xxx_messageInfo_Employee.Size(m) } func (m *Employee) XXX_DiscardUnknown() { xxx_messageInfo_Employee.DiscardUnknown(m) } var xxx_messageInfo_Employee proto.InternalMessageInfo func (m *Employee) GetName() string { if m != nil { return m.Name } return \u0026#34;\u0026#34; } func (m *Employee) GetId() int32 { if m != nil { return m.Id } return 0 } func (m *Employee) GetPhones() []*Employee_PhoneNumber { if m != nil { return m.Phones } return nil } type Employee_PhoneNumber struct { Number string `protobuf:\u0026#34;bytes,1,opt,name=number\u0026#34; json:\u0026#34;number,omitempty\u0026#34;` Type Employee_PhoneType `protobuf:\u0026#34;varint,2,opt,name=type,enum=tutorial.Employee_PhoneType\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee_PhoneNumber) Reset() { *m = Employee_PhoneNumber{} } func (m *Employee_PhoneNumber) String() string { return proto.CompactTextString(m) } func (*Employee_PhoneNumber) ProtoMessage() {} func (*Employee_PhoneNumber) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } func (m *Employee_PhoneNumber) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee_PhoneNumber.Unmarshal(m, b) } func (m *Employee_PhoneNumber) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee_PhoneNumber.Marshal(b, m, deterministic) } func (dst *Employee_PhoneNumber) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee_PhoneNumber.Merge(dst, src) } func (m *Employee_PhoneNumber) XXX_Size() int { return xxx_messageInfo_Employee_PhoneNumber.Size(m) } func (m *Employee_PhoneNumber) XXX_DiscardUnknown() { xxx_messageInfo_Employee_PhoneNumber.DiscardUnknown(m) } var xxx_messageInfo_Employee_PhoneNumber proto.InternalMessageInfo func (m *Employee_PhoneNumber) GetNumber() string { if m != nil { return m.Number } return \u0026#34;\u0026#34; } func (m *Employee_PhoneNumber) GetType() Employee_PhoneType { if m != nil { return m.Type } return Employee_MOBILE } func init() { proto.RegisterType((*Employee)(nil), \u0026#34;tutorial.Employee\u0026#34;) proto.RegisterType((*Employee_PhoneNumber)(nil), \u0026#34;tutorial.Employee.PhoneNumber\u0026#34;) proto.RegisterEnum(\u0026#34;tutorial.Employee_PhoneType\u0026#34;, Employee_PhoneType_name, Employee_PhoneType_value) } func init() { proto.RegisterFile(\u0026#34;employee.proto\u0026#34;, fileDescriptor_employee_7c804fd7a46a4aa3) } var fileDescriptor_employee_7c804fd7a46a4aa3 = []byte{ // 208 bytes of a gzipped FileDescriptorProto \t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0xe2, 0x4b, 0xcd, 0x2d, 0xc8, 0xc9, 0xaf, 0x4c, 0x4d, 0xd5, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x28, 0x29, 0x2d, 0xc9, 0x2f, 0xca, 0x4c, 0xcc, 0x51, 0x7a, 0xc3, 0xc8, 0xc5, 0xe1, 0x0a, 0x95, 0x14, 0x12, 0xe2, 0x62, 0xc9, 0x4b, 0xcc, 0x4d, 0x95, 0x60, 0x54, 0x60, 0xd4, 0xe0, 0x0c, 0x02, 0xb3, 0x85, 0xf8, 0xb8, 0x98, 0x32, 0x53, 0x24, 0x98, 0x14, 0x18, 0x35, 0x58, 0x83, 0x98, 0x32, 0x53, 0x84, 0xcc, 0xb8, 0xd8, 0x0a, 0x32, 0xf2, 0xf3, 0x52, 0x8b, 0x25, 0x98, 0x15, 0x98, 0x35, 0xb8, 0x8d, 0xe4, 0xf4, 0x60, 0x66, 0xe9, 0xc1, 0xcc, 0xd1, 0x0b, 0x00, 0x29, 0xf0, 0x2b, 0xcd, 0x4d, 0x4a, 0x2d, 0x0a, 0x82, 0xaa, 0x96, 0x0a, 0xe7, 0xe2, 0x46, 0x12, 0x16, 0x12, 0xe3, 0x62, 0xcb, 0x03, 0xb3, 0xa0, 0x96, 0x41, 0x79, 0x42, 0x06, 0x5c, 0x2c, 0x25, 0x95, 0x05, 0xa9, 0x60, 0x0b, 0xf9, 0x8c, 0x64, 0x70, 0x19, 0x1e, 0x52, 0x59, 0x90, 0x1a, 0x04, 0x56, 0xa9, 0xa4, 0xcd, 0xc5, 0x09, 0x17, 0x12, 0xe2, 0xe2, 0x62, 0xf3, 0xf5, 0x77, 0xf2, 0xf4, 0x71, 0x15, 0x60, 0x10, 0xe2, 0xe0, 0x62, 0xf1, 0xf0, 0xf7, 0x75, 0x15, 0x60, 0x04, 0xb1, 0xc2, 0xfd, 0x83, 0xbc, 0x05, 0x98, 0x92, 0xd8, 0xc0, 0xfe, 0x37, 0x06, 0x04, 0x00, 0x00, 0xff, 0xff, 0x5c, 0x1e, 0x53, 0x56, 0x11, 0x01, 0x00, 0x00, }   小結 下一章節會使用本篇基於 protobuffer 建置出來的 go library 去撰寫一個簡單的 client server，並且透過 GRPC 去傳輸 protobuffer。\n","description":"","id":32,"section":"posts","tags":["GRPC","proto"],"title":"初嚐Protocol Buffer好滋味","uri":"https://blog.jjmengze.website/posts/protocol/grpc/"},{"content":"  記錄一下，Open vSwitch (OVS)常用的指令。\n在ubuntu上安裝ovs\nsudo apt install openvswitch-switch -y 新增一個bridge sudo ovs-vsctl add-br br0 顯示所有bridge sudo ovs-vsctl list-br 給某個bridge新增一個port sudo ovs-vsctl add-port br0 veth1 顯示綁定到該port的bridge sudo ovs-vsctl port-to-br veth1 連接controller到ovs br0上 sudo ovs-vsctl set-controller br0 tcp:\u0026lt;controller IP\u0026gt; 顯示所有的bridge、連接的port以及controller sudo ovs-vsctl show ","description":"","id":33,"section":"posts","tags":["SDN"],"title":"OVS 常用的指令","uri":"https://blog.jjmengze.website/posts/sdn/ovs-commonly-command/"},{"content":"  kubernetes Controller Manager 的那些元件 在 Kubernetes Master Node 另外一個相當重要的元件可視為整體 Kubernetes 系統架構的監控狀態中心，也是就是 Controller Manager 元件，主要由以下幾種控制器控制 Kubernetes Container 叢集的狀態：\n  Replication Controller ：監控叢集的副本狀態並盡可能修正副本狀態。\n  Node Controller ： 監控叢集的 Node 狀態並負責 Node 狀態的更新。\n  CronJob Controller ：監控叢集週期性任務的狀態並負責任務狀態的更新。\n  DaemonSet Controller ：監控叢集 Daemon 資源型態的狀態並負責其狀態的更新。\n  Deployment Controller ：監控叢集 Deployment 資源型態的狀態並負責與 Replication Controller 進行交互與狀態的更新。\n  Endpoint Controller ：監控叢集 Pod 的 IP 狀態並負責與 Service Controller 進行資料的交換。\n  Garbage Collector Controller ：收集已被叢集刪除的元件確認是否還有依賴的資源未被刪除。\n  Namespace Controller ：維護並且監控叢集 Namespace 資源型態建立與刪除的狀態。\n  Job Controller ：監控叢集單次任務的狀態並負責任務狀態的更新。\n  Pod AutoScaler Controller ：監控叢集 Pod 水平擴展以及負責該擴展的狀態。\n  RelicaSet Controller ：監控叢集副本升級狀態以及維護該升級資訊。\n  Service Controller ：監控叢集服務暴露資訊之狀態並與 Endpoint Controller 進行資料的交互更新。\n  ServiceAccount Controller ：負責監控叢集服務應用的帳戶認證與權限控管。\n  StatefulSet Controller ：監控叢集現有狀態服務的變化負責其資源的建立與刪除。\n  Volume Controller ：監控叢集掛載 volume 資訊與狀態並負責即時更新。\n  Resource quota Controller ：監控叢集 CPU 與 Memory 資源資訊並回饋給 API-Server 當前資源狀態。\n    圖引用自superuser\n","description":"","id":34,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(controller-manger)","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-controller-manger-overview/"},{"content":"  kubernetes control plane 的那些元件 如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態，所以了解 Kubernetes 的大腦是一件相當重要的事情。\n  在 Master Node 主要由 API-Server 、 Controller Manager 以及 Scheduler 三大基礎元件所組合而成，其中 API-Server 元件可視為整體 Kubernetes Container 調度系統架構的管理中心，也是 Kubernetes Container 叢集調度系統中唯一能與後端儲存 etcd 元件溝通的元件，主要提供以下服務：\n  API-Server 提供叢集安全且可靠的使用機制，所有的 Client 請求都需要經過 API-Server 的認證。 API-Server 支持多種認證機制如 WebSocket 、 Keystone 、 Token 等等，如果 API-Server 認證成功， Client 請求將會傳入 Kubernetes 叢集內進行處理；而對於認證失敗的請求則返回 HTTP 401 的錯誤。\n HTTPS Certificate Authority ，基於 Certificate Authority (CA) 證書簽名的雙向數字證書認證方式。 HTTP Token Authority ，透過發放 Token 的方式識別每個使用者的身份，以及透過該 Token 限制使用者能存取的範圍。 HTTP Basic Authority ，當使用者向 API Server 發起 HTTP 請求時，會在資料中帶入 Username 、 UID 、 Groups 、 Extra fields 作為使用者的身份驗證的依據。    API-Server 同時提供 REST API 的呼叫介面如 Create 、 Read 、 Update 、 Delete 等一系列操作對 Kubernetes 的 Pod 、 Service 、 Deployment 等元件進行操作，例如使用者可以透過 REST API 向 API-Server 請求建立、刪除、更新以及讀取 Kubernetes 的資源。\n  API-Server 元件提供專有的監控介面給 Controller Manger 元件與 Scheduler 元件，可以透過該專有介面讀取特定資料，例如 Controller Manger 透過 API-Server 提供的介面讀取 Pod 的狀態並且加以控制， Scheduler 透過介面讀取 Kubernetes Node 目前的壓力情況作為排程 Pod 的參考依據。\n  大致上流程如下圖所示，當請求送進 kubernetes cluster 會通過Api server的認證，接著檢查該次請求的內容請求，最後再送進 etcd 做永久性儲存。（以後有機會再來談談Authoization跟Admission control 的細節）\n引用giantswarm.io\n","description":"","id":35,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(api-server)","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-apiserver-overview/"},{"content":"  來說說etcd Kubernetes Container 叢集調度系統中後端採用 etcd 作為叢集狀態的儲存元件，而該 etcd 元件是由 CoreOS 公司開發並且開源貢獻給 CNCF 的 key/value 儲存專案。 etcd 主要負責將 Kubernetes API-Server 所處理過的資料進行加密儲存。 etcd 元件大多數會位於 Kubernetes 系統架構中的 Master Node 上。 API-Server 若要存取 etcd 上的資料必須先經過 etcd 的身分驗證流程確認使用者存取的身份與範圍是否合法，若身分確認無誤與存取範圍合法則將使用者所請求的資料透過 REST API 回傳資料給使用者。\nKubernetes 在通常情況下只會替系統管理員在 Master Node 上建立一個 etcd 元件，可能因為外在因素或是 etcd 請求負載過於龐大使得 etcd 元件發生故障，進而導致整體 Kubernetes發生API-Server 無法存取後端資料的問題。針對此問題系統管理員可以透過 Kubernetes Container 叢集調度技術建立一個高可用的 etcd 服務叢集，若單一個 etcd 元件發生故障時仍有其他 etcd 元件可以立即地補上作為備援使用。\netcd 怎麼在分散的世界保持一致 etcd 元件組成 etcd 叢集服務時首要任務就是確保後端資料是一制性的，以防使用者存取到錯誤的資料， etcd 採用的是 Raft 一至性共識演算法。\n透過 Raft 演算法使得在同一個時間點上可以維持多個 etcd 元件儲存的狀態，這就保證了後端儲存的資料是保持一致，此外該演算法保證了當少數 etcd 元件崩潰或失效時仍不影響整體叢集的同步。\nRaft 就是一種 leader-based 的共識算法並且使用心跳機制 (Heartbeat mechanism) 觸發同步與選舉，在該演算法裡面分別有三種身份第一種是 Leader 主要是叢集的領導人其他身份都必須跟領導人的資料同步，第二種是 Candidate 是當領導人無回應時欲參加該次選舉的節點稱為候選人，最後一種為 Follower 在選舉期間負責投票給候選人的節點如圖所示。\n選舉的過程   一開始叢集各個節點身份都為Follower，並且節點上預設的選舉逾時時間(Election Time Out)與一般作業時間(Normal Operation)皆為範圍隨機亂數。在一般作業時間內節點會期待自己收到Leader週期性傳送過來的Heartbeat並且重置一般作業時間，若是超過一般作業時間節點都沒有收到Leader所傳送過來的Heartbeat該節點會當作目前沒有Leader的狀態，將觸發Leader的重選。\n  由於節點觸發Leader的重選此時節點將從Follower身份轉變為Candidate身份並且紀錄當前任期的號碼向叢集內其餘節點發送投票之請求，在觸發選舉後會有一段選舉逾時時間在該時間內，對於同一任期Follower只能選投一名Candidate，Candidate收到大於等於(2n+1) Follower的同意票，則該節點從Candidate身份轉變為叢集的新Leader。若在選舉逾時時間結束沒有收到(2n+1) Follower的同意票則視為當前選舉無效，再增加一次當前任期重新發起新一輪的選舉。而新的Leader需透過週期性的發送Heartbeat給各個節點來維持該Leader身份。\n  當發生在選舉逾時時間結束沒有收到(2n+1) Follower的同意票，無法選出Leader的情況，根據Raft演算法會讓節點縮短選舉逾時時間加速選出叢集的Leader以減少叢集無法同步的時間，另外在選舉期間節點可能會收到來自其他節點宣稱自己是Leader的心跳，該節點會確認心跳訊息內的任期編號是否大於當前任期，若是大於當前任期所有節點身份轉變為Follower身份並且與該Leader同步資料，若是心跳訊息的任期編號小於當前任期則無視該節點，繼續任期投票工作。\n  當叢集的Leader被選出之後，Leader會將使用者的每個請求都包裝成一個Commit並且透過Heartbeat週期性的複製到其他節點做為副本，當一個commit超過半數的節點都以複製並儲存才會能算該commit成功。此外該commit的元資料(metadata)會記錄當時commit的Leader任期號碼是多少，該任期號用來判斷節點之間副本不一致情形並且在每個一個節點儲存Commit都會有一個整數索引值來確認其在Commit目前已經儲存到第幾個位址如圖所示。\n  各個節點發生儲存內容不一致的情況，若任期三是第三節點繼續當Leader，在Leader發送Heartbeat的同時會將其他節點的資料強制覆蓋過去，利用Leader的強制性解決資料不統一的情況。第三節點發送一次Heartbeat強制同步個節點的index的Commit，如下圖所示。\n後話 會繼續接著把 Kubernetes 的其他元件繼續講解一遍，如果有誤的地方歡迎大家指出謝謝～～\nREF  \u0026ldquo;Etcd | Coreos\u0026rdquo;. Coreos.Com, 2019, https://coreos.com/blog/etcd. Oliveira, Caio, et al. \u0026ldquo;Evaluating raft in docker on kubernetes.\u0026rdquo; International Conference on Systems Science. Springer, Cham, 2016. ","description":"","id":36,"section":"posts","tags":["kubernetes"],"title":"etcd 那回事","uri":"https://blog.jjmengze.website/posts/kubernetes/etcd/"},{"content":"  什麼是 Kubernetes Kubernetes 是一個以 Apache 授權條款的開源專案主要作為 Container 叢集的系統管理平台，其系統具備高度可擴展性，由 Joe Beda、Brendan Burns 和 Craig McLuckie 等 Google 工程師建立此套系統。\nKubernetes v1.0於2015年7月21日發布。隨著v1.0版本發布，Google與Linux 基金會合作建立了Cloud Native Computing Foundation (CNCF)，並把Kubernetes作為種子技術來提供。\n來看看架構圖 Kubernetes Container 叢集系統管理平台，將能夠透過Kubernetes 的機制提供VNF擁有更好的可靠性、負載平衡、滾動更新等特性。然而在 Kubernetes Container 叢集管理平台系統中，擁有不同的節點角色與基礎元件。\n如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態。\n  就 Master Node 而言 Kubernetes 提供了一種方便操作的 kubectl 命令列介面工具，對於管理人員來說，只要透過 kubectl 就能向 Kubernetes Container 叢集下達指令並得到當前叢集服務運行的狀態。 Kubernetes 同時考量安全性的問題，當任何人員想要對叢集進行操作都必須先經過 Kubernetes 的身份驗證，當驗證成功叢集才會接收操作命令反之亦然。\n當驗證成功後所有的操作行為例如：部署 Container 服務、存取 Container 服務執行的狀態、刪除 Container 服務等行為，都會經由 Kubernetes Master Node 上的 API-Server 透過gRPC通訊協定向其他元件進行溝通並取回資料，此外在 Kubernetes 叢集每個節點上都會運行一個名為 kubelet 元件，該 kubelet 元件主要負責接收 Master Node 所指派的任務並在該節點上部署 Container 服務並且時刻的監視 node 上的 Container 服務的狀態，並且透過 gRPC 通訊協定回報給 Kubernetes Master Node上的API-Server，當 API-Server 收到節點資訊並彙整後將存入後端的 etcd key/value 資料庫。\n後話 後續包含Scheduler、Controller Manager、API-Server、etcd、kube-proxy、kubelet 等元件，將一一介紹 Kubenetes 架構中的主要元件與相關的資源型態。\n","description":"","id":37,"section":"posts","tags":["kubernetes"],"title":"Kubernetes 淺入淺出","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes/"},{"content":"  VM 與 Container 架構圖 與 VM 相比使用 Container 具有各種優點，在要執行數百個應用程式的使用情境之下使用基於 Container 的解決方案顯得相當高效， Container 所佔的空間以及效能損失都優於 VM 。在需要快速回收資源與啟動服務的應用情境下，透過 Container 承載這些服務遠比選擇 VM 的效果要好，由於 Container 啟動的速度只需要幾十秒的時間， VM 啟動所需要的時間卻高達數分鐘。\nVM 與 Container 比較表     VM Container     Communication 透過Ethernet Devices 透過IPC：Socket、pipes、Signals等機制   Guest OS 獨立的Kernel 共用宿主Kernel   Performance 效能消耗較大 效能消耗較小   Isolation 隔離性較好 隔離性較差   Startup time 數分鐘 幾十秒   Storage 佔較大空間 佔較小空間    僅透過 VM ，將相關的 service 部署到 VM 中以用來處理使用者請求的服務，由於 VM 的效能損失以及啟動時間的問題，採用 container 的方式可以大幅度節省效能的損失以及加速部署與產品上限的時間。\n但是對於部署規模較大的運算叢集來說，Container 技術僅能縮短服務啟動的時間，並且沒有一個方便管理 Container 化叢集的方式與確認當前 Container 服務運行的狀態。若管理人員不小心刪除叢集的某一服務將造成災難性的危害。因此，需要建置一個 Container 叢集管理平台管理 Container 服務。\nGoogle 累積多年管理 Container 叢集經驗，進而開發 Kubernetes Container 叢集調度技術的開源專案，同時也是目前業界廣為採用的 Container 叢集管理系統如公有雲的 AWS 的 EKS 、 Google 提供的 GKE 以及 Azure 的 AKS ，對於管理人員來說使用 Kubernetes 除了能大量減少建置叢集的複雜流程外，也可以透過 Kubernetes 內建的元件建置所需要 Container 服務，透過 Kubernetes 可以提高服務的可用性，因此以下將進一步說明 Kubernetes 的系統架構以及相關元件。\nREF  Xavier, Miguel G., et al. \u0026ldquo;Performance evaluation of container-based virtualization for high performance computing environments.\u0026rdquo; 2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. IEEE, 2013. Soltesz, Stephen, et al. \u0026ldquo;Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors.\u0026rdquo; ACM SIGOPS Operating Systems Review. Vol. 41. No. 3. ACM, 2007. Burns, Brendan, et al. \u0026ldquo;Borg, omega, and kubernetes.\u0026rdquo; (2016). Cherrueau, Ronan-Alexandre, et al. \u0026ldquo;Edge computing resource management system: a critical building block! initiating the debate via openstack.\u0026rdquo; {USENIX} Workshop on Hot Topics in Edge Computing (HotEdge 18). 2018. Web.Kaust.Edu.Sa, 2019, http://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS240/F17/slides/L3-cloud-VM.pdf ","description":"","id":38,"section":"posts","tags":["kubernetes"],"title":"來看看Container","uri":"https://blog.jjmengze.website/posts/kubernetes/container/"}]