[{"content":"  在 Kubernetes 資源是怎麼被回收的？ 是由 kubernetes master node 上的 kube-controller-manger 中的 garbage-collection controller 進行回收管理的。\n我們先不管 kubernetes 底層是怎麼回收這些垃圾物件的，先把焦點放在我們要怎麼設定yaml檔，畢竟我們是yaml工程師嘛(笑)，本篇文章會用進行幾個實驗來觀察各種 kubernetes 回收策略是如何進行的。\n當我們刪除物件時，可以指定該物件底下關聯的子物件是否也跟著自動刪除。\n 自動刪除附屬的行為也稱為 級聯刪除（Cascading Deletion）  Kubernetes 中有兩種 Cascading Deletion (聯集) 刪除分別是：\n 後台（Background） 模式 前台（Foreground） 模式。  Foreground 條件  物件的 metadata.finalizers 被設定為 foregroundDeletion 物件處於 deletion in progress 狀態（deletionTimestamp被建立）  行為  需要等到物件所有的關聯的子物件被删除完之後，才可以删除該物件 如何確定關聯的子物件的父親是誰？  透過子物件的 ownerReferences 來確定    Background kubernetes 立刻 馬上 删除物件， garbage-collection controller 會在後台(背景)删除該物件的子物件。\n除了在背景刪除子物件的行為外還有一種是不刪除子物件，讓子物件變成孤兒(Orphan)。\n實驗 propagation Policy (Foreground) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的deployment ReplicaSet 以及pod的狀態\n1 2 3 4 5 6 7 8 9  kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s   取得ReplicaSet與pod的ownerReferences 用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:597d36f5-968a-4025-8621-b24f17f7f3a6]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:97fb6974-6882-4459-b6b0-b39357a7650b]]   destroy Foreground 透過指定的刪除模式來刪除物件，這裡是透過Foreground 的方式刪除物件。\ncurl -X DELETE curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\ -d '{\u0026quot;kind\u0026quot;:\u0026quot;DeleteOptions\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;propagationPolicy\u0026quot;:\u0026quot;Foreground\u0026quot;}' \\ -H \u0026quot;Content-Type: application/json\u0026quot; 狀態 取的deployment 以及pod的狀態觀察刪除狀態。\n從這個狀態可以看到所有的 pod 都在 Terminating 的狀態， ReplicaSet 以及 deployment 都沒有先移除。\n1 2 3 4 5 6 7 8 9 10 11  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-nkbrb 0/1 Terminating 0 133m pod/nginx-deployment-6b474476c4-zh5g7 0/1 Terminating 0 133m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 0/3 0 0 133m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 0 0 133m   比較差異 比較刪除前後的差異\nDeployment Deployment狀態的差異\n從 diff 的狀態可以看出來，在 metadata 的部分修改了 finalizers 並且指定了 foregroundDeletion 的模式表示使用 foreground 刪除模式，另外新增了 deletionTimestamp 的時間確定了物件的刪除時間。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  --- a/dpeloy.yaml +++ b/dpeloy.yaml - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2  namespace: default - resourceVersion: \u0026#34;1480766\u0026#34; + resourceVersion: \u0026#34;1499040\u0026#34;  ... status: - availableReplicas: 3  conditions: - - lastTransitionTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - lastUpdateTime: \u0026#34;2020-08-10T06:10:18Z\u0026#34; - message: Deployment has minimum availability. - reason: MinimumReplicasAvailable - status: \u0026#34;True\u0026#34; - type: Available  ... - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 - updatedReplicas: 3 + - lastTransitionTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + lastUpdateTime: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + message: Deployment does not have minimum availability. + reason: MinimumReplicasUnavailable + status: \u0026#34;False\u0026#34; + type: Available + observedGeneration: 2 + unavailableReplicas: 3    ReplicaSet 觀察ReplicaSet的變化也是確定了刪除的時間 deletionTimestamp 以及修改時間 time ，以及是透過 foregroundDeletion 的策略進行刪除。\n在狀態欄的地方也能看出現在狀態replicas的數量被縮減為0個。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  --- a/rs.yaml +++ b/rs.yaml  creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; - generation: 1 + deletionGracePeriodSeconds: 0 + deletionTimestamp: \u0026#34;2020-08-10T08:22:55Z\u0026#34; + finalizers: + - foregroundDeletion + generation: 2 ... - time: \u0026#34;2020-08-10T06:10:18Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:55Z\u0026#34; ... ownerReferences: @@ -86,7 +87,7 @@ metadata:  kind: Deployment name: nginx-deployment uid: 597d36f5-968a-4025-8621-b24f17f7f3a6 - resourceVersion: \u0026#34;1480765\u0026#34; + resourceVersion: \u0026#34;1499039\u0026#34; ... terminationGracePeriodSeconds: 30 status: - availableReplicas: 3 - fullyLabeledReplicas: 3 - observedGeneration: 1 - readyReplicas: 3 - replicas: 3 + observedGeneration: 2 + replicas: 0   Pod pod 的部分也想當的簡單，因為沒有子物件所以只要確定刪除時間 deletionTimestamp 以及修改時間 time 。\npod的狀態會被修改成 pending 以及相關的資源都會被移除例如: pod ip。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  --- a/pod.yaml +++ b/pod.yaml  creationTimestamp: \u0026#34;2020-08-10T06:09:12Z\u0026#34; + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T08:23:25Z\u0026#34; ... - time: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + time: \u0026#34;2020-08-10T08:22:57Z\u0026#34;  uid: 97fb6974-6882-4459-b6b0-b39357a7650b - resourceVersion: \u0026#34;1480754\u0026#34; + resourceVersion: \u0026#34;1499048\u0026#34; status: \u0026#34;True\u0026#34; type: Initialized - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34;  type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T06:10:16Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T08:22:57Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; ... containerStatuses: - - containerID: docker://f20bbce2b58ac42426b61fc21e3f1a61938a51a4dc30277f50e9ac7aea88aa3d - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34;  lastState: {} name: nginx - ready: true + ready: false  restartCount: 0 - started: true + started: false  state: - running: - startedAt: \u0026#34;2020-08-10T06:10:16Z\u0026#34; + waiting: + reason: ContainerCreating  hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.5 - podIPs: - - ip: 10.32.0.5 + phase: Pending   實驗propagationPolicy (Background) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的Deployment ReplicaSet 以及Pod的狀態\n1 2 3 4 5 6 7 8 9 10 11  kubectl get deploy,rs,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 55s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 55s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-6qvvg 1/1 Running 0 55s pod/nginx-deployment-6b474476c4-plsn7 1/1 Running 0 55s   取得ReplicaSet與pod的ownerReferences用來確定物件之間的關係\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  kubectl get replicaset.apps/nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:8a2be904-c306-426c-881e-c6914415c5fe]] kubectl get pod nginx-deployment-6b474476c4-6qvvg -o go-template --template={{.metadata.ownerReferences}} [map[ apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:565540ad-fbf1-4d74-8841-f0475e12a200]]   destroy background 1 2 3 4  curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Background\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34;   狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在Terminating 的狀態，但是replicaset 以及 deployment 都先被移除。\n1 2 3 4 5  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-4hjx8 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-6qvvg 0/1 Terminating 0 31m pod/nginx-deployment-6b474476c4-plsn7 0/1 Terminating 0 31m   比較差異 deployment deployment狀態的差異\n可以看到deployment 直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  --- a/dpeloy.yaml +++ b/dpeloy.yaml -apiVersion: apps/v1 -kind: Deployment -metadata: - annotations: - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - kubectl.kubernetes.io/last-applied-configuration: | - {\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3,\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.14.2\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;containerPort\u0026#34;:80}]}]}}}} - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - managedFields: - - apiVersion: apps/v1 ... ...   replicaset 可以看到 replicaset 也是直接被殺掉了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  --- a/rs.yaml +++ b/rs.yaml -apiVersion: apps/v1 -kind: ReplicaSet -metadata: - annotations: - deployment.kubernetes.io/desired-replicas: \u0026#34;3\u0026#34; - deployment.kubernetes.io/max-replicas: \u0026#34;4\u0026#34; - deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; - creationTimestamp: \u0026#34;2020-08-10T10:05:25Z\u0026#34; - generation: 1 - labels: - app: nginx - pod-template-hash: 6b474476c4 - managedFields: - - apiVersion: apps/v1 - fieldsType: FieldsV1 - fieldsV1: - f:metadata: - f:annotations: - .: {} - f:deployment.kubernetes.io/desired-replicas: {} - f:deployment.kubernetes.io/max-replicas: {} - f:deployment.kubernetes.io/revision: {} ... ...   pod 可以看到 pod 是緩慢的回收 ， 可以看到被設定了移除的時間以及相關狀態。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  --- a/pod.yaml +++ b/pod.yaml - time: \u0026#34;2020-08-10T10:05:25Z\u0026#34; + time: \u0026#34;2020-08-10T10:08:20Z\u0026#34;  + deletionGracePeriodSeconds: 30 + deletionTimestamp: \u0026#34;2020-08-10T10:08:20Z\u0026#34;  generateName: nginx-deployment-6b474476c4- - resourceVersion: \u0026#34;1513382\u0026#34; + resourceVersion: \u0026#34;1513717\u0026#34; ... - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34;  type: Ready - lastProbeTime: null - lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; - status: \u0026#34;True\u0026#34; + lastTransitionTime: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + message: \u0026#39;containers with unready status: [nginx]\u0026#39; + reason: ContainersNotReady + status: \u0026#34;False\u0026#34; - - containerID: docker://bb34d6af8dbe1c72c423fece3d9d797ec8a5a0b62fd82f1f46bfcf5d67157be1 - image: nginx:1.14.2 - imageID: docker-pullable://nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d + - image: nginx:1.14.2 + imageID: \u0026#34;\u0026#34;  lastState: {} name: nginx - ready: true + ready: false  restartCount: 0 - started: true + started: false  state: - running: - startedAt: \u0026#34;2020-08-10T10:08:20Z\u0026#34; + waiting: + reason: ContainerCreating  hostIP: 172.18.0.5 - phase: Running - podIP: 10.32.0.6 - podIPs: - - ip: 10.32.0.6 + phase: Pending ...   實驗 propagation Policy (Orphan) deploy 部署測試的nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 EOF deployment.apps/nginx-deployment created   狀態 取的deployment replicaset 以及pod的狀態\n1 2 3 4 5 6 7 8 9  kubectl get deploy,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 4m44s NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-nbtkw 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-nkbrb 1/1 Running 0 4m44s pod/nginx-deployment-6b474476c4-zh5g7 1/1 Running 0 4m44s   取得replicaset與pod的ownerReferences確定物件之間的關係\n1 2 3 4 5 6 7  kubectl get rs nginx-deployment-6b474476c4 -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:Deployment name:nginx-deployment uid:b1d1a61d-8b51-4511-8c91-de44aaa2cdd0]] kubectl get pod nginx-deployment-6b474476c4-nbtkw -o go-template --template={{.metadata.ownerReferences}} [map[apiVersion:apps/v1 blockOwnerDeletion:true controller:true kind:ReplicaSet name:nginx-deployment-6b474476c4 uid:b052f80f-d72a-4e32-a4c4-f598274f2b07]]   destroy Orphan 1 2 3  curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \\  -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\  -H \u0026#34;Content-Type: application/json\u0026#34;   狀態 取的deployment 以及pod的狀態\n從這個狀態可以看到所有的 pod 都在 Running 的狀態，ReplicaSet 以及 Pod 都沒有被移除，只有 Deployment 被殺掉而已。\n1 2 3 4 5 6 7 8 9  kubectl get pod,deploy,rs NAME READY STATUS RESTARTS AGE pod/nginx-deployment-6b474476c4-d6wns 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-d7dtf 1/1 Running 0 12m pod/nginx-deployment-6b474476c4-m7rbz 1/1 Running 0 12m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-6b474476c4 3 3 3 12m   小結 從上面三個實驗可以看到不同的移除方式會有不同的結果\n  Front Ground\n需要等到關聯的子物件被刪除後才會進行清理的動作(打上deletionTimestamp)\n  BackGround\n先刪除物件(打上deletionTimestamp)，再慢慢回收子物件(打上deletionTimestamp)\n  3.Orphan\n直接把物件刪除（打上deletionTimestamp），所有子物件不做任何動作。\n","description":"","id":2,"section":"posts","tags":["kubernetes"],"title":"學習Kubernetes Garbage Collection機制","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-garbage-collection/"},{"content":"  Istio 簡介 Istio提供了一個完整的微服務應用解決方案，透過為整個服務網路提供行為監控和細粒度的控制來滿足微服務應用程序的多樣化需求。\n Istio提供了非常簡單的方式建立具有監控(monitor)、負載平衡(load balance)、服務間的認證（service-to-service authentication）\u0026hellip;等功能的網路功能，而不需要對服務的程式碼進行任何修改。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role RAM CPUs IP Address     Master 16G 8Cores 10.20.0.154   Node1 16G 8Cores 10.20.0.164   Node2 16G 8Cores 10.20.0.174    這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.11』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and docker and other package dependencies:\n1 2 3 4 5 6 7 8  $apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl  \nKubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0  \n透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker  \n將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf  \n在master節點上使用kubeadm進行kubernetes叢集的初始化\n1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16  \n會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4  You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 172.24.0.3:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config  \n在master 安裝Kubernetes CNI，這邊採用的是Calico。\n1 2  $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml  \n等所有的pod都完成，在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.11.1 node-1 Ready \u0026lt;none\u0026gt; 2m v1.11.1 node-2 Ready \u0026lt;none\u0026gt; 2m v1.11.1  \n在master節點上，下載並且安裝helm\n1 2 3  $wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz $tar zxvf helm-v2.9.1-linux-amd64.tar.gz $mv linux-amd64/helm /usr/bin  \n為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role，最後在初始化helm 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF $helm init --service-account tiller Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming!  \n完後成後可透過kubectl指令確認\n1 2 3 4 5 6  $kubectl get pod,svc -l app=helm -n kube-system NAME READY STATUS RESTARTS AGE pod/tiller-deploy-759cb9df9-b7n7j 1/1 Running 0 4m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tiller-deploy ClusterIP 10.107.71.110 \u0026lt;none\u0026gt; 44134/TCP 4m  \n安裝Istio 透過官方提供的腳本，下載Istio並安裝istioctl binary\n1 2 3  $curl -L https://git.io/getLatestIstio | sh - $cd istio-1.0.0/ $cp bin/istioctl /usr/bin/  \n在helm version 2.10.0以前的版本Istio還是需要手安裝Istio CRD\n1 2  $kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml $kubectl apply -f install/kubernetes/helm/istio/charts/certmanager/templates/crds.yaml  \n透過kubectl指令檢查Istio是否安裝成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  $kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7d8f9748c5-zd4vb 1/1 Running 0 4m istio-egressgateway-676c8546c5-fq55b 1/1 Running 0 4m istio-galley-5669f7c9b-q98ld 1/1 Running 0 4m istio-ingressgateway-5475685bbb-5jfwv 1/1 Running 0 4m istio-pilot-5795d6d695-2vfq9 2/2 Running 0 4m istio-policy-7f945bf487-brtxn 2/2 Running 0 4m istio-sidecar-injector-d96cd9459-ws647 1/1 Running 0 4m istio-statsd-prom-bridge-549d687fd9-mb99f 1/1 Running 0 4m istio-telemetry-6c587bdbc4-tzblq 2/2 Running 0 4m prometheus-6ffc56584f-xcpsj 1/1 Running 0 4m root@sdn-k8s-b4:/home/ubuntu|⇒ kubectl get pod,svc NAME READY STATUS RESTARTS AGE pod/app-debug-6b4f85c9cc-gq7nx 1/1 Running 1 28d pod/config 1/1 Running 0 1d pod/hello-55f998cd56-d2q8n 1/1 Running 0 1d pod/hellogo-bb9bd67f7-ckmkc 1/1 Running 0 1d pod/myapp-pod 0/1 Completed 0 1d pod/myapp-pod2 0/1 Completed 0 1d pod/network-controller-server-tcp-7x6vm 1/1 Running 0 1d pod/network-controller-server-tcp-kjtm9 1/1 Running 0 1d pod/network-controller-server-unix-2rnfv 1/1 Running 0 1d pod/network-controller-server-unix-lh8qh 1/1 Running 0 1d pod/nginx-6f858d4d45-vrlgb 1/1 Running 0 1d pod/onos-65df85486f-tvk5t 1/1 Running 0 1d pod/skydive-agent-9x2n4 1/1 Running 0 6h pod/skydive-agent-rww57 1/1 Running 0 6h pod/skydive-analyzer-5f9556687f-gvdbj 2/2 Running 0 6h $kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-citadel ClusterIP 10.106.166.113 \u0026lt;none\u0026gt; 8060/TCP,9093/TCP 5m istio-egressgateway NodePort 10.100.183.7 \u0026lt;none\u0026gt; 80:31607/TCP,443:31053/TCP 5m istio-galley ClusterIP 10.109.104.146 \u0026lt;none\u0026gt; 443/TCP,9093/TCP 5m istio-ingressgateway NodePort 10.101.66.117 \u0026lt;none\u0026gt; 80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:32388/TCP,8060:32100/TCP, 15030:30847/TCP,15031:32749/TCP 5m istio-pilot ClusterIP 10.102.202.205 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 5m istio-policy ClusterIP 10.97.181.32 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP 5m istio-sidecar-injector ClusterIP 10.96.165.139 \u0026lt;none\u0026gt; 443/TCP 5m istio-statsd-prom-bridge ClusterIP 10.101.82.72 \u0026lt;none\u0026gt; 9102/TCP,9125/UDP 5m istio-telemetry ClusterIP 10.108.94.224 \u0026lt;none\u0026gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 5m prometheus ClusterIP 10.104.3.226 \u0026lt;none\u0026gt; 9090/TCP 5m  \n範例：Bookinfo Application 這邊示範Istio官方提供的範例：Bookinfo Application\n1 2 3 4 5 6 7 8 9 10 11  $kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) service/details created deployment.extensions/details-v1 created service/ratings created deployment.extensions/ratings-v1 created service/reviews created deployment.extensions/reviews-v1 created deployment.extensions/reviews-v2 created deployment.extensions/reviews-v3 created service/productpage created deployment.extensions/productpage-v1 created  \n透過kubectl指令確認安裝的pod及service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-fc9649d9c-tqbcn 2/2 Running 0 1m productpage-v1-58845c779c-2lqxg 2/2 Running 0 27m ratings-v1-6cc485c997-zqvqt 2/2 Running 0 1m reviews-v1-76987687b7-hfrn2 2/2 Running 0 1m reviews-v2-86749dcd5-ffmvs 2/2 Running 0 1m reviews-v3-7f4746b959-zr4ml 2/2 Running 0 1m $kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.101.208.129 \u0026lt;none\u0026gt; 9080/TCP 1m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 27m productpage ClusterIP 10.101.11.13 \u0026lt;none\u0026gt; 9080/TCP 1m ratings ClusterIP 10.105.132.197 \u0026lt;none\u0026gt; 9080/TCP 1m reviews ClusterIP 10.103.199.76 \u0026lt;none\u0026gt; 9080/TCP 1m  \n建立一個Gateway讓叢及外部可以存取\n1  $kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  \n接著我們透過瀏覽器去存取我們服務，在網址的地方輸入http://\u0026lt;node or master ip\u0026gt;:31380/productpage，網頁會顯下圖的網站內容。\n  Bookinfo Application 網頁服務\n  不斷重新整理該服務的網頁，會發現網頁上的星星的部分有改變。從沒星星==\u0026gt;黑星星==\u0026gt;紅星星切換，分別對應到pod中的三個版本的review,預設的載平衡功能是輪詢（Round Robin）\n設定destination rule，此時還是輪詢法\n1  $kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml  \nIntelligent Routing Istio 提供了智能路由（Intelligent Routing），這邊示範如何使用Istio管理各種服務的流量\n依照版本進行路由管理 1 2 3 4 5  $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml virtualservice.networking.istio.io/productpage created virtualservice.networking.istio.io/reviews created virtualservice.networking.istio.io/ratings created virtualservice.networking.istio.io/details created   這時候我們再回到瀏覽器查看Bookinfo Application 網頁服務，不管重新整理多少次網頁，都看不到網頁上有星星的標示，因為此時所有的請求都被轉送到review v1版本上\n依照用戶進行路由管理 1 2  $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml virtualservice.networking.istio.io/reviews create   我們回到瀏覽器上，按下右上角的Sign in 的按鈕。帳號密碼都為jason，登入後會發現不管怎麼更新頁面，網頁上都是呈現黑星星的標示。\n當我們登出後，也進行更新頁面的動作，網頁上不會出現任何星星的標示。\n因為當我們登入jason後，所有的路由都請求都會被轉發到review v2版本上。\nFault injection 有時候我們的程式碼裡面會有bug，可以透過注入故障的方式發現這些潛伏在裡面的bug。\nBookinfo特別示範一個http延遲的例子，為了測BookInfo的微服務，在reviews:v2和ratings以及用戶jason之間注入七秒延遲。此測試將發Bookinfo故意塞進去的一個錯誤。\n剛剛的路由規則，如果已經被您刪除掉。請再利用以下指令加回他的路由規則。\n1 2  $kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml $kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml  \n使用以下指令注入一個http delay ，該指令在reviews:v2和ratings以及用戶jason之間注入七秒http delay\n1  $kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml  \n這時我們去存取網頁會拋出一個異常的錯誤，因為我們把服務與服務之間的存取的時間拉長，我們可以透過注入錯誤的方式發現一個存取延遲的bug。\n  注入延遲後的Bookinfo Application網頁服務\n  結論 玩過 istio 之後發現功能十分強大，但架構過於複雜當有問題出現時，維運與開發人員難以排查狀況與問題的發生點，但目前 istio 還在 1.0 版本 未來發展起來應該是一頭猛獸 ，會持續關注 service mesh 的相關議題。\nistio 背後撐腰的公司非常可怕 ，可以觀察這個專案後續的走向，作為學習的方向！！\n","description":"","id":8,"section":"posts","tags":["servicemesh","kubernetes"],"title":"service mesh 之 Istio 1.0 安裝及極度簡易操作","uri":"https://blog.jjmengze.website/posts/istio/istio1.0-install/"},{"content":"  Serverless 在進入Kubeless之前先科普一下什麼是Serverless。\n就字面上的意思來說就是沒有Server，那沒有Server又是什麼意思？\n先從設計開發到部署開始！\n我們想要做出一套系統軟體，勢必會經歷設計開發部署\u0026hellip;等流程。\n最後總會部署到單台或是多台的Server上面，當把服務架設到Server上時有許多方案可以提供我們參考。\n例如: Google 所提供的 GKE，AWS 所提供的 EC2 亦或是 Microsoft 所提供的 Azure。\n選擇完使用哪個雲服務商所提供的Cloud後，還要考慮這套系統究竟需要多少台Server、多大空間Stroage\u0026hellip;等問題。\n在微服務的架構之下，我們的每個服務會運行在各個Server上（先不考慮Container XD）\n在你的業務擴展後，我們就需要更多更多的資源，也就是更多更多的Server。\n很多微服務事實上被call到的機會很少，但他還是佔了一台Server的資源及空間。同時這些很少被使用到的微服務也一點一滴的花掉你的錢（開Server可是要錢的啊！！！）\n有了Serverless又或是稱為FaaS的出現後，我們再也不需要去關心那些Server，只要把微服務source code提交給雲服務商，雲服務商能提供一個運行的環境給你，總而言之就是開發者不需要再去關心底下Infrastructure Layer。\nKubeless 簡介 Kubeless是一個Kubernetes-native 無伺服器(Serverless)框架，只要撰寫程式（Function），而不需要了解底層基礎建設（Infrastructure Layer）。Kubeless利用Kubernetes資源提供自動擴展，API路由，監控，故障排除\u0026hellip;等功能。\n環境配置 本次安裝是使用OpenStack上的三台虛擬機器去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role RAM CPUs IP Address     Master 8G 4Cores 172.24.0.2   Node1 8G 4Cores 172.24.0.2   Node2 8G 4Cores 172.24.0.4    部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝套件 安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7  $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl  \n關閉Swap Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0  \n重啟Docker 透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker  \nIPv4 forward設定 將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf  \nCluster 初始化 在master節點上使用kubeadm進行kubernetes叢集的初始化\n1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16   1  $sudo kubeadm init --pod-network-cidr=192.168.0.0/16   會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4 5  ... You can now join any number of machines by running the following on each node as root: kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5  \n在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 172.24.0.2:6443 --token i67sjb.0nvjxbldwuh342of --discovery-token-ca-cert-hash sha256:aa23e1e7a4d55d06fbdf34fa2a1c703dd7e7cfff735b0b0fe800b4335aff68b5   在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config  \n安裝Calico CNI 1 2  $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml $kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml   檢查環境 在master上操作kubectl指令即可看到所有node呈現ready的狀態\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  $kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-node-cgtxh 2/2 Running 0 1m calico-node-qjrbm 2/2 Running 0 1m calico-node-v59b2 2/2 Running 0 2m coredns-78fcdf6894-dz9fs 1/1 Running 0 4m coredns-78fcdf6894-mn6k8 1/1 Running 0 4m etcd-master 1/1 Running 0 3m kube-apiserver-master 1/1 Running 0 3m kube-controller-manager-master 1/1 Running 0 3m kube-proxy-5xj2l 1/1 Running 0 4m kube-proxy-bh7wb 1/1 Running 0 1m kube-proxy-jqpqg 1/1 Running 0 1m kube-scheduler-master 1/1 Running 0 3m $kubectl get node NAME STATUS ROLES AGE VERSION master Ready master 4m v1.10.0 node-1 Ready \u0026lt;none\u0026gt; 2m v1.10.0 node-2 Ready \u0026lt;none\u0026gt; 2m v1.10.0  \n安裝Kubeless 安裝CLI 安裝操作 Kubeless 的 CLI\n1 2 3 4  $wget https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.7/kubeless_linux-amd64.zip apt install -y unzip unzip kubeless_linux-amd64.zip $sudo mv ~/bundles/kubeless_linux-amd64 /usr/local/bin/  \n檢查CLI是否安裝完成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  Serverless framework for Kubernetes Usage: kubeless [command] Available Commands: autoscale manage autoscale to function on Kubeless completion Output shell completion code for the specified shell. function function specific operations get-server-config Print the current configuration of the controller help Help about any command topic manage message topics in Kubeless trigger trigger specific operations version Print the version of Kubeless Flags: -h, --help help for kubeless Use \u0026#34;kubeless [command] --help\u0026#34; for more information about a command.  \n安裝Kubeless Kubeless官方針對不同Kubernetes環境提供了多種腳本（非RBAC，RBAC和openshift），這邊我採用的是有RBAC的部署。\n1 2 3 4 5 6 7 8 9 10 11 12 13  $kubectl create ns kubeless namespace/kubeless created $kubectl create -f https://github.com/kubeless/kubeless/releases/download/v1.0.0-alpha.8/kubeless-v1.0.0-alpha.8.yaml configmap/kubeless-config created deployment.apps/kubeless-controller-manager created serviceaccount/controller-acct created clusterrole.rbac.authorization.k8s.io/kubeless-controller-deployer created clusterrolebinding.rbac.authorization.k8s.io/kubeless-controller-deployer created customresourcedefinition.apiextensions.k8s.io/functions.kubeless.io created customresourcedefinition.apiextensions.k8s.io/httptriggers.kubeless.io created customresourcedefinition.apiextensions.k8s.io/cronjobtriggers.kubeless.io created  \n可以透過kubetl指令觀察kubeless是否有被部署起來\n1 2 3 4 5 6 7 8 9 10 11 12  kubectl get pods -n kubeless NAME READY STATUS RESTARTS AGE kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 1m $kubectl get deployment -n kubeless NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kubeless-controller-manager 1 1 1 1 2m $kubectl get crd NAME CREATED AT cronjobtriggers.kubeless.io 2018-08-24T12:44:57Z functions.kubeless.io 2018-08-24T12:44:57Z httptriggers.kubeless.io 2018-08-24T12:44:57Z   ### 安裝Kubeless UI 官方提供了操作Kubeless的Dashboard，可以透過該介面操作。 ```bash $kubectl create -f https://raw.githubusercontent.com/kubeless/kubeless-ui/master/k8s.yaml 可以透過kubetl指令觀察Kubeless的Dashboard是否有被部署起來\n1 2 3 4 5 6 7  kubectl -n kubeless get pod,svc NAME READY STATUS RESTARTS AGE pod/kubeless-controller-manager-66868fb689-77fkp 3/3 Running 0 39m pod/ui-6d868664c5-kr99m 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ui NodePort 10.100.15.46 \u0026lt;none\u0026gt; 3000:30722/TCP 1m   可以透Browser瀏覽 Kubeless UI(172.24.0.2:30722)\n  Kubeless Dashboard\n  建立及測試Function 這邊示範一個簡單的質數判斷的Function，以Go為例。\n這邊會示範從UI上操作以及使用CLI操作\nUI操作Kubeless 透過Browser連進Kubeless Dashboard 後點選 Create Function\n  建立一個Function\n  按下Create後，就可以撰寫我們判斷質數的程式碼。\n程式的撰寫畫面會如下圖所示\n  在function內撰寫我們想要的功能\n  撰寫完，質數判斷的function大致上是這個樣子\n  質數判斷的function\n  測試Function 接著可以輸入右側的Request，填入你想要判斷的數值。如：177,9487\u0026hellip;等數值。\n填完數值後按下Run fuction，發現底下會沒有輸出！！！\n 因為kubeless UI有RBAC的問題，這邊偷懶直接給cluster-admin。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubeless-ui-default roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: User name: system:serviceaccount:kubeless:ui-acct apiGroup: rbac.authorization.k8s.io EOF     這邊還會產生另外一個問題，在Kubeless UI 所產生的Request只能是String型態的。\n在這個範例之中會看到以下輸出。\n   只能得到must be a positive integer的回應\n  要解決這個問題我們可以藉由Kubeless CLI發送請求給特定的function。\n1 2 3 4 5 6  $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數   結束實驗後我們可以透過Kubeless UI刪除掉剛剛建立的checkprime function\nCLI操作Kubeless 建立一個checkprime.go，並且撰寫質數判斷的function。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  package kubeless import ( \u0026#34;strconv\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/kubeless/kubeless/pkg/functions\u0026#34; ) func IsPrime(event functions.Event, context functions.Context) (string, error) { num, err := strconv.Atoi(event.Data) if err != nil { fmt.Println(err) return \u0026#34;must be a positive integer\u0026#34;, nil } if num \u0026lt;= 1 { return \u0026#34;must be a positive integer\u0026#34;, nil } for i := 2; i \u0026lt; num; i++ { if num%i == 0 { return \u0026#34;非質數\u0026#34;, nil break } } return \u0026#34;質數\u0026#34;, nil }   透過kubeless CLI幫我們建立Function到環境上\n1 2 3 4  $kubeless function deploy checkprime --runtime go1.10 --from-file checkprime.go --handler checkprime.IsPrime INFO[0000] Deploying function... INFO[0000] Function checkprime submitted for deployment INFO[0000] Check the deployment status executing \u0026#39;kubeless function ls checkprime\u0026#39;   透過Kubeless CLI 、 Kubectl 去確認Function 有沒有成功的被建立到環境上：\n1 2 3 4 5 6 7 8 9 10 11  $kubectl get functions NAME AGE checkprime 1m $kubeless function ls NAME NAMESPACE\tHANDLER RUNTIME\tDEPENDENCIES\tSTATUS checkprime\tdefault checkprime.IsPrime\tgo1.10 1/1 READY $kubectl get po NAME READY STATUS RESTARTS AGE checkprime-7d4f$b7b64c-k47w9 1/1 Running 0 37s   測試Function 透過Kubeless CLI去測試剛剛撰寫的質數判斷是否正確\n1 2 3 4 5 6  $kubeless function call checkprime --data 9487 非質數 $kubeless function call checkprime --data 177 非質數 $kubeless function call checkprime --data 7 質數   另外也能透過kubernetes把該function proxy 出來進行測試\n1 2 3 4 5 6 7 8 9  $kubectl proxy -p 34567 \u0026amp; [1] 3533 Starting to serve on 127.0.0.1:34567 $curl --data 9487\\  --header \u0026#34;Content-Type:application/json\u0026#34; \\  localhost:34567/api/v1/namespaces/default/services/checkprime:8080/proxy/ 非質數   結束實驗後我們可以透過Kubeless CLI刪除掉剛剛建立的checkprime function\n1 2 3 4 5  $kubeless function delete checkprime $kubeless function ls NAME\tNAMESPACE\tHANDLER\tRUNTIME\tDEPENDENCIES\tSTATUS   小結 還有很多開源的 FAAS 框架可以直接套用在 kubernetes 上，本篇文章只針對 kubeless 做一個非常簡單的 Demo ，有機會的話還會玩玩看 Knative , fission \u0026hellip;等框架。\n","description":"","id":9,"section":"posts","tags":["kubernetes","serverless"],"title":"FAAS之kubeless的新滋味","uri":"https://blog.jjmengze.website/posts/kubeless/kubeless/"},{"content":"  kolla是為了提供production-ready的 OpenStack Cloud 之container和deployment tools。\n環境配置 本次安裝是使用三台Bare Metal去作部署，作業系統採用的是Ubuntu 16.04 LTS版。\n   Kubernetes Role OpeStack Role RAM CPUs IP Address     Master Controller 16G 8Cores 10.0.0.190   Node1 Compute1 16G 8Cores 10.0.0.191   Node2 Compute2 16G 8Cores 10.0.0.192    部署kubernetes 這邊利用的是kubeadm進行kubernetes的安裝『版本是Kubernetes1.10』，可以參考官方網站的部屬方式。\n安裝 Kubernetes latest version and other dependencies:\n1 2 3 4 5 6 7 8 9  $curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo -E apt-key add - $cat \u0026lt;\u0026lt;EOF \u0026gt; kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF $sudo cp -aR kubernetes.list /etc/apt/sources.list.d/kubernetes.list $sudo apt-get update $sudo apt-get install -y docker.io kubelet kubeadm kubectl   Kubernetes v1.8+ 要求關閉系統 Swap，如果不想關閉系統的Swap需要修改 kubelet 設定參數，我們利用以下指令關閉系統Swap：\n1  $swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0   透過以下指令啟動Docker Daemon。\n1  $systemctl enable docker \u0026amp;\u0026amp; systemctl start docker   確認Docker是否支援Cgroup Driver或是Systemd Driver，進一步修改 kubelet 設定參數。\n1 2  $CGROUP_DRIVER=$(sudo docker info | grep \u0026#34;Cgroup Driver\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) $sudo sed -i \u0026#34;s|KUBELET_KUBECONFIG_ARGS=|KUBELET_KUBECONFIG_ARGS=--cgroup-driver=$CGROUP_DRIVER|g\u0026#34; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf   將橋接的IPv4流量傳遞給iptables\n1 2 3 4 5 6  $cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $sysctl -p /etc/sysctl.d/k8s.conf   設置Kubernetes Server CIDR的DNS位置。\n1  $sudo sed -i \u0026#39;s/10.96.0.10/10.3.3.10/g\u0026#39; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf   利用以下重新載入kubelet相關的啟動參數\n1 2 3 4  $sudo systemctl daemon-reload $sudo systemctl stop kubelet $sudo systemctl enable kubelet $sudo systemctl start kubelet   在master節點上使用kubeadm進行kubernetes 叢集的初始化\n1  $sudo kubeadm init --feature-gates CoreDNS=true --pod-network-cidr=10.1.0.0/16 --service-cidr=10.3.3.0/24   會得到下列輸出，我們利用下列輸出的資訊將其他節點加入叢集。\n1 2 3 4  You can now join any number of machines by running the following on each node as root: kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481   在其他節點上我們可以利用以下指令加入叢集。\n1  $kubeadm join 10.0.0.181:6443 --token pieol0.2kfzpwhosxuqhe6t --discovery-token-ca-cert-hash sha256:e55b423135642404ffc60bcae4793732f18b4ce2866a8419c87b7dd92724a481   在master節點設定kube config。\n1 2 3  $mkdir -p $HOME/.kube $sudo -H cp /etc/kubernetes/admin.conf $HOME/.kube/config $sudo -H chown $(id -u):$(id -g) $HOME/.kube/config   在master 安裝Kubernetes CNI，這邊採用的是Canal。\n1 2 3 4 5  $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/rbac.yaml $kubectl apply -f rbac.yaml $wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml $sed -i \u0026#34;s@10.244.0.0/16@10.1.0.0/16@\u0026#34; canal.yaml $kubectl apply -f canal.yaml   因為我們要把kubernetes master當作openstack controller的角色，我們將kubernetes master 的節點污染拿掉。\n1  $kubectl taint nodes --all=true node-role.kubernetes.io/master:NoSchedule-   在CNI安裝完成後可透過下列指令來檢查，Node \u0026amp; Pod是否都準備好了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  $kubectl get pod,node -o wide --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/canal-297sw 3/3 Running 0 2m kube-system pod/canal-f82qj 3/3 Running 0 2m kube-system pod/canal-zxfbk 3/3 Running 0 2m kube-system pod/coredns-7997f8864c-cglcr 1/1 Running 0 9m kube-system pod/coredns-7997f8864c-sxf2c 1/1 Running 0 9m kube-system pod/etcd-node1 1/1 Running 0 8m kube-system pod/kube-apiserver-node1 1/1 Running 0 8m kube-system pod/kube-controller-manager-node1 1/1 Running 0 8m kube-system pod/kube-proxy-6gwws 1/1 Running 0 6m kube-system pod/kube-proxy-9xbdq 1/1 Running 0 6m kube-system pod/kube-proxy-z54k4 1/1 Running 0 9m kube-system pod/kube-scheduler-node1 1/1 Running 0 8m NAME STATUS ROLES AGE VERSION node1 Ready master 8m v1.10.3 node2 Ready master 8m v1.10.3 node3 Ready master 8m v1.10.3   利用BusyBox ，驗證Kubernetes 環境例如DNS 是否有通。\n1 2 3 4 5 6 7 8 9  $kubectl run -i -t $(uuidgen) --image=busybox --restart=Never $nslookup kubernetes Server: 10.3.3.10 Address 1: 10.3.3.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.3.3.1 kubernetes.default.svc.cluster.local   為kubernetes Helm 建立Tiller Service Account以及綁定Cluster-Admin Role\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $kubectl create serviceaccount tiller --namespace kube-system $cat \u0026lt;\u0026lt;EOF | kubectl create -f - kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026#34;\u0026#34; EOF   接著我們安裝Kubernetes Helm，用來管理Helm package的元件。\n1 2 3 4  $curl -L https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh $chmod 700 get_helm.sh $./get_helm.sh $helm init --service-account tiller   由於kolla kubernetes 需要用到ansible git ，我們在master的節點需要安裝ansible ，其他節點需安裝python。\n1 2 3  $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y software-properties-common git python python-pip $sudo apt-add-repository -y ppa:ansible/ansible $sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y ansible   建立一個資料夾方便我們接下來的步驟\n1 2  $mkdir kolla-bringup $cd kolla-bringup   下載 kolla-kubernetes \u0026amp; kolla ansible\n1 2 3 4 5 6 7  $git clone http://github.com/openstack/kolla-ansible $git clone http://github.com/openstack/kolla-kubernetes $cd kolla-kubernetes $git checkout 22ed0c232d7666afb6e288001b8814deea664992 $cd ../kolla-ansible $git checkout origin/stable/pike $cd ..   使用pip 安裝kolla-kubernetes \u0026amp; kolla-ansible所需要的套件\n1  $sudo pip install -U kolla-ansible/ kolla-kubernetes   把相關設定檔製到/etc底下\n1 2  $cp -Ra kolla-kubernetes/etc/kolla/ /etc $cp -Ra kolla-kubernetes/etc/kolla-kubernetes/ /etc   pip剛剛安裝的項目這時候可以幫我們生成default passwords\n1  $sudo kolla-kubernetes-genpwd   建立一個Kubernetes namespaces來隔離Kolla deployment\n1  $kubectl create namespace kolla   使用Label標記要成為Controller及Compute的節點，我們這邊將node1當成Controller，node2 node3當成Compute\n1 2 3  $kubectl label node node1 kolla_controller=true $kubectl label node node2 kolla_compute=true $kubectl label node node3 kolla_compute=true   將/etc/kolla/globals.yml設定檔，修改成與自己環境相符\n將/etc/kolla/globals.yml中的network_interface設置為Management interface name。例如：eth1\n將/etc/kolla/globals.yml中的neutron_external_interface設置為neutron external interface name。例如：eth2\n將相關的openstack設定加入/etc/kolla/globals.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  $cat \u0026lt;\u0026lt;EOF \u0026gt; add-to-globals.yml kolla_install_type: \u0026#34;source\u0026#34; tempest_image_alt_id: \u0026#34;{{ tempest_image_id }}\u0026#34; tempest_flavor_ref_alt_id: \u0026#34;{{ tempest_flavor_ref_id }}\u0026#34; neutron_plugin_agent: \u0026#34;openvswitch\u0026#34; api_interface_address: 0.0.0.0 tunnel_interface_address: 0.0.0.0 orchestration_engine: KUBERNETES memcached_servers: \u0026#34;memcached\u0026#34; keystone_admin_url: \u0026#34;http://keystone-admin:35357/v3\u0026#34; keystone_internal_url: \u0026#34;http://keystone-internal:5000/v3\u0026#34; keystone_public_url: \u0026#34;http://keystone-public:5000/v3\u0026#34; glance_registry_host: \u0026#34;glance-registry\u0026#34; neutron_host: \u0026#34;neutron\u0026#34; keystone_database_address: \u0026#34;mariadb\u0026#34; glance_database_address: \u0026#34;mariadb\u0026#34; nova_database_address: \u0026#34;mariadb\u0026#34; nova_api_database_address: \u0026#34;mariadb\u0026#34; neutron_database_address: \u0026#34;mariadb\u0026#34; cinder_database_address: \u0026#34;mariadb\u0026#34; ironic_database_address: \u0026#34;mariadb\u0026#34; placement_database_address: \u0026#34;mariadb\u0026#34; rabbitmq_servers: \u0026#34;rabbitmq\u0026#34; openstack_logging_debug: \u0026#34;True\u0026#34; enable_heat: \u0026#34;no\u0026#34; enable_cinder: \u0026#34;yes\u0026#34; enable_cinder_backend_lvm: \u0026#34;yes\u0026#34; enable_cinder_backend_iscsi: \u0026#34;yes\u0026#34; enable_cinder_backend_rbd: \u0026#34;no\u0026#34; enable_ceph: \u0026#34;no\u0026#34; enable_elasticsearch: \u0026#34;no\u0026#34; enable_kibana: \u0026#34;no\u0026#34; glance_backend_ceph: \u0026#34;no\u0026#34; cinder_backend_ceph: \u0026#34;no\u0026#34; nova_backend_ceph: \u0026#34;no\u0026#34; EOF $cat ./add-to-globals.yml | sudo tee -a /etc/kolla/globals.yml   接下來透過ansible幫我們產生OpneStack設定檔\n1  $ansible-playbook -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla kolla-kubernetes/ansible/site.yml   使用官方提供的腳本幫助我們快速的把OpenStack的password，加到Kubernetes secrets\n1  $kolla-kubernetes/tools/secret-generator.py create   使用之前在pip安裝的檔案，快速的把OpenStack的設定檔，加入Kubernetes config maps裡\n1 2 3 4 5 6 7 8 9 10  $kollakube res create configmap \\  mariadb keystone horizon rabbitmq memcached nova-api nova-conductor \\  nova-scheduler glance-api-haproxy glance-registry-haproxy glance-api \\  glance-registry neutron-server neutron-dhcp-agent neutron-l3-agent \\  neutron-metadata-agent neutron-openvswitch-agent openvswitch-db-server \\  openvswitch-vswitchd nova-libvirt nova-compute nova-consoleauth \\  nova-novncproxy nova-novncproxy-haproxy neutron-server-haproxy \\  nova-api-haproxy cinder-api cinder-api-haproxy cinder-backup \\  cinder-scheduler cinder-volume iscsid tgtd keepalived \\  placement-api placement-api-haproxy   透過官方提供的腳本建立Kolla Helm Chart\n1  $kolla-kubernetes/tools/helm_build_all.sh .   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  $cat \u0026lt;\u0026lt;EOF \u0026gt; cloud.yaml global: kolla: all: docker_registry: docker.io image_tag: \u0026#34;4.0.0\u0026#34; kube_logger: false external_vip: \u0026#34;192.168.7.105\u0026#34; base_distro: \u0026#34;centos\u0026#34; install_type: \u0026#34;source\u0026#34; tunnel_interface: \u0026#34;docker0\u0026#34; keystone: all: admin_port_external: \u0026#34;true\u0026#34; dns_name: \u0026#34;192.168.7.105\u0026#34; public: all: port_external: \u0026#34;true\u0026#34; rabbitmq: all: cookie: 67 glance: api: all: port_external: \u0026#34;true\u0026#34; cinder: api: all: port_external: \u0026#34;true\u0026#34; volume_lvm: all: element_name: cinder-volume daemonset: lvm_backends: - \u0026#39;192.168.7.105\u0026#39;: \u0026#39;cinder-volumes\u0026#39; ironic: conductor: daemonset: selector_key: \u0026#34;kolla_conductor\u0026#34; nova: placement_api: all: port_external: true novncproxy: all: port: 6080 port_external: true openvswitch: all: add_port: true ext_bridge_name: br-ex ext_interface_name: enp1s0f1 setup_bridge: true horizon: all: port_external: true EOF   將該數值修改成環境上Management interface的IP。例如10.0.0.178\n1  $sed -i \u0026#34;s@192.168.7.105@10.0.0.178@g\u0026#34; ./cloud.yaml   將該數值修改成環境上ext_interface_name的interface。例如：eth1\n1  $sed -i \u0026#34;s@enp1s0f1@eth1@g\u0026#34; ./cloud.yaml   將該數值修改成環境上Management interface。例如：eth2\n1  $sed -i \u0026#34;s@docker0@eth2@g\u0026#34; ./cloud.yaml   在這邊建立一個給Openstack rbac ，讓後來helm啟動的元件去取得Kubernetes資源不會有權限問題（OpenStack官方RBAC這一點還沒修正\u0026hellip;）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  $cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kolla EOF   一個一個把openstack的服務使用helm啟動起來。（例如mariadb,rabbitmq,memcached\u0026hellip;等)\n1 2 3 4 5 6 7 8 9 10 11  $helm install --debug kolla-kubernetes/helm/service/mariadb --namespace kolla --name mariadb --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/rabbitmq --namespace kolla --name rabbitmq --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/memcached --namespace kolla --name memcached --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/keystone --namespace kolla --name keystone --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/glance --namespace kolla --name glance --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/cinder-control --namespace kolla --name cinder-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/horizon --namespace kolla --name horizon --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/openvswitch --namespace kolla --name openvswitch --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/neutron --namespace kolla --name neutron --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-control --namespace kolla --name nova-control --values ./cloud.yaml $helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml   這邊有可能遇到第一個問題，也就是nova-compute init 會卡在這個畫面。\n1  nova-api-create-cell-r288q 0/1 Init:2/3 0 5min   這邊發生了一些問題，官方沒有去修正他。先把這個helm chart 刪除掉，我在這邊修正了helm的設定檔案，去修改keystone的url。\n1 2 3 4  $helm delete --purge nova-compute $vim kolla-bringup/kolla-kubernetes/helm/microservice/nova-api-create-simple-cell-job/templates/nova-api-create-cell.yaml {{- $keystonePort := include \u0026#34;kolla_val_get_str\u0026#34; (dict \u0026#34;key\u0026#34; \u0026#34;port\u0026#34; \u0026#34;searchPath\u0026#34; $keystoneSearchPath \u0026#34;Values\u0026#34; .Values )| default \u0026#34;5000\u0026#34; }}   再重新build一次helm chart\n1  $kolla-kubernetes/tools/helm_build_all.sh .   再重新run一次修正過後的chart.\n1  helm install --debug kolla-kubernetes/helm/service/nova-compute --namespace kolla --name nova-compute --values ./cloud.yaml   確認所有服務運作正常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78  $ kubectl get pod -n kolla NAME READY STATUS RESTARTS AGE cinder-api-5d6fd874b5-tzwlr 3/3 Running 0 11h cinder-create-db-x552q 0/2 Completed 0 11h cinder-create-keystone-endpoint-admin-7xhjp 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv2-252bg 0/1 Completed 0 11h cinder-create-keystone-endpoint-adminv3-4zcv7 0/1 Completed 0 11h cinder-create-keystone-endpoint-internal-s2n77 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv2-9wr7f 0/1 Completed 0 11h cinder-create-keystone-endpoint-internalv3-2srh2 0/1 Completed 0 11h cinder-create-keystone-endpoint-public-7mksf 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv2-pqhms 0/1 Completed 0 11h cinder-create-keystone-endpoint-publicv3-8z6xg 0/1 Completed 0 11h cinder-create-keystone-service-4sbp6 0/1 Completed 0 11h cinder-create-keystone-servicev2-9h88v 0/1 Completed 0 11h cinder-create-keystone-servicev3-p89wk 0/1 Completed 0 11h cinder-create-keystone-user-4whfz 0/1 Completed 0 11h cinder-manage-db-hgppr 0/1 Completed 0 11h cinder-scheduler-0 1/1 Running 0 11h glance-api-6f649fbf8d-9hwzn 1/1 Running 0 11h glance-create-db-76lwc 0/2 Completed 0 11h glance-create-keystone-endpoint-admin-m4mxm 0/1 Completed 0 11h glance-create-keystone-endpoint-internal-q9whd 0/1 Completed 0 11h glance-create-keystone-endpoint-public-stszm 0/1 Completed 0 11h glance-create-keystone-service-hcznf 0/1 Completed 0 11h glance-create-keystone-user-9f2g7 0/1 Completed 0 11h glance-manage-db-ch6rp 0/1 Completed 0 11h glance-registry-684d9cc765-d5g5p 3/3 Running 0 11h horizon-7bc45d8df6-8ndt6 1/1 Running 0 11h keystone-b55d658-4bmpf 1/1 Running 0 12h keystone-create-db-rb9wq 0/2 Completed 0 12h keystone-create-endpoints-65m86 0/1 Completed 0 12h keystone-fernet-setup-job-knfm8 0/1 Completed 0 12h keystone-manage-db-x4m2n 0/1 Completed 0 12h mariadb-0 1/1 Running 0 12h mariadb-init-element-ndbxt 0/1 Completed 0 12h memcached-7b95fd6b69-v8f4v 2/2 Running 0 12h neutron-create-db-w2hqk 0/2 Completed 0 11h neutron-create-keystone-endpoint-admin-hkg8p 0/1 Completed 0 11h neutron-create-keystone-endpoint-internal-cwzrt 0/1 Completed 0 11h neutron-create-keystone-endpoint-public-bjzzk 0/1 Completed 0 11h neutron-create-keystone-service-q7ms9 0/1 Completed 0 11h neutron-create-keystone-user-zvqnw 0/1 Completed 0 11h neutron-dhcp-agent-l5qkg 1/1 Running 0 11h neutron-l3-agent-network-64v9x 1/1 Running 0 11h neutron-manage-db-5dqkn 0/1 Completed 0 11h neutron-metadata-agent-network-ttf5v 1/1 Running 0 11h neutron-openvswitch-agent-network-j6llm 1/1 Running 0 11h neutron-server-6d74c78c98-xzdd8 3/3 Running 0 11h nova-api-7d5cf595bc-rxg4k 3/3 Running 0 11h nova-api-create-cell-r288q 0/1 Completed 0 11h nova-api-create-db-5w2lg 0/2 Completed 0 11h nova-api-manage-db-wd4b8 0/1 Completed 0 11h nova-cell0-create-db-5bz6v 0/2 Completed 0 11h nova-compute-wn5lb 1/1 Running 0 11h nova-compute-xkv8r 1/1 Running 0 11h nova-conductor-0 1/1 Running 0 11h nova-consoleauth-0 1/1 Running 0 11h nova-create-db-476gl 0/2 Completed 0 11h nova-create-keystone-endpoint-admin-xbt8x 0/1 Completed 0 11h nova-create-keystone-endpoint-internal-58dvx 0/1 Completed 0 11h nova-create-keystone-endpoint-public-8c56c 0/1 Completed 0 11h nova-create-keystone-service-jngxg 0/1 Completed 0 11h nova-create-keystone-user-4gc62 0/1 Completed 0 11h nova-libvirt-kbcrl 1/1 Running 0 11h nova-libvirt-n2nnj 1/1 Running 0 11h nova-novncproxy-79bf74796f-9p7ct 3/3 Running 0 11h nova-scheduler-0 1/1 Running 0 11h openvswitch-ovsdb-network-rwwrz 1/1 Running 0 11h openvswitch-vswitchd-network-6q9w9 1/1 Running 0 11h placement-api-create-keystone-endpoint-admin-bg9ct 0/1 Completed 0 11h placement-api-create-keystone-endpoint-internal-v998h 0/1 Completed 0 11h placement-api-create-keystone-endpoint-public-p6mvf 0/1 Completed 0 11h placement-api-fc8f68544-rvhwc 1/1 Running 0 11h placement-create-keystone-service-blj57 0/1 Completed 0 11h placement-create-keystone-user-tw5k4 0/1 Completed 0 11h rabbitmq-0 1/1 Running 0 12h rabbitmq-init-element-gtvgw 0/1 Completed 0 12h   使用官方的tool建立admin的openrc file，建立完成的檔案會存在當前使用者的home目錄下。\n1  $kolla-kubernetes/tools/build_local_admin_keystonerc.sh ext   接者安裝OpenStack clients套件\n1 2 3  $sudo pip install \u0026#34;python-openstackclient\u0026#34; $sudo pip install \u0026#34;python-neutronclient\u0026#34; $sudo pip install \u0026#34;python-cinderclient\u0026#34;   使用openstack語法去產生image,network\u0026hellip;等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $source ~/keystonerc_admin $IMAGE_URL=http://download.cirros-cloud.net/0.3.5/ $IMAGE=cirros-0.3.5-x86_64-disk.img $IMAGE_NAME=cirros $IMAGE_TYPE=linux EXT_NET_CIDR=\u0026#39;172.24.10.1/24\u0026#39; EXT_NET_RANGE=\u0026#39;start=172.24.10.10,end=172.24.10.200\u0026#39; EXT_NET_GATEWAY=\u0026#39;172.24.10.1\u0026#39; $curl -L -o ./${IMAGE} ${IMAGE_URL}/${IMAGE} $openstack image create --disk-format qcow2 --container-format bare --public \\  --property os_type=${IMAGE_TYPE} --file ./${IMAGE} ${IMAGE_NAME} $openstack network create --external --provider-physical-network physnet1 \\  --provider-network-type flat public1 $openstack subnet create --no-dhcp \\  --allocation-pool ${EXT_NET_RANGE} --network public1 \\  --subnet-range ${EXT_NET_CIDR} --gateway ${EXT_NET_GATEWAY} public1-subnet openstack flavor create --id 1 --ram 512 --disk 1 --vcpus 1 m1.tiny openstack flavor create --id 2 --ram 2048 --disk 20 --vcpus 1 m1.small openstack flavor create --id 3 --ram 4096 --disk 40 --vcpus 2 m1.medium openstack flavor create --id 4 --ram 8192 --disk 80 --vcpus 4 m1.large openstack flavor create --id 5 --ram 16384 --disk 160 --vcpus 8 m1.xlarge   可以使用openstack horizon操作openstack dashboard，使用admin user的帳號、密碼進行登入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  $kubectl get service -n kolla NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cinder-api ClusterIP 10.3.3.125 10.0.0.182 8776/TCP 12h glance-api ClusterIP 10.3.3.141 10.0.0.182 9292/TCP 12h glance-registry ClusterIP 10.3.3.15 \u0026lt;none\u0026gt; 9191/TCP 12h horizon ClusterIP 10.3.3.11 10.0.0.182 80/TCP 12h keystone-admin ClusterIP 10.3.3.35 10.0.0.182 35357/TCP 12h keystone-internal ClusterIP 10.3.3.228 \u0026lt;none\u0026gt; 5000/TCP 12h keystone-public ClusterIP 10.3.3.124 10.0.0.182 5000/TCP 12h mariadb ClusterIP 10.3.3.98 \u0026lt;none\u0026gt; 3306/TCP 12h memcached ClusterIP 10.3.3.140 \u0026lt;none\u0026gt; 11211/TCP 12h neutron-server ClusterIP 10.3.3.21 10.0.0.182 9696/TCP 12h nova-api ClusterIP 10.3.3.150 10.0.0.182 8774/TCP 12h nova-metadata ClusterIP 10.3.3.217 \u0026lt;none\u0026gt; 8775/TCP 12h nova-novncproxy ClusterIP 10.3.3.4 10.0.0.182 6080/TCP 12h nova-placement-api ClusterIP 10.3.3.159 10.0.0.182 8780/TCP 12h rabbitmq ClusterIP 10.3.3.66 \u0026lt;none\u0026gt; 5672/TCP 12h rabbitmq-mgmt ClusterIP 10.3.3.37 \u0026lt;none\u0026gt; 15672/TCP 12h $cat keystonerc_admin unset OS_SERVICE_TOKEN export OS_USERNAME=admin export OS_PASSWORD=kQHEss3THBmHCQWdqNd2b51U8xRB3hKPH6KD4kx3 export OS_AUTH_URL=http://10.0.0.182:5000/v3 export PS1=\u0026#39;[\\u@\\h \\W(keystone_admin)]$ \u0026#39; export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 export OS_REGION_NAME=RegionOne export OS_VOLUME_API_VERSION=2    登入即可看到openstack的操作畫面。\n  -- 如果需要拆除你的OpenStack Kolla Kubernetes環境 在你的master上，使用helm拆除OpenStack相關元件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  $helm install --debug ~/kolla-bringup/kolla-kubernetes/helm/service/nova-cleanup --namespace kolla --name nova-cleanup --values ~/kolla-bringup/cloud.yaml $helm delete mariadb --purge $helm delete mariadb --purge $helm delete rabbitmq --purge $helm delete memcached --purge $helm delete keystone --purge $helm delete glance --purge $helm delete cinder-control --purge $helm delete horizon --purge $helm delete openvswitch --purge $helm delete neutron --purge $helm delete nova-control --purge $helm delete nova-compute --purge $helm delete nova-cell0-create-db-job --purge $helm delete cinder-volume-lvm --purge   在每個節點上拆除相關的OpenStack volume\n1  $sudo rm -rf /var/lib/kolla/volumes/*   在每個節點上刪除Kubernetes\n1 2 3 4  $sudo kubeadm reset $sudo rm -rf /etc/kolla $sudo rm -rf /etc/kubernetes $sudo rm -rf /etc/kolla-kubernetes   ","description":"","id":10,"section":"posts","tags":["openStack","kubernetes"],"title":"Kolla Kubernetes","uri":"https://blog.jjmengze.website/posts/openstack/kolla-kubernetes/"},{"content":"  TODO","description":"","id":11,"section":"posts","tags":["GRPC","proto"],"title":"GRPC 與 Proto buffer 附身合體","uri":"https://blog.jjmengze.website/posts/protocol/grpc-server-client/"},{"content":"  什麼是Protocol Buffer Protocol Buffer 是由是 Google 所推出的一種輕量且高效的結構化資料存儲格式。\n將結構化的資料進行序列化，實現資料的傳輸以及資料的儲存。\nProtocol Buffer 比 XML、Json 更小、更快、使用及維護上更加的簡單（可以將 api speic 直接轉換成魏應的程式語言）！\n優點  體積小 跨平台 跨語言 傳輸速度快 維護成本低 序列化速度快  定義資料結構 結構的定義很簡單，檔案以 .proto 作為後輟。\nCoding style可以參考這裡：https://developers.google.com/protocol-buffers/docs/style\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  syntax = \u0026#34;proto3\u0026#34;;package tutorial;message Employee { string name = 1; int32 id = 2; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { string number = 1; PhoneType type = 2; } repeated PhoneNumber phones = 3;}  第一行指定您正在使用proto3語法\n接下來package定義在golang裡所屬於的package\nmessage定義傳輸的訊息\n訊息內容有name、id、枚舉以及一組內部phone number的message\n編譯所撰寫好的.proto 安裝編譯時所需要用的套件，也可以參考官方的方法決定安裝的方式。\n1  go get -u github.com/golang/protobuf/protoc-gen-go   \n\n  \n\n我們使用剛剛安裝的套件進行編譯成 go library 或是可以編成 java python \u0026hellip;. 等的 library 以達到跨平台跨語言的支持，生成的 library 直接幫我們寫好壓縮以及解壓縮的方法開發者只要專心撰寫自己的業務邏輯即可，本篇文章以 go 作為範例。\n1  protoc --go_out=. *.proto   編譯完成後的檔案會是樣的命名方式：employee.pb.go\n檔案內容會如同下面的範例所示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170  // Code generated by protoc-gen-go. DO NOT EDIT. // source: employee.proto  package tutorial import proto \u0026#34;github.com/golang/protobuf/proto\u0026#34; import fmt \u0026#34;fmt\u0026#34; import math \u0026#34;math\u0026#34; // Reference imports to suppress errors if they are not otherwise used. var _ = proto.Marshal var _ = fmt.Errorf var _ = math.Inf // This is a compile-time assertion to ensure that this generated file // is compatible with the proto package it is being compiled against. // A compilation error at this line likely means your copy of the // proto package needs to be updated. const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package  type Employee_PhoneType int32 const ( Employee_MOBILE Employee_PhoneType = 0 Employee_HOME Employee_PhoneType = 1 Employee_WORK Employee_PhoneType = 2 ) var Employee_PhoneType_name = map[int32]string{ 0: \u0026#34;MOBILE\u0026#34;, 1: \u0026#34;HOME\u0026#34;, 2: \u0026#34;WORK\u0026#34;, } var Employee_PhoneType_value = map[string]int32{ \u0026#34;MOBILE\u0026#34;: 0, \u0026#34;HOME\u0026#34;: 1, \u0026#34;WORK\u0026#34;: 2, } func (x Employee_PhoneType) String() string { return proto.EnumName(Employee_PhoneType_name, int32(x)) } func (Employee_PhoneType) EnumDescriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } type Employee struct { Name string `protobuf:\u0026#34;bytes,1,opt,name=name\u0026#34; json:\u0026#34;name,omitempty\u0026#34;` Id int32 `protobuf:\u0026#34;varint,2,opt,name=id\u0026#34; json:\u0026#34;id,omitempty\u0026#34;` Phones []*Employee_PhoneNumber `protobuf:\u0026#34;bytes,3,rep,name=phones\u0026#34; json:\u0026#34;phones,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee) Reset() { *m = Employee{} } func (m *Employee) String() string { return proto.CompactTextString(m) } func (*Employee) ProtoMessage() {} func (*Employee) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0} } func (m *Employee) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee.Unmarshal(m, b) } func (m *Employee) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee.Marshal(b, m, deterministic) } func (dst *Employee) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee.Merge(dst, src) } func (m *Employee) XXX_Size() int { return xxx_messageInfo_Employee.Size(m) } func (m *Employee) XXX_DiscardUnknown() { xxx_messageInfo_Employee.DiscardUnknown(m) } var xxx_messageInfo_Employee proto.InternalMessageInfo func (m *Employee) GetName() string { if m != nil { return m.Name } return \u0026#34;\u0026#34; } func (m *Employee) GetId() int32 { if m != nil { return m.Id } return 0 } func (m *Employee) GetPhones() []*Employee_PhoneNumber { if m != nil { return m.Phones } return nil } type Employee_PhoneNumber struct { Number string `protobuf:\u0026#34;bytes,1,opt,name=number\u0026#34; json:\u0026#34;number,omitempty\u0026#34;` Type Employee_PhoneType `protobuf:\u0026#34;varint,2,opt,name=type,enum=tutorial.Employee_PhoneType\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` XXX_NoUnkeyedLiteral struct{} `json:\u0026#34;-\u0026#34;` XXX_unrecognized []byte `json:\u0026#34;-\u0026#34;` XXX_sizecache int32 `json:\u0026#34;-\u0026#34;` } func (m *Employee_PhoneNumber) Reset() { *m = Employee_PhoneNumber{} } func (m *Employee_PhoneNumber) String() string { return proto.CompactTextString(m) } func (*Employee_PhoneNumber) ProtoMessage() {} func (*Employee_PhoneNumber) Descriptor() ([]byte, []int) { return fileDescriptor_employee_7c804fd7a46a4aa3, []int{0, 0} } func (m *Employee_PhoneNumber) XXX_Unmarshal(b []byte) error { return xxx_messageInfo_Employee_PhoneNumber.Unmarshal(m, b) } func (m *Employee_PhoneNumber) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) { return xxx_messageInfo_Employee_PhoneNumber.Marshal(b, m, deterministic) } func (dst *Employee_PhoneNumber) XXX_Merge(src proto.Message) { xxx_messageInfo_Employee_PhoneNumber.Merge(dst, src) } func (m *Employee_PhoneNumber) XXX_Size() int { return xxx_messageInfo_Employee_PhoneNumber.Size(m) } func (m *Employee_PhoneNumber) XXX_DiscardUnknown() { xxx_messageInfo_Employee_PhoneNumber.DiscardUnknown(m) } var xxx_messageInfo_Employee_PhoneNumber proto.InternalMessageInfo func (m *Employee_PhoneNumber) GetNumber() string { if m != nil { return m.Number } return \u0026#34;\u0026#34; } func (m *Employee_PhoneNumber) GetType() Employee_PhoneType { if m != nil { return m.Type } return Employee_MOBILE } func init() { proto.RegisterType((*Employee)(nil), \u0026#34;tutorial.Employee\u0026#34;) proto.RegisterType((*Employee_PhoneNumber)(nil), \u0026#34;tutorial.Employee.PhoneNumber\u0026#34;) proto.RegisterEnum(\u0026#34;tutorial.Employee_PhoneType\u0026#34;, Employee_PhoneType_name, Employee_PhoneType_value) } func init() { proto.RegisterFile(\u0026#34;employee.proto\u0026#34;, fileDescriptor_employee_7c804fd7a46a4aa3) } var fileDescriptor_employee_7c804fd7a46a4aa3 = []byte{ // 208 bytes of a gzipped FileDescriptorProto \t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0xe2, 0x4b, 0xcd, 0x2d, 0xc8, 0xc9, 0xaf, 0x4c, 0x4d, 0xd5, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x28, 0x29, 0x2d, 0xc9, 0x2f, 0xca, 0x4c, 0xcc, 0x51, 0x7a, 0xc3, 0xc8, 0xc5, 0xe1, 0x0a, 0x95, 0x14, 0x12, 0xe2, 0x62, 0xc9, 0x4b, 0xcc, 0x4d, 0x95, 0x60, 0x54, 0x60, 0xd4, 0xe0, 0x0c, 0x02, 0xb3, 0x85, 0xf8, 0xb8, 0x98, 0x32, 0x53, 0x24, 0x98, 0x14, 0x18, 0x35, 0x58, 0x83, 0x98, 0x32, 0x53, 0x84, 0xcc, 0xb8, 0xd8, 0x0a, 0x32, 0xf2, 0xf3, 0x52, 0x8b, 0x25, 0x98, 0x15, 0x98, 0x35, 0xb8, 0x8d, 0xe4, 0xf4, 0x60, 0x66, 0xe9, 0xc1, 0xcc, 0xd1, 0x0b, 0x00, 0x29, 0xf0, 0x2b, 0xcd, 0x4d, 0x4a, 0x2d, 0x0a, 0x82, 0xaa, 0x96, 0x0a, 0xe7, 0xe2, 0x46, 0x12, 0x16, 0x12, 0xe3, 0x62, 0xcb, 0x03, 0xb3, 0xa0, 0x96, 0x41, 0x79, 0x42, 0x06, 0x5c, 0x2c, 0x25, 0x95, 0x05, 0xa9, 0x60, 0x0b, 0xf9, 0x8c, 0x64, 0x70, 0x19, 0x1e, 0x52, 0x59, 0x90, 0x1a, 0x04, 0x56, 0xa9, 0xa4, 0xcd, 0xc5, 0x09, 0x17, 0x12, 0xe2, 0xe2, 0x62, 0xf3, 0xf5, 0x77, 0xf2, 0xf4, 0x71, 0x15, 0x60, 0x10, 0xe2, 0xe0, 0x62, 0xf1, 0xf0, 0xf7, 0x75, 0x15, 0x60, 0x04, 0xb1, 0xc2, 0xfd, 0x83, 0xbc, 0x05, 0x98, 0x92, 0xd8, 0xc0, 0xfe, 0x37, 0x06, 0x04, 0x00, 0x00, 0xff, 0xff, 0x5c, 0x1e, 0x53, 0x56, 0x11, 0x01, 0x00, 0x00, }   小結 下一章節會使用本篇基於 protobuffer 建置出來的 go library 去撰寫一個簡單的 client server，並且透過 GRPC 去傳輸 protobuffer。\n","description":"","id":12,"section":"posts","tags":["GRPC","proto"],"title":"初嚐Protocol Buffer好滋味","uri":"https://blog.jjmengze.website/posts/protocol/grpc/"},{"content":"  記錄一下，Open vSwitch (OVS)常用的指令。\n在ubuntu上安裝ovs\nsudo apt install openvswitch-switch -y 新增一個bridge sudo ovs-vsctl add-br br0 顯示所有bridge sudo ovs-vsctl list-br 給某個bridge新增一個port sudo ovs-vsctl add-port br0 veth1 顯示綁定到該port的bridge sudo ovs-vsctl port-to-br veth1 連接controller到ovs br0上 sudo ovs-vsctl set-controller br0 tcp:\u0026lt;controller IP\u0026gt; 顯示所有的bridge、連接的port以及controller sudo ovs-vsctl show ","description":"","id":13,"section":"posts","tags":["SDN"],"title":"OVS 常用的指令","uri":"https://blog.jjmengze.website/posts/sdn/ovs-commonly-command/"},{"content":"  kubernetes Controller Manager 的那些元件 在 Kubernetes Master Node 另外一個相當重要的元件可視為整體 Kubernetes 系統架構的監控狀態中心，也是就是 Controller Manager 元件，主要由以下幾種控制器控制 Kubernetes Container 叢集的狀態：\n  Replication Controller ：監控叢集的副本狀態並盡可能修正副本狀態。\n  Node Controller ： 監控叢集的 Node 狀態並負責 Node 狀態的更新。\n  CronJob Controller ：監控叢集週期性任務的狀態並負責任務狀態的更新。\n  DaemonSet Controller ：監控叢集 Daemon 資源型態的狀態並負責其狀態的更新。\n  Deployment Controller ：監控叢集 Deployment 資源型態的狀態並負責與 Replication Controller 進行交互與狀態的更新。\n  Endpoint Controller ：監控叢集 Pod 的 IP 狀態並負責與 Service Controller 進行資料的交換。\n  Garbage Collector Controller ：收集已被叢集刪除的元件確認是否還有依賴的資源未被刪除。\n  Namespace Controller ：維護並且監控叢集 Namespace 資源型態建立與刪除的狀態。\n  Job Controller ：監控叢集單次任務的狀態並負責任務狀態的更新。\n  Pod AutoScaler Controller ：監控叢集 Pod 水平擴展以及負責該擴展的狀態。\n  RelicaSet Controller ：監控叢集副本升級狀態以及維護該升級資訊。\n  Service Controller ：監控叢集服務暴露資訊之狀態並與 Endpoint Controller 進行資料的交互更新。\n  ServiceAccount Controller ：負責監控叢集服務應用的帳戶認證與權限控管。\n  StatefulSet Controller ：監控叢集現有狀態服務的變化負責其資源的建立與刪除。\n  Volume Controller ：監控叢集掛載 volume 資訊與狀態並負責即時更新。\n  Resource quota Controller ：監控叢集 CPU 與 Memory 資源資訊並回饋給 API-Server 當前資源狀態。\n    圖引用自superuser\n","description":"","id":14,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(controller-manger)","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-controller-manger-overview/"},{"content":"  kubernetes control plane 的那些元件 如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態，所以了解 Kubernetes 的大腦是一件相當重要的事情。\n  在 Master Node 主要由 API-Server 、 Controller Manager 以及 Scheduler 三大基礎元件所組合而成，其中 API-Server 元件可視為整體 Kubernetes Container 調度系統架構的管理中心，也是 Kubernetes Container 叢集調度系統中唯一能與後端儲存 etcd 元件溝通的元件，主要提供以下服務：\n  API-Server 提供叢集安全且可靠的使用機制，所有的 Client 請求都需要經過 API-Server 的認證。 API-Server 支持多種認證機制如 WebSocket 、 Keystone 、 Token 等等，如果 API-Server 認證成功， Client 請求將會傳入 Kubernetes 叢集內進行處理；而對於認證失敗的請求則返回 HTTP 401 的錯誤。\n HTTPS Certificate Authority ，基於 Certificate Authority (CA) 證書簽名的雙向數字證書認證方式。 HTTP Token Authority ，透過發放 Token 的方式識別每個使用者的身份，以及透過該 Token 限制使用者能存取的範圍。 HTTP Basic Authority ，當使用者向 API Server 發起 HTTP 請求時，會在資料中帶入 Username 、 UID 、 Groups 、 Extra fields 作為使用者的身份驗證的依據。    API-Server 同時提供 REST API 的呼叫介面如 Create 、 Read 、 Update 、 Delete 等一系列操作對 Kubernetes 的 Pod 、 Service 、 Deployment 等元件進行操作，例如使用者可以透過 REST API 向 API-Server 請求建立、刪除、更新以及讀取 Kubernetes 的資源。\n  API-Server 元件提供專有的監控介面給 Controller Manger 元件與 Scheduler 元件，可以透過該專有介面讀取特定資料，例如 Controller Manger 透過 API-Server 提供的介面讀取 Pod 的狀態並且加以控制， Scheduler 透過介面讀取 Kubernetes Node 目前的壓力情況作為排程 Pod 的參考依據。\n  大致上流程如下圖所示，當請求送進 kubernetes cluster 會通過Api server的認證，接著檢查該次請求的內容請求，最後再送進 etcd 做永久性儲存。（以後有機會再來談談Authoization跟Admission control 的細節）\n引用giantswarm.io\n","description":"","id":15,"section":"posts","tags":["kubernetes"],"title":"Kubernetes Control Plane 這件小小事(api-server)","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes-apiserver-overview/"},{"content":"  來說說etcd Kubernetes Container 叢集調度系統中後端採用 etcd 作為叢集狀態的儲存元件，而該 etcd 元件是由 CoreOS 公司開發並且開源貢獻給 CNCF 的 key/value 儲存專案。 etcd 主要負責將 Kubernetes API-Server 所處理過的資料進行加密儲存。 etcd 元件大多數會位於 Kubernetes 系統架構中的 Master Node 上。 API-Server 若要存取 etcd 上的資料必須先經過 etcd 的身分驗證流程確認使用者存取的身份與範圍是否合法，若身分確認無誤與存取範圍合法則將使用者所請求的資料透過 REST API 回傳資料給使用者。\nKubernetes 在通常情況下只會替系統管理員在 Master Node 上建立一個 etcd 元件，可能因為外在因素或是 etcd 請求負載過於龐大使得 etcd 元件發生故障，進而導致整體 Kubernetes發生API-Server 無法存取後端資料的問題。針對此問題系統管理員可以透過 Kubernetes Container 叢集調度技術建立一個高可用的 etcd 服務叢集，若單一個 etcd 元件發生故障時仍有其他 etcd 元件可以立即地補上作為備援使用。\netcd 怎麼在分散的世界保持一致 etcd 元件組成 etcd 叢集服務時首要任務就是確保後端資料是一制性的，以防使用者存取到錯誤的資料， etcd 採用的是 Raft 一至性共識演算法。\n透過 Raft 演算法使得在同一個時間點上可以維持多個 etcd 元件儲存的狀態，這就保證了後端儲存的資料是保持一致，此外該演算法保證了當少數 etcd 元件崩潰或失效時仍不影響整體叢集的同步。\nRaft 就是一種 leader-based 的共識算法並且使用心跳機制 (Heartbeat mechanism) 觸發同步與選舉，在該演算法裡面分別有三種身份第一種是 Leader 主要是叢集的領導人其他身份都必須跟領導人的資料同步，第二種是 Candidate 是當領導人無回應時欲參加該次選舉的節點稱為候選人，最後一種為 Follower 在選舉期間負責投票給候選人的節點如圖所示。\n選舉的過程   一開始叢集各個節點身份都為Follower，並且節點上預設的選舉逾時時間(Election Time Out)與一般作業時間(Normal Operation)皆為範圍隨機亂數。在一般作業時間內節點會期待自己收到Leader週期性傳送過來的Heartbeat並且重置一般作業時間，若是超過一般作業時間節點都沒有收到Leader所傳送過來的Heartbeat該節點會當作目前沒有Leader的狀態，將觸發Leader的重選。\n  由於節點觸發Leader的重選此時節點將從Follower身份轉變為Candidate身份並且紀錄當前任期的號碼向叢集內其餘節點發送投票之請求，在觸發選舉後會有一段選舉逾時時間在該時間內，對於同一任期Follower只能選投一名Candidate，Candidate收到大於等於(2n+1) Follower的同意票，則該節點從Candidate身份轉變為叢集的新Leader。若在選舉逾時時間結束沒有收到(2n+1) Follower的同意票則視為當前選舉無效，再增加一次當前任期重新發起新一輪的選舉。而新的Leader需透過週期性的發送Heartbeat給各個節點來維持該Leader身份。\n  當發生在選舉逾時時間結束沒有收到(2n+1) Follower的同意票，無法選出Leader的情況，根據Raft演算法會讓節點縮短選舉逾時時間加速選出叢集的Leader以減少叢集無法同步的時間，另外在選舉期間節點可能會收到來自其他節點宣稱自己是Leader的心跳，該節點會確認心跳訊息內的任期編號是否大於當前任期，若是大於當前任期所有節點身份轉變為Follower身份並且與該Leader同步資料，若是心跳訊息的任期編號小於當前任期則無視該節點，繼續任期投票工作。\n  當叢集的Leader被選出之後，Leader會將使用者的每個請求都包裝成一個Commit並且透過Heartbeat週期性的複製到其他節點做為副本，當一個commit超過半數的節點都以複製並儲存才會能算該commit成功。此外該commit的元資料(metadata)會記錄當時commit的Leader任期號碼是多少，該任期號用來判斷節點之間副本不一致情形並且在每個一個節點儲存Commit都會有一個整數索引值來確認其在Commit目前已經儲存到第幾個位址如圖所示。\n  各個節點發生儲存內容不一致的情況，若任期三是第三節點繼續當Leader，在Leader發送Heartbeat的同時會將其他節點的資料強制覆蓋過去，利用Leader的強制性解決資料不統一的情況。第三節點發送一次Heartbeat強制同步個節點的index的Commit，如下圖所示。\n後話 會繼續接著把 Kubernetes 的其他元件繼續講解一遍，如果有誤的地方歡迎大家指出謝謝～～\nREF  \u0026ldquo;Etcd | Coreos\u0026rdquo;. Coreos.Com, 2019, https://coreos.com/blog/etcd. Oliveira, Caio, et al. \u0026ldquo;Evaluating raft in docker on kubernetes.\u0026rdquo; International Conference on Systems Science. Springer, Cham, 2016. ","description":"","id":16,"section":"posts","tags":["kubernetes"],"title":"etcd 那回事","uri":"https://blog.jjmengze.website/posts/kubernetes/etcd/"},{"content":"  什麼是 Kubernetes Kubernetes 是一個以 Apache 授權條款的開源專案主要作為 Container 叢集的系統管理平台，其系統具備高度可擴展性，由 Joe Beda、Brendan Burns 和 Craig McLuckie 等 Google 工程師建立此套系統。\nKubernetes v1.0於2015年7月21日發布。隨著v1.0版本發布，Google與Linux 基金會合作建立了Cloud Native Computing Foundation (CNCF)，並把Kubernetes作為種子技術來提供。\n來看看架構圖 Kubernetes Container 叢集系統管理平台，將能夠透過Kubernetes 的機制提供VNF擁有更好的可靠性、負載平衡、滾動更新等特性。然而在 Kubernetes Container 叢集管理平台系統中，擁有不同的節點角色與基礎元件。\n如下圖所示， Kubernektes 整體架構是主從式架構( Client–server model )由 Master Node 負責 Container 叢集管理與調度相當於 Kubernetes 的大腦， Worker Node 負責運行 Master Node 所派送過來的任務，並將任務確實執行並且定期回報 Master Node 目前的節點的狀態。\n  就 Master Node 而言 Kubernetes 提供了一種方便操作的 kubectl 命令列介面工具，對於管理人員來說，只要透過 kubectl 就能向 Kubernetes Container 叢集下達指令並得到當前叢集服務運行的狀態。 Kubernetes 同時考量安全性的問題，當任何人員想要對叢集進行操作都必須先經過 Kubernetes 的身份驗證，當驗證成功叢集才會接收操作命令反之亦然。\n當驗證成功後所有的操作行為例如：部署 Container 服務、存取 Container 服務執行的狀態、刪除 Container 服務等行為，都會經由 Kubernetes Master Node 上的 API-Server 透過gRPC通訊協定向其他元件進行溝通並取回資料，此外在 Kubernetes 叢集每個節點上都會運行一個名為 kubelet 元件，該 kubelet 元件主要負責接收 Master Node 所指派的任務並在該節點上部署 Container 服務並且時刻的監視 node 上的 Container 服務的狀態，並且透過 gRPC 通訊協定回報給 Kubernetes Master Node上的API-Server，當 API-Server 收到節點資訊並彙整後將存入後端的 etcd key/value 資料庫。\n後話 後續包含Scheduler、Controller Manager、API-Server、etcd、kube-proxy、kubelet 等元件，將一一介紹 Kubenetes 架構中的主要元件與相關的資源型態。\n","description":"","id":17,"section":"posts","tags":["kubernetes"],"title":"Kubernetes 淺入淺出","uri":"https://blog.jjmengze.website/posts/kubernetes/kubernetes/"},{"content":"VM 與 Container 架構圖 與 VM 相比使用 Container 具有各種優點，在要執行數百個應用程式的使用情境之下使用基於 Container 的解決方案顯得相當高效， Container 所佔的空間以及效能損失都優於 VM 。在需要快速回收資源與啟動服務的應用情境下，透過 Container 承載這些服務遠比選擇 VM 的效果要好，由於 Container 啟動的速度只需要幾十秒的時間， VM 啟動所需要的時間卻高達數分鐘。\nVM 與 Container 比較表     VM Container     Communication 透過Ethernet Devices 透過IPC：Socket、pipes、Signals等機制   Guest OS 獨立的Kernel 共用宿主Kernel   Performance 效能消耗較大 效能消耗較小   Isolation 隔離性較好 隔離性較差   Startup time 數分鐘 幾十秒   Storage 佔較大空間 佔較小空間    僅透過 VM ，將相關的 service 部署到 VM 中以用來處理使用者請求的服務，由於 VM 的效能損失以及啟動時間的問題，採用 container 的方式可以大幅度節省效能的損失以及加速部署與產品上限的時間。\n但是對於部署規模較大的運算叢集來說，Container 技術僅能縮短服務啟動的時間，並且沒有一個方便管理 Container 化叢集的方式與確認當前 Container 服務運行的狀態。若管理人員不小心刪除叢集的某一服務將造成災難性的危害。因此，需要建置一個 Container 叢集管理平台管理 Container 服務。\nGoogle 累積多年管理 Container 叢集經驗，進而開發 Kubernetes Container 叢集調度技術的開源專案，同時也是目前業界廣為採用的 Container 叢集管理系統如公有雲的 AWS 的 EKS 、 Google 提供的 GKE 以及 Azure 的 AKS ，對於管理人員來說使用 Kubernetes 除了能大量減少建置叢集的複雜流程外，也可以透過 Kubernetes 內建的元件建置所需要 Container 服務，透過 Kubernetes 可以提高服務的可用性，因此以下將進一步說明 Kubernetes 的系統架構以及相關元件。\nREF  Xavier, Miguel G., et al. \u0026ldquo;Performance evaluation of container-based virtualization for high performance computing environments.\u0026rdquo; 2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. IEEE, 2013. Soltesz, Stephen, et al. \u0026ldquo;Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors.\u0026rdquo; ACM SIGOPS Operating Systems Review. Vol. 41. No. 3. ACM, 2007. Burns, Brendan, et al. \u0026ldquo;Borg, omega, and kubernetes.\u0026rdquo; (2016). Cherrueau, Ronan-Alexandre, et al. \u0026ldquo;Edge computing resource management system: a critical building block! initiating the debate via openstack.\u0026rdquo; {USENIX} Workshop on Hot Topics in Edge Computing (HotEdge 18). 2018. Web.Kaust.Edu.Sa, 2019, http://web.kaust.edu.sa/Faculty/MarcoCanini/classes/CS240/F17/slides/L3-cloud-VM.pdf ","description":"","id":18,"section":"posts","tags":["kubernetes"],"title":"來看看Container","uri":"https://blog.jjmengze.website/posts/kubernetes/container/"}]